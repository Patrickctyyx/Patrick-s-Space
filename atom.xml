<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Patrick&#39;s Space</title>
  
  <subtitle>Stay hungry, stay foolish!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.patrickcty.cc/"/>
  <updated>2021-06-25T15:30:21.866Z</updated>
  <id>https://blog.patrickcty.cc/</id>
  
  <author>
    <name>Patrick</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>swin-object-detection遇到的问题</title>
    <link href="https://blog.patrickcty.cc/2021/06/25/swin-object-detection%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://blog.patrickcty.cc/2021/06/25/swin-object-detection遇到的问题/</id>
    <published>2021-06-25T15:24:13.000Z</published>
    <updated>2021-06-25T15:30:21.866Z</updated>
    
    <content type="html"><![CDATA[<p>在配置 <a href="https://github.com/SwinTransformer/Swin-Transformer-Object-Detection" target="_blank" rel="noopener">Swin-Transformer-Object-Detection</a> 的过程中出现了 pycocotools 不兼容额问题，但是另一个类似的环境在安装 mmdetection 的时候并没有问题。（pycocotools 版本相同）</p><p>于是按照 mmdetection 的结构对 <code>mmdet/datasets/coco.py</code> 进行修改。</p><p>修改 <code>COCO</code> 和 <code>COCOEval</code> 的引用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .api_wrappers <span class="keyword">import</span> COCO, COCOeval</span><br></pre></td></tr></table></figure><p>去掉 <code>get_img_ids</code> 的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.img_ids = self.coco.get_img_ids()</span><br></pre></td></tr></table></figure><p>新建 <code>mmdet/datasets/api_wrappers/coco_api.py</code>，将以下内容粘贴进去</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This file add snake case alias for coco api</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pycocotools</span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO <span class="keyword">as</span> _COCO</span><br><span class="line"><span class="keyword">from</span> pycocotools.cocoeval <span class="keyword">import</span> COCOeval <span class="keyword">as</span> _COCOeval</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">COCO</span><span class="params">(_COCO)</span>:</span></span><br><span class="line">    <span class="string">"""This class is almost the same as official pycocotools package.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It implements some snake case function aliases. So that the COCO class has</span></span><br><span class="line"><span class="string">    the same interface as LVIS class.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, annotation_file=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> getattr(pycocotools, <span class="string">'__version__'</span>, <span class="string">'0'</span>) &gt;= <span class="string">'12.0.2'</span>:</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">'mmpycocotools is deprecated. Please install official pycocotools by "pip install pycocotools"'</span>,  <span class="comment"># noqa: E501</span></span><br><span class="line">                UserWarning)</span><br><span class="line">        super().__init__(annotation_file=annotation_file)</span><br><span class="line">        self.img_ann_map = self.imgToAnns</span><br><span class="line">        self.cat_img_map = self.catToImgs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_ann_ids</span><span class="params">(self, img_ids=[], cat_ids=[], area_rng=[], iscrowd=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.getAnnIds(img_ids, cat_ids, area_rng, iscrowd)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_cat_ids</span><span class="params">(self, cat_names=[], sup_names=[], cat_ids=[])</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.getCatIds(cat_names, sup_names, cat_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_img_ids</span><span class="params">(self, img_ids=[], cat_ids=[])</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.getImgIds(img_ids, cat_ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_anns</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.loadAnns(ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_cats</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.loadCats(ids)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_imgs</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.loadImgs(ids)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># just for the ease of import</span></span><br><span class="line">COCOeval = _COCOeval</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在配置 &lt;a href=&quot;https://github.com/SwinTransformer/Swin-Transformer-Object-Detection&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Swin-Transformer-Object
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mmdet常用配置</title>
    <link href="https://blog.patrickcty.cc/2021/06/19/mmdet%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"/>
    <id>https://blog.patrickcty.cc/2021/06/19/mmdet常用配置/</id>
    <published>2021-06-19T14:37:06.000Z</published>
    <updated>2021-06-25T15:14:50.494Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 总的 epoch 数</span></span><br><span class="line">total_epochs = <span class="number">200</span></span><br><span class="line">runner = dict(type=<span class="string">'EpochBasedRunner'</span>, max_epochs=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 batchsize</span></span><br><span class="line">data = dict(samples_per_gpu=<span class="number">2</span>, workers_per_gpu=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证的间隔，太短的话可能增多训练时间</span></span><br><span class="line">evaluation = dict(metric=[<span class="string">'bbox'</span>, <span class="string">'segm'</span>], interval=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出参数的文件夹，默认是在项目目录下的</span></span><br><span class="line">work_dir = <span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存 checkpoint 的间隔</span></span><br><span class="line">checkpoint_config = dict(interval=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载检查点</span></span><br><span class="line">load_from = <span class="string">'xxxx.pth'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从中断的地方继续训练</span></span><br><span class="line">resume_from = <span class="string">'xxxx.pth'</span></span><br></pre></td></tr></table></figure><p>根据 step 来训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr_config = dict(policy=<span class="string">'poly'</span>, power=<span class="number">0.9</span>, min_lr=<span class="number">1e-4</span>, by_epoch=<span class="literal">False</span>)</span><br><span class="line">total_iters = <span class="number">160000</span></span><br><span class="line">checkpoint_config = dict(by_epoch=<span class="literal">False</span>, interval=<span class="number">16000</span>)</span><br><span class="line">evaluation = dict(interval=<span class="number">16000</span>, metric=<span class="string">'mIoU'</span>)</span><br></pre></td></tr></table></figure><p>P.S. 如果某个配置中有 <code>_delete_=True</code>，那么表明用当前配置覆盖掉继承的配置，不加的话仅仅只覆盖掉当前指定的配置。比如当前指定了 <code>evaluation = dict(interval=16000, metric=&#39;mIoU&#39;)</code>，而继承的配置中传入了三个参数，那另一个参数还是会使用原本的配置，但是加了 <code>_delete_=True</code> 另一个参数就不会使用原本的配置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mmsegmentation二分类配置</title>
    <link href="https://blog.patrickcty.cc/2021/05/21/mmsegmentation%E4%BA%8C%E5%88%86%E7%B1%BB%E9%85%8D%E7%BD%AE/"/>
    <id>https://blog.patrickcty.cc/2021/05/21/mmsegmentation二分类配置/</id>
    <published>2021-05-21T09:26:51.000Z</published>
    <updated>2021-06-06T08:27:25.443Z</updated>
    
    <content type="html"><![CDATA[<h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><p>configs 目录下有很多已有的配置，在自己乱试之前先看看！比如医学图像分割也是二分类任务，并且其最有名的模型是 U-Net，因此今天的配置都可以直接参考相关的配置。</p><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><p>由于 mmsegmentation 的版本更新得比较快，因此基于此开发的项目环境一般不太兼容，最好每个都重新搞一个虚拟环境，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">conda create -n open-mmlab python=3.7 -y</span><br><span class="line">conda activate open-mmlab</span><br><span class="line"></span><br><span class="line">conda install pytorch=1.6.0 torchvision cudatoolkit=10.1 -c pytorch -y</span><br><span class="line">pip install mmcv-full==1.2.2 -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html</span><br><span class="line">git clone https://github.com/fudan-zvg/SETR.git  # 某个基于 mmsegmentation 开发的项目</span><br><span class="line">cd SETR</span><br><span class="line">pip install -e .  # or "python setup.py develop"</span><br><span class="line">pip install -r requirements/optional.txt</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 可选，这里是为了切换多个 CUDA 版本</span><br><span class="line">conda env config vars set PATH=/home/sse/anaconda3/envs/open-mmlab/bin:/usr/local/cuda-10.1/bin:$PATH LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64:$LD_LIBRARY_PATH -n open-mmlab</span><br></pre></td></tr></table></figure><h2 id="纲要"><a href="#纲要" class="headerlink" title="纲要"></a>纲要</h2><ul><li>自定义数据集<ul><li><code>configs/_base_/datasets/&lt;your_dataset&gt;.py</code></li><li><code>mmseg/datasets/&lt;your_dataset&gt;.py</code></li><li><code>mmseg/datasets/__init__.py</code></li></ul></li><li>自定义训练</li></ul><h3 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h3><p>MM 系列框架都经过了非常高的封装，我们只用修改配置文件就可以非常容易地训练不同的模型。其本质上就是通过各个模块的注册来实现的。比如说在 <code>mmseg/datasets</code> 目录下有一个 <code>builder.py</code> 文件，这个文件中有 <code>DATASETS</code> 和 <code>PIPELINE</code> 两个全局变量，前者用来注册数据集，后者用来注册数据加载处理的一些操作。比如打开 <code>ade.py</code> 这个数据集定义文件，我们可以看到在定义类的时候有一个装饰器 <a href="mailto:`@DATASETS.registe" target="_blank" rel="noopener">`@DATASETS.registe</a>r_module()<code>自动注册了这个数据集。因此如果要自定义数据集甚至是网络的模块，只用在相关位置定义然后注册即可。注册的话还有另一个步骤，那就是将其添加到</code>mmseg/datasets/<strong>init</strong>.py` 之中。</p><h4 id="创建数据集类"><a href="#创建数据集类" class="headerlink" title="创建数据集类"></a>创建数据集类</h4><p>先别慌着写，先把已有的数据集类都扫一遍。扫一遍之后我们可以发现要自定义一个数据集类，首先需要继承 <code>CustomDataset</code> 类，然后再在其中自定义 <code>CLASSES</code> 类变量与 <code>PALETTE</code> 类变量，其中前者是为了将类别标号与自然语言对应起来，后者是可视化的时候对应不同的颜色。再就只用根据数据集的特点来自定义构造方法了。我们先看看 <code>CustomDataset</code> 构造方法的参数，在 <code>mmseg/datasets/custom.py</code> 的 47 行，由于其中大部分都通过配置文件来指定，因此我们只集中于几个重要的参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Args:</span><br><span class="line">    img_suffix (str): 图像的后缀，只读取特定后缀的图像，对于混合搭建的数据集非常友好，默认值：&apos;.jpg&apos;</span><br><span class="line">    seg_map_suffix (str): GT 的后缀，只读取特定后缀的 GT，默认值：&apos;.png&apos;</span><br><span class="line">    ignore_index (int): 忽视掉的标签，默认值：255。比如我们可以把 GT padding 设为 255，这样这部分就不会被网络处理</span><br><span class="line">    reduce_zero_label (bool): 忽视为 0 的标签，在实现中是将 0 变成 255，然后将其他所有标签值减一，默认值：False</span><br></pre></td></tr></table></figure><h4 id="添加索引"><a href="#添加索引" class="headerlink" title="添加索引"></a>添加索引</h4><p>然后观察完之后得知 <code>drive.py</code>，<code>hrf.py</code>，<code>stare.py</code> 这几个数据集是二分类数据集，因此数据的组织可以参考这几个。新建一个 <code>mmseg/datasets/saliency.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mmseg/datasets/saliency.py</span></span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .builder <span class="keyword">import</span> DATASETS</span><br><span class="line"><span class="keyword">from</span> .custom <span class="keyword">import</span> CustomDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@DATASETS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SaliencyDataset</span><span class="params">(CustomDataset)</span>:</span></span><br><span class="line">    <span class="string">"""Saliency dataset.</span></span><br><span class="line"><span class="string">    In segmentation map annotation for Saliency, 0 stands for background, which is</span></span><br><span class="line"><span class="string">    included in 2 categories. ``reduce_zero_label`` is fixed to False. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    CLASSES = (<span class="string">'background'</span>, <span class="string">'saliency'</span>)</span><br><span class="line"></span><br><span class="line">    PALETTE = [[<span class="number">120</span>, <span class="number">120</span>, <span class="number">120</span>], [<span class="number">6</span>, <span class="number">230</span>, <span class="number">230</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(SaliencyDataset, self).__init__(</span><br><span class="line">            reduce_zero_label=<span class="literal">False</span>,</span><br><span class="line">            **kwargs)</span><br><span class="line">        <span class="keyword">assert</span> osp.exists(self.img_dir)</span><br></pre></td></tr></table></figure><p>然后将定义好的类添加到 <code>mmseg/datasets/__init__.py</code> 中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .saliency.py <span class="keyword">import</span> SaliencyDataset</span><br><span class="line"></span><br><span class="line">__all__ = [</span><br><span class="line">    ..., <span class="string">'SaliencyDataset'</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="添加数据集配置"><a href="#添加数据集配置" class="headerlink" title="添加数据集配置"></a>添加数据集配置</h4><p>添加 <code>configs/_base_/datasets/cod10k.py</code>，内容依旧对着上面提到的二分类数据集改即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset settings</span></span><br><span class="line">dataset_type = <span class="string">'SaliencyDataset'</span></span><br><span class="line">data_root = <span class="string">'data/Saliency'</span>  <span class="comment"># 修改成你数据集所在路径</span></span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[<span class="number">123.675</span>, <span class="number">116.28</span>, <span class="number">103.53</span>], std=[<span class="number">58.395</span>, <span class="number">57.12</span>, <span class="number">57.375</span>], to_rgb=<span class="literal">True</span>)</span><br><span class="line">img_scale = (<span class="number">800</span>, <span class="number">800</span>)  <span class="comment"># 修改成你数据集合适的大小</span></span><br><span class="line">crop_size = (<span class="number">384</span>, <span class="number">384</span>)  <span class="comment"># 修改成合适的大小</span></span><br><span class="line">train_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(type=<span class="string">'LoadAnnotations'</span>),  <span class="comment"># 这里不要 reduce_zero_label!!!</span></span><br><span class="line">    dict(type=<span class="string">'Resize'</span>, img_scale=img_scale, ratio_range=(<span class="number">0.5</span>, <span class="number">2.0</span>)),</span><br><span class="line">    dict(type=<span class="string">'RandomCrop'</span>, crop_size=crop_size, cat_max_ratio=<span class="number">0.75</span>),</span><br><span class="line">    dict(type=<span class="string">'RandomFlip'</span>, prob=<span class="number">0.5</span>),</span><br><span class="line">    dict(type=<span class="string">'PhotoMetricDistortion'</span>),</span><br><span class="line">    dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">    dict(type=<span class="string">'Pad'</span>, size=crop_size, pad_val=<span class="number">0</span>, seg_pad_val=<span class="number">255</span>),</span><br><span class="line">    dict(type=<span class="string">'DefaultFormatBundle'</span>),</span><br><span class="line">    dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>, <span class="string">'gt_semantic_seg'</span>])</span><br><span class="line">]</span><br><span class="line">test_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(</span><br><span class="line">        type=<span class="string">'MultiScaleFlipAug'</span>,</span><br><span class="line">        img_scale=img_scale,</span><br><span class="line">        <span class="comment"># img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],</span></span><br><span class="line">        flip=<span class="literal">False</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            dict(type=<span class="string">'Resize'</span>, keep_ratio=<span class="literal">True</span>),</span><br><span class="line">            dict(type=<span class="string">'RandomFlip'</span>),</span><br><span class="line">            dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">            dict(type=<span class="string">'ImageToTensor'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">            dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>])</span><br><span class="line">        ])</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">data = dict(</span><br><span class="line">    samples_per_gpu=<span class="number">4</span>,</span><br><span class="line">    workers_per_gpu=<span class="number">4</span>,</span><br><span class="line">    train=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        data_root=data_root,</span><br><span class="line">        img_dir=<span class="string">'images/training'</span>,  <span class="comment"># 修改数据集路径，下同</span></span><br><span class="line">        ann_dir=<span class="string">'annotations/training'</span>,</span><br><span class="line">        pipeline=train_pipeline),</span><br><span class="line">    val=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        data_root=data_root,</span><br><span class="line">        img_dir=<span class="string">'images/validation'</span>,</span><br><span class="line">        ann_dir=<span class="string">'annotations/validation'</span>,</span><br><span class="line">        pipeline=test_pipeline),</span><br><span class="line">    test=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        data_root=data_root,</span><br><span class="line">        img_dir=<span class="string">'images/validation'</span>,</span><br><span class="line">        ann_dir=<span class="string">'annotations/validation'</span>,</span><br><span class="line">        pipeline=test_pipeline))</span><br></pre></td></tr></table></figure><h3 id="自定义训练配置"><a href="#自定义训练配置" class="headerlink" title="自定义训练配置"></a>自定义训练配置</h3><p>新建一个 <code>configs/swin/upernet_swin_tiny_patch4_window7_384x384_40k_cod.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">_base_ = [  <span class="comment"># 注意修改路径</span></span><br><span class="line">    <span class="string">'../_base_/models/upernet_swin.py'</span>, <span class="string">'../_base_/datasets/cod10k.py'</span>,</span><br><span class="line">    <span class="string">'../_base_/default_runtime.py'</span>, <span class="string">'../_base_/schedules/schedule_40k.py'</span></span><br><span class="line">]</span><br><span class="line">model = dict(</span><br><span class="line">    backbone=dict(</span><br><span class="line">        embed_dim=<span class="number">96</span>,</span><br><span class="line">        depths=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span>],</span><br><span class="line">        num_heads=[<span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>],</span><br><span class="line">        window_size=<span class="number">7</span>,</span><br><span class="line">        ape=<span class="literal">False</span>,</span><br><span class="line">        drop_path_rate=<span class="number">0.3</span>,</span><br><span class="line">        patch_norm=<span class="literal">True</span>,</span><br><span class="line">        use_checkpoint=<span class="literal">False</span></span><br><span class="line">    ),</span><br><span class="line">    decode_head=dict(</span><br><span class="line">        in_channels=[<span class="number">96</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">768</span>],</span><br><span class="line">        num_classes=<span class="number">2</span>,  <span class="comment"># 注意这里是两类</span></span><br><span class="line">        <span class="comment"># 由于正负样本非常不平衡，因此这里给正样本加了权重</span></span><br><span class="line">        loss_decode=dict(class_weight=(<span class="number">1</span> ,<span class="number">8</span>))</span><br><span class="line">    ),</span><br><span class="line">    auxiliary_head=dict(</span><br><span class="line">        in_channels=<span class="number">384</span>,</span><br><span class="line">        num_classes=<span class="number">2</span>,  <span class="comment"># 注意这里是两类</span></span><br><span class="line">        loss_decode=dict(class_weight=(<span class="number">1</span> ,<span class="number">8</span>))</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdamW optimizer, no weight decay for position embedding &amp; layer norm in backbone</span></span><br><span class="line">optimizer = dict(_delete_=<span class="literal">True</span>, type=<span class="string">'AdamW'</span>, lr=<span class="number">0.00006</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), weight_decay=<span class="number">0.01</span>,</span><br><span class="line">                 paramwise_cfg=dict(custom_keys=&#123;<span class="string">'absolute_pos_embed'</span>: dict(decay_mult=<span class="number">0.</span>),</span><br><span class="line">                                                 <span class="string">'relative_position_bias_table'</span>: dict(decay_mult=<span class="number">0.</span>),</span><br><span class="line">                                                 <span class="string">'norm'</span>: dict(decay_mult=<span class="number">0.</span>)&#125;))</span><br><span class="line"></span><br><span class="line">lr_config = dict(_delete_=<span class="literal">True</span>, policy=<span class="string">'poly'</span>,</span><br><span class="line">                 warmup=<span class="string">'linear'</span>,</span><br><span class="line">                 warmup_iters=<span class="number">1500</span>,</span><br><span class="line">                 warmup_ratio=<span class="number">1e-6</span>,</span><br><span class="line">                 power=<span class="number">1.0</span>, min_lr=<span class="number">0.0</span>, by_epoch=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># By default, models are trained on 8 GPUs with 2 images per GPU</span></span><br><span class="line"><span class="comment"># 修改了 batchsize 之后按比例修改 lr，bs 越大 lr 应该越大</span></span><br><span class="line">data = dict(samples_per_gpu=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tools/dist_train.sh configs/swin/upernet_swin_tiny_patch4_window7_384x384_40k_cod.py 2 --options model.pretrained=&lt;PRETRAIN_MODEL&gt;</span><br></pre></td></tr></table></figure><p>其中 2 指的是 GPU 的数量，具体的还是看原项目的 <a href="https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation" target="_blank" rel="noopener">README</a></p><h2 id="一些报错"><a href="#一些报错" class="headerlink" title="一些报错"></a>一些报错</h2><h3 id="CUDA-error-an-illegal-memory-access-was-encountered"><a href="#CUDA-error-an-illegal-memory-access-was-encountered" class="headerlink" title="CUDA error: an illegal memory access was encountered"></a>CUDA error: an illegal memory access was encountered</h3><p>==mmsegmentation 中 gt 的范围是 [0, num_classes - 1]==，如果 gt 的值超过了这个范围就会出这个问题。由于增强等原因，可能前几个 ep 训练正常，到某个地方突然就报错。</p><h3 id="KeyError-“‘XXX-is-not-in-the-YYY-registry’”"><a href="#KeyError-“‘XXX-is-not-in-the-YYY-registry’”" class="headerlink" title="KeyError: “‘XXX is not in the YYY registry’”"></a>KeyError: “‘XXX is not in the YYY registry’”</h3><p>这个错误非常常见，我们只需要在 <code>mmseg</code> 下的相应文件里面去找即可。比如如果 YYY 是 backbone，那么就去 <code>mmseg/models/backbones/__init__.py</code> 里面去找，如果里面没有 XXX，那么要么是打错了，要么是不支持这个 backbone，也有可能没安装好。</p><p>第三种可能就输入 <code>pip install -e .</code> 命令重新安装即可。</p><h3 id="Training-loss-is-always-0-000"><a href="#Training-loss-is-always-0-000" class="headerlink" title="Training loss is always 0.000"></a>Training loss is always 0.000</h3><p>很有可能是类配置错了或者数据集配置有问题，对着上面的排查一下就好</p><h3 id="RuntimeError-CUDA-error-device-side-assert-triggered"><a href="#RuntimeError-CUDA-error-device-side-assert-triggered" class="headerlink" title="RuntimeError: CUDA error: device-side assert triggered"></a>RuntimeError: CUDA error: device-side assert triggered</h3><p>检查一下是不是 <code>num_classes</code> 配置有问题</p><h3 id="RuntimeError-DataLoader-worker-pid-5657-is-killed-by-signal-Killed"><a href="#RuntimeError-DataLoader-worker-pid-5657-is-killed-by-signal-Killed" class="headerlink" title="RuntimeError: DataLoader worker (pid 5657) is killed by signal: Killed"></a>RuntimeError: DataLoader worker (pid 5657) is killed by signal: Killed</h3><p>检查一下是不是爆内存了，检查一下数据集是不是有问题</p><h3 id="跑-SETR-测试的时候出问题"><a href="#跑-SETR-测试的时候出问题" class="headerlink" title="跑 SETR 测试的时候出问题"></a>跑 SETR 测试的时候出问题</h3><p>vit 只支持固定大小的输入，正常的 test 流程出来的图片大小不一，会报错。</p><p>SETR 中在配置文件中设置 <code>test_cfg = dict(mode=&#39;slide&#39;, crop_size=crop_size, stride=(320, 320))</code>，不使用整个图像来预测，而是用固定大小的块来进行预测。</p><p>但是我们使用的数据集中有的图像较小，导致 crop 出来的还是大小不一，因此还需要在 <code>configs/_base_/datasets/cod10k.py</code> 里面修改 <code>test_pipeline</code>，具体来说就是在 Resize 之后再加上一个 Pad 操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transforms=[</span><br><span class="line">    dict(type=<span class="string">'Resize'</span>, ...),</span><br><span class="line">    dict(type=<span class="string">'Pad'</span>, size=crop_size, pad_val=<span class="number">0</span>, seg_pad_val=<span class="number">255</span>),</span><br><span class="line">    ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><p>优先检查数据集是不是有问题，然后再检查配置文件是不是没对应上（尤其是类别数）。另外今天遇到一个问题是在 <code>configs/_base_/datasets/cod10k.py</code> 里面错误地设置了 <code>reduce_zero_lable=True</code>，导致训练出现了异常。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;教训&quot;&gt;&lt;a href=&quot;#教训&quot; class=&quot;headerlink&quot; title=&quot;教训&quot;&gt;&lt;/a&gt;教训&lt;/h2&gt;&lt;p&gt;configs 目录下有很多已有的配置，在自己乱试之前先看看！比如医学图像分割也是二分类任务，并且其最有名的模型是 U-Net，因此今天的配
      
    
    </summary>
    
    
      <category term="mmsegmentation" scheme="https://blog.patrickcty.cc/tags/mmsegmentation/"/>
    
  </entry>
  
  <entry>
    <title>使用COCOAPI评测结果</title>
    <link href="https://blog.patrickcty.cc/2021/02/15/%E4%BD%BF%E7%94%A8COCOAPI%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C/"/>
    <id>https://blog.patrickcty.cc/2021/02/15/使用COCOAPI评测结果/</id>
    <published>2021-02-15T02:44:13.000Z</published>
    <updated>2021-02-15T09:14:45.613Z</updated>
    
    <content type="html"><![CDATA[<p>目标检测、实例分割以及关键点检测都可以使用 COCO API 来进行评测，主要方法有两种：</p><ol><li>按照 COCO 的格式保存预测的结果，然后评测</li><li>不保存预测结果，直接评测</li></ol><h2 id="保存预测结果"><a href="#保存预测结果" class="headerlink" title="保存预测结果"></a>保存预测结果</h2><p>这一种方法要简单一些，只用给定格式来保存即可，具体格式参考<a href="https://zhuanlan.zhihu.com/p/134236324" target="_blank" rel="noopener">这篇博客</a>，注意事项如下：</p><ol><li>Box 是按照 [x, y, w, h] 的格式来保存</li><li>Mask 是用 RLE 格式来保存，使用如下代码来转换</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycocotools.mask <span class="keyword">as</span> mask_util</span><br><span class="line"></span><br><span class="line">masks = masks &gt; <span class="number">0.5</span> <span class="comment"># 首先转化为二值 mask，这里假设 mask 通道为 1</span></span><br><span class="line">rle = mask_util.encode(np.array(segmap_masked[:, :, np.newaxis], dtype=np.uint8, order=<span class="string">"F"</span>))[<span class="number">0</span>]</span><br><span class="line">rle[<span class="string">'counts'</span>] = rle[<span class="string">"counts"</span>].decode(<span class="string">"utf-8"</span>)</span><br><span class="line">dataset_results.append(&#123;<span class="string">'image_id'</span>: all_imgs[i][<span class="string">'id'</span>], <span class="string">'category_id'</span>: <span class="number">1</span>,</span><br><span class="line">                        <span class="string">'segmentation'</span>: rle, <span class="string">"score"</span>: float(cls_scores[k])&#125;)</span><br></pre></td></tr></table></figure><p>获得预测结果之后使用以下代码来评测:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from pycocotools.coco impor COCO</span><br><span class="line">from pycocotools.cocoeval import COCOeval</span><br><span class="line"></span><br><span class="line">cocoGt = COCO(PATH_TO_GT_JSON)    </span><br><span class="line">cocoDt = cocoGt.loadRes(PATH_TO_RESULT_JSON)</span><br><span class="line"># 最后一个参数是数据格式，按照具体任务来指定</span><br><span class="line">cocoEval = COCOeval(cocoGt, cocoDt, &quot;segm&quot;)</span><br><span class="line">cocoEval.evaluate()</span><br><span class="line">cocoEval.accumulate()</span><br><span class="line">cocoEval.summarize()</span><br></pre></td></tr></table></figure><h2 id="直接评测"><a href="#直接评测" class="headerlink" title="直接评测"></a>直接评测</h2><p>主要思想还是先创建一个空的 COCOeval 对象，然后每次预测的时候更新结果到其中，最后再直接评测，这个实现起来可能比上面的要麻烦，这里直接贴上 DETR 里面的部分代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每次预测后更新结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, predictions)</span>:</span></span><br><span class="line">    img_ids = [predictions[<span class="string">'image_id'</span>]]</span><br><span class="line">    self.img_ids.extend(img_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iou_type <span class="keyword">in</span> self.iou_types:</span><br><span class="line">        results = self.prepare(predictions, iou_type)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># suppress pycocotools prints</span></span><br><span class="line">        <span class="keyword">with</span> open(os.devnull, <span class="string">'w'</span>) <span class="keyword">as</span> devnull:</span><br><span class="line">            <span class="keyword">with</span> contextlib.redirect_stdout(devnull):</span><br><span class="line">                <span class="comment"># 将预测结果转换为 COCO 的对象</span></span><br><span class="line">                coco_dt = COCO.loadRes(self.coco_gt, results) <span class="keyword">if</span> results <span class="keyword">else</span> COCO()</span><br><span class="line">        coco_eval = self.coco_eval[iou_type]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将结果保存到 COCOeval 对象中</span></span><br><span class="line">        coco_eval.cocoDt = coco_dt</span><br><span class="line">        coco_eval.params.imgIds = list(img_ids)</span><br><span class="line">        <span class="comment"># 进行 evaluate 操作，作用等同于上面代码段中的 cocoEval.evaluate()</span></span><br><span class="line">        img_ids, eval_imgs = evaluate(coco_eval)</span><br><span class="line"></span><br><span class="line">        self.eval_imgs[iou_type].append(eval_imgs)</span><br></pre></td></tr></table></figure><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>直接评测的完整代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(sess, net, inputs, test_collect, data_loader, base_ds)</span>:</span></span><br><span class="line"></span><br><span class="line">    iou_types = (<span class="string">'segm'</span>, )</span><br><span class="line">    coco_evaluator = CocoEvaluator(base_ds, iou_types)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> im, image_id <span class="keyword">in</span> data_loader:</span><br><span class="line">        cat_prob, boxes, seg_pred, masks = im_detect(sess, net, inputs, im, test_collect)</span><br><span class="line"></span><br><span class="line">        cls_scores = cat_prob[:, <span class="number">1</span>]</span><br><span class="line">        segmaps = np.zeros([len(seg_pred), im.shape[<span class="number">0</span>], im.shape[<span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(len(seg_pred)):</span><br><span class="line">            img_for_single_instance = copy.deepcopy(im)</span><br><span class="line"></span><br><span class="line">            segmap = seg_pred[k, :, :, <span class="number">1</span>]</span><br><span class="line">            segmap = cv2.resize(segmap, (img_for_single_instance.shape[<span class="number">1</span>], img_for_single_instance.shape[<span class="number">0</span>]),</span><br><span class="line">                                interpolation=cv2.INTER_LANCZOS4)</span><br><span class="line">            segmap_masked = segmap * masks[k]</span><br><span class="line">            segmaps[k] = segmap_masked</span><br><span class="line"></span><br><span class="line">        res = &#123;<span class="string">'scores'</span>: cls_scores, <span class="string">'segmaps'</span>: segmaps, <span class="string">'image_id'</span>: image_id&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每次预测后更新结果</span></span><br><span class="line">        <span class="keyword">if</span> coco_evaluator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            coco_evaluator.update(res)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># accumulate predictions from all images</span></span><br><span class="line">    <span class="keyword">if</span> coco_evaluator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        coco_evaluator.synchronize_between_processes()</span><br><span class="line">        coco_evaluator.accumulate()</span><br><span class="line">        coco_evaluator.summarize()</span><br></pre></td></tr></table></figure><p>DETR COCOEvaluator，有删改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">COCO evaluator that works in distributed mode.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Mostly copy-paste from https://github.com/pytorch/vision/blob/edfd5a7/references/detection/coco_eval.py</span></span><br><span class="line"><span class="string">The difference is that there is less copy-pasting from pycocotools</span></span><br><span class="line"><span class="string">in the end of the file, as python3 can suppress prints with contextlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> contextlib</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pycocotools.cocoeval <span class="keyword">import</span> COCOeval</span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO</span><br><span class="line"><span class="keyword">import</span> pycocotools.mask <span class="keyword">as</span> mask_util</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CocoEvaluator</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, coco_gt, iou_types)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(iou_types, (list, tuple))</span><br><span class="line">        coco_gt = copy.deepcopy(coco_gt)</span><br><span class="line">        self.coco_gt = coco_gt</span><br><span class="line"></span><br><span class="line">        self.iou_types = iou_types</span><br><span class="line">        self.coco_eval = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> iou_type <span class="keyword">in</span> iou_types:</span><br><span class="line">            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)</span><br><span class="line"></span><br><span class="line">        self.img_ids = []</span><br><span class="line">        self.eval_imgs = &#123;k: [] <span class="keyword">for</span> k <span class="keyword">in</span> iou_types&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, predictions)</span>:</span></span><br><span class="line">        img_ids = [predictions[<span class="string">'image_id'</span>]]</span><br><span class="line">        self.img_ids.extend(img_ids)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> iou_type <span class="keyword">in</span> self.iou_types:</span><br><span class="line">            results = self.prepare(predictions, iou_type)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># suppress pycocotools prints</span></span><br><span class="line">            <span class="keyword">with</span> open(os.devnull, <span class="string">'w'</span>) <span class="keyword">as</span> devnull:</span><br><span class="line">                <span class="keyword">with</span> contextlib.redirect_stdout(devnull):</span><br><span class="line">                    coco_dt = COCO.loadRes(self.coco_gt, results) <span class="keyword">if</span> results <span class="keyword">else</span> COCO()</span><br><span class="line">            coco_eval = self.coco_eval[iou_type]</span><br><span class="line"></span><br><span class="line">            coco_eval.cocoDt = coco_dt</span><br><span class="line">            coco_eval.params.imgIds = list(img_ids)</span><br><span class="line">            img_ids, eval_imgs = evaluate(coco_eval)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print('eeee', eval_imgs)</span></span><br><span class="line">            self.eval_imgs[iou_type].append(eval_imgs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">synchronize_between_processes</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> iou_type <span class="keyword">in</span> self.iou_types:</span><br><span class="line">            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], <span class="number">2</span>)</span><br><span class="line">            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> coco_eval <span class="keyword">in</span> self.coco_eval.values():</span><br><span class="line">            coco_eval.accumulate()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">summarize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> iou_type, coco_eval <span class="keyword">in</span> self.coco_eval.items():</span><br><span class="line">            print(<span class="string">"IoU metric: &#123;&#125;"</span>.format(iou_type))</span><br><span class="line">            coco_eval.summarize()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare</span><span class="params">(self, predictions, iou_type)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> iou_type == <span class="string">"bbox"</span>:</span><br><span class="line">            <span class="keyword">return</span> self.prepare_for_coco_detection(predictions)</span><br><span class="line">        <span class="keyword">elif</span> iou_type == <span class="string">"segm"</span>:</span><br><span class="line">            <span class="keyword">return</span> self.prepare_for_coco_segmentation(predictions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unknown iou type &#123;&#125;"</span>.format(iou_type))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_for_coco_detection</span><span class="params">(self, predictions)</span>:</span></span><br><span class="line">        coco_results = []</span><br><span class="line">        <span class="keyword">for</span> original_id, prediction <span class="keyword">in</span> predictions.items():</span><br><span class="line">            <span class="keyword">if</span> len(prediction) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            boxes = prediction[<span class="string">"boxes"</span>]</span><br><span class="line">            boxes = convert_to_xywh(boxes).tolist()</span><br><span class="line">            scores = prediction[<span class="string">"scores"</span>].tolist()</span><br><span class="line">            labels = prediction[<span class="string">"labels"</span>].tolist()</span><br><span class="line"></span><br><span class="line">            coco_results.extend(</span><br><span class="line">                [</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="string">"image_id"</span>: original_id,</span><br><span class="line">                        <span class="string">"category_id"</span>: labels[k],</span><br><span class="line">                        <span class="string">"bbox"</span>: box,</span><br><span class="line">                        <span class="string">"score"</span>: scores[k],</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">for</span> k, box <span class="keyword">in</span> enumerate(boxes)</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> coco_results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_for_coco_segmentation</span><span class="params">(self, predictions)</span>:</span></span><br><span class="line">        coco_results = []</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(len(predictions[<span class="string">'scores'</span>])):</span><br><span class="line">            masks = predictions[<span class="string">'segmaps'</span>][idx]</span><br><span class="line">            masks = masks &gt; <span class="number">0.5</span></span><br><span class="line">            scores = predictions[<span class="string">'scores'</span>][idx]</span><br><span class="line">            rle = mask_util.encode(np.array(masks[:, :, np.newaxis], dtype=np.uint8, order=<span class="string">"F"</span>))[<span class="number">0</span>]</span><br><span class="line">            rle[<span class="string">'counts'</span>] = rle[<span class="string">"counts"</span>].decode(<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line">            coco_results.append(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">"image_id"</span>: predictions[<span class="string">'image_id'</span>],</span><br><span class="line">                    <span class="string">"category_id"</span>: <span class="number">1</span>,</span><br><span class="line">                    <span class="string">"segmentation"</span>: rle,</span><br><span class="line">                    <span class="string">"score"</span>: scores,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> coco_results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_to_xywh</span><span class="params">(boxes)</span>:</span></span><br><span class="line">    xmin, ymin, xmax, ymax = boxes.unbind(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> np.stack((xmin, ymin, xmax - xmin, ymax - ymin), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_common_coco_eval</span><span class="params">(coco_eval, img_ids, eval_imgs)</span>:</span></span><br><span class="line">    img_ids = list(img_ids)</span><br><span class="line">    eval_imgs = list(eval_imgs.flatten())</span><br><span class="line"></span><br><span class="line">    coco_eval.evalImgs = eval_imgs</span><br><span class="line">    coco_eval.params.imgIds = img_ids</span><br><span class="line">    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># From pycocotools, just removed the prints and fixed</span></span><br><span class="line"><span class="comment"># a Python3 bug about unicode not defined</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># tic = time.time()</span></span><br><span class="line">    <span class="comment"># print('Running per image evaluation...')</span></span><br><span class="line">    p = self.params</span><br><span class="line">    <span class="comment"># add backward compatibility if useSegm is specified in params</span></span><br><span class="line">    <span class="keyword">if</span> p.useSegm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p.iouType = <span class="string">'segm'</span> <span class="keyword">if</span> p.useSegm == <span class="number">1</span> <span class="keyword">else</span> <span class="string">'bbox'</span></span><br><span class="line">        print(<span class="string">'useSegm (deprecated) is not None. Running &#123;&#125; evaluation'</span>.format(p.iouType))</span><br><span class="line">    <span class="comment"># print('Evaluate annotation type *&#123;&#125;*'.format(p.iouType))</span></span><br><span class="line">    p.imgIds = list(np.unique(p.imgIds))</span><br><span class="line">    <span class="keyword">if</span> p.useCats:</span><br><span class="line">        p.catIds = list(np.unique(p.catIds))</span><br><span class="line">    p.maxDets = sorted(p.maxDets)</span><br><span class="line">    self.params = p</span><br><span class="line"></span><br><span class="line">    self._prepare()</span><br><span class="line">    <span class="comment"># loop through images, area range, max detection number</span></span><br><span class="line">    catIds = p.catIds <span class="keyword">if</span> p.useCats <span class="keyword">else</span> [<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> p.iouType == <span class="string">'segm'</span> <span class="keyword">or</span> p.iouType == <span class="string">'bbox'</span>:</span><br><span class="line">        computeIoU = self.computeIoU</span><br><span class="line">    <span class="keyword">elif</span> p.iouType == <span class="string">'keypoints'</span>:</span><br><span class="line">        computeIoU = self.computeOks</span><br><span class="line">    self.ious = &#123;</span><br><span class="line">        (imgId, catId): computeIoU(imgId, catId)</span><br><span class="line">        <span class="keyword">for</span> imgId <span class="keyword">in</span> p.imgIds</span><br><span class="line">        <span class="keyword">for</span> catId <span class="keyword">in</span> catIds&#125;</span><br><span class="line"></span><br><span class="line">    evaluateImg = self.evaluateImg</span><br><span class="line">    maxDet = p.maxDets[<span class="number">-1</span>]</span><br><span class="line">    evalImgs = [</span><br><span class="line">        evaluateImg(imgId, catId, areaRng, maxDet)</span><br><span class="line">        <span class="keyword">for</span> catId <span class="keyword">in</span> catIds</span><br><span class="line">        <span class="keyword">for</span> areaRng <span class="keyword">in</span> p.areaRng</span><br><span class="line">        <span class="keyword">for</span> imgId <span class="keyword">in</span> p.imgIds</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># this is NOT in the pycocotools code, but could be done outside</span></span><br><span class="line">    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))</span><br><span class="line">    self._paramsEval = copy.deepcopy(self.params)</span><br><span class="line">    <span class="comment"># toc = time.time()</span></span><br><span class="line">    <span class="comment"># print('DONE (t=&#123;:0.2f&#125;s).'.format(toc-tic))</span></span><br><span class="line">    <span class="keyword">return</span> p.imgIds, evalImgs</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################################</span></span><br><span class="line"><span class="comment"># end of straight copy from pycocotools, just removing the prints</span></span><br><span class="line"><span class="comment">#################################################################</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目标检测、实例分割以及关键点检测都可以使用 COCO API 来进行评测，主要方法有两种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;按照 COCO 的格式保存预测的结果，然后评测&lt;/li&gt;
&lt;li&gt;不保存预测结果，直接评测&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;保存预测结果&quot;&gt;&lt;a hr
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>获取 COCO 数据集分布信息</title>
    <link href="https://blog.patrickcty.cc/2021/02/09/%E8%8E%B7%E5%8F%96COCO%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E5%B8%83%E4%BF%A1%E6%81%AF/"/>
    <id>https://blog.patrickcty.cc/2021/02/09/获取COCO数据集分布信息/</id>
    <published>2021-02-09T14:04:03.000Z</published>
    <updated>2021-02-09T14:30:53.377Z</updated>
    
    <content type="html"><![CDATA[<p>使用 COCO API 来获得数据集中的数据分布信息，参考<a href="https://blog.csdn.net/gzj2013/article/details/82425954" target="_blank" rel="noopener">CSDN 文章</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO</span><br><span class="line"></span><br><span class="line">coco=COCO(PATH_TO_COCO_JSON)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有类别对应的名称与 id，一共有 80 类，id 号则是 1~90</span></span><br><span class="line">cat_info = [(c[<span class="string">'id'</span>], c[<span class="string">'name'</span>]) <span class="keyword">for</span> c <span class="keyword">in</span> coco.cats.values()]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> cat_id, cat_name <span class="keyword">in</span> cat_info:</span><br><span class="line">    anns = coco.catToImgs[cat_id]</span><br><span class="line">    print(<span class="string">"&#123;&#125; | &#123;&#125; | &#123;:.2%&#125; | &#123;&#125; | &#123;:.2%&#125;"</span>.format(cat_name, len(set(anns)), len(set(anns)) / len(coco.imgs), len(anns), len(anns) / len(coco.anns)))</span><br></pre></td></tr></table></figure><p>其中 val2017 分布如下：</p><table><thead><tr><th style="text-align:center">类别名</th><th style="text-align:center">图像数</th><th style="text-align:center">类别比例</th><th style="text-align:center">框数</th><th style="text-align:center">框比例</th></tr></thead><tbody><tr><td style="text-align:center">person</td><td style="text-align:center">2693</td><td style="text-align:center">53.86%</td><td style="text-align:center">11004</td><td style="text-align:center">29.92%</td></tr><tr><td style="text-align:center">bicycle</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">316</td><td style="text-align:center">0.86%</td></tr><tr><td style="text-align:center">car</td><td style="text-align:center">535</td><td style="text-align:center">10.70%</td><td style="text-align:center">1932</td><td style="text-align:center">5.25%</td></tr><tr><td style="text-align:center">motorcycle</td><td style="text-align:center">159</td><td style="text-align:center">3.18%</td><td style="text-align:center">371</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">airplane</td><td style="text-align:center">97</td><td style="text-align:center">1.94%</td><td style="text-align:center">143</td><td style="text-align:center">0.39%</td></tr><tr><td style="text-align:center">bus</td><td style="text-align:center">189</td><td style="text-align:center">3.78%</td><td style="text-align:center">285</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">train</td><td style="text-align:center">157</td><td style="text-align:center">3.14%</td><td style="text-align:center">190</td><td style="text-align:center">0.52%</td></tr><tr><td style="text-align:center">truck</td><td style="text-align:center">250</td><td style="text-align:center">5.00%</td><td style="text-align:center">415</td><td style="text-align:center">1.13%</td></tr><tr><td style="text-align:center">boat</td><td style="text-align:center">121</td><td style="text-align:center">2.42%</td><td style="text-align:center">430</td><td style="text-align:center">1.17%</td></tr><tr><td style="text-align:center">traffic light</td><td style="text-align:center">191</td><td style="text-align:center">3.82%</td><td style="text-align:center">637</td><td style="text-align:center">1.73%</td></tr><tr><td style="text-align:center">fire hydrant</td><td style="text-align:center">86</td><td style="text-align:center">1.72%</td><td style="text-align:center">101</td><td style="text-align:center">0.27%</td></tr><tr><td style="text-align:center">stop sign</td><td style="text-align:center">69</td><td style="text-align:center">1.38%</td><td style="text-align:center">75</td><td style="text-align:center">0.20%</td></tr><tr><td style="text-align:center">parking meter</td><td style="text-align:center">37</td><td style="text-align:center">0.74%</td><td style="text-align:center">60</td><td style="text-align:center">0.16%</td></tr><tr><td style="text-align:center">bench</td><td style="text-align:center">235</td><td style="text-align:center">4.70%</td><td style="text-align:center">413</td><td style="text-align:center">1.12%</td></tr><tr><td style="text-align:center">bird</td><td style="text-align:center">125</td><td style="text-align:center">2.50%</td><td style="text-align:center">440</td><td style="text-align:center">1.20%</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">184</td><td style="text-align:center">3.68%</td><td style="text-align:center">202</td><td style="text-align:center">0.55%</td></tr><tr><td style="text-align:center">dog</td><td style="text-align:center">177</td><td style="text-align:center">3.54%</td><td style="text-align:center">218</td><td style="text-align:center">0.59%</td></tr><tr><td style="text-align:center">horse</td><td style="text-align:center">128</td><td style="text-align:center">2.56%</td><td style="text-align:center">273</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">sheep</td><td style="text-align:center">65</td><td style="text-align:center">1.30%</td><td style="text-align:center">361</td><td style="text-align:center">0.98%</td></tr><tr><td style="text-align:center">cow</td><td style="text-align:center">87</td><td style="text-align:center">1.74%</td><td style="text-align:center">380</td><td style="text-align:center">1.03%</td></tr><tr><td style="text-align:center">elephant</td><td style="text-align:center">89</td><td style="text-align:center">1.78%</td><td style="text-align:center">255</td><td style="text-align:center">0.69%</td></tr><tr><td style="text-align:center">bear</td><td style="text-align:center">49</td><td style="text-align:center">0.98%</td><td style="text-align:center">71</td><td style="text-align:center">0.19%</td></tr><tr><td style="text-align:center">zebra</td><td style="text-align:center">85</td><td style="text-align:center">1.70%</td><td style="text-align:center">268</td><td style="text-align:center">0.73%</td></tr><tr><td style="text-align:center">giraffe</td><td style="text-align:center">101</td><td style="text-align:center">2.02%</td><td style="text-align:center">232</td><td style="text-align:center">0.63%</td></tr><tr><td style="text-align:center">backpack</td><td style="text-align:center">228</td><td style="text-align:center">4.56%</td><td style="text-align:center">371</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">umbrella</td><td style="text-align:center">174</td><td style="text-align:center">3.48%</td><td style="text-align:center">413</td><td style="text-align:center">1.12%</td></tr><tr><td style="text-align:center">handbag</td><td style="text-align:center">292</td><td style="text-align:center">5.84%</td><td style="text-align:center">540</td><td style="text-align:center">1.47%</td></tr><tr><td style="text-align:center">tie</td><td style="text-align:center">145</td><td style="text-align:center">2.90%</td><td style="text-align:center">254</td><td style="text-align:center">0.69%</td></tr><tr><td style="text-align:center">suitcase</td><td style="text-align:center">105</td><td style="text-align:center">2.10%</td><td style="text-align:center">303</td><td style="text-align:center">0.82%</td></tr><tr><td style="text-align:center">frisbee</td><td style="text-align:center">84</td><td style="text-align:center">1.68%</td><td style="text-align:center">115</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">skis</td><td style="text-align:center">120</td><td style="text-align:center">2.40%</td><td style="text-align:center">241</td><td style="text-align:center">0.66%</td></tr><tr><td style="text-align:center">snowboard</td><td style="text-align:center">49</td><td style="text-align:center">0.98%</td><td style="text-align:center">69</td><td style="text-align:center">0.19%</td></tr><tr><td style="text-align:center">sports ball</td><td style="text-align:center">169</td><td style="text-align:center">3.38%</td><td style="text-align:center">263</td><td style="text-align:center">0.72%</td></tr><tr><td style="text-align:center">kite</td><td style="text-align:center">91</td><td style="text-align:center">1.82%</td><td style="text-align:center">336</td><td style="text-align:center">0.91%</td></tr><tr><td style="text-align:center">baseball bat</td><td style="text-align:center">97</td><td style="text-align:center">1.94%</td><td style="text-align:center">146</td><td style="text-align:center">0.40%</td></tr><tr><td style="text-align:center">baseball glove</td><td style="text-align:center">100</td><td style="text-align:center">2.00%</td><td style="text-align:center">148</td><td style="text-align:center">0.40%</td></tr><tr><td style="text-align:center">skateboard</td><td style="text-align:center">127</td><td style="text-align:center">2.54%</td><td style="text-align:center">179</td><td style="text-align:center">0.49%</td></tr><tr><td style="text-align:center">surfboard</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">269</td><td style="text-align:center">0.73%</td></tr><tr><td style="text-align:center">tennis racket</td><td style="text-align:center">167</td><td style="text-align:center">3.34%</td><td style="text-align:center">225</td><td style="text-align:center">0.61%</td></tr><tr><td style="text-align:center">bottle</td><td style="text-align:center">379</td><td style="text-align:center">7.58%</td><td style="text-align:center">1025</td><td style="text-align:center">2.79%</td></tr><tr><td style="text-align:center">wine glass</td><td style="text-align:center">110</td><td style="text-align:center">2.20%</td><td style="text-align:center">343</td><td style="text-align:center">0.93%</td></tr><tr><td style="text-align:center">cup</td><td style="text-align:center">390</td><td style="text-align:center">7.80%</td><td style="text-align:center">899</td><td style="text-align:center">2.44%</td></tr><tr><td style="text-align:center">fork</td><td style="text-align:center">155</td><td style="text-align:center">3.10%</td><td style="text-align:center">215</td><td style="text-align:center">0.58%</td></tr><tr><td style="text-align:center">knife</td><td style="text-align:center">181</td><td style="text-align:center">3.62%</td><td style="text-align:center">326</td><td style="text-align:center">0.89%</td></tr><tr><td style="text-align:center">spoon</td><td style="text-align:center">153</td><td style="text-align:center">3.06%</td><td style="text-align:center">253</td><td style="text-align:center">0.69%</td></tr><tr><td style="text-align:center">bowl</td><td style="text-align:center">314</td><td style="text-align:center">6.28%</td><td style="text-align:center">626</td><td style="text-align:center">1.70%</td></tr><tr><td style="text-align:center">banana</td><td style="text-align:center">103</td><td style="text-align:center">2.06%</td><td style="text-align:center">379</td><td style="text-align:center">1.03%</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">76</td><td style="text-align:center">1.52%</td><td style="text-align:center">239</td><td style="text-align:center">0.65%</td></tr><tr><td style="text-align:center">sandwich</td><td style="text-align:center">98</td><td style="text-align:center">1.96%</td><td style="text-align:center">177</td><td style="text-align:center">0.48%</td></tr><tr><td style="text-align:center">orange</td><td style="text-align:center">85</td><td style="text-align:center">1.70%</td><td style="text-align:center">287</td><td style="text-align:center">0.78%</td></tr><tr><td style="text-align:center">broccoli</td><td style="text-align:center">71</td><td style="text-align:center">1.42%</td><td style="text-align:center">316</td><td style="text-align:center">0.86%</td></tr><tr><td style="text-align:center">carrot</td><td style="text-align:center">81</td><td style="text-align:center">1.62%</td><td style="text-align:center">371</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">hot dog</td><td style="text-align:center">51</td><td style="text-align:center">1.02%</td><td style="text-align:center">127</td><td style="text-align:center">0.35%</td></tr><tr><td style="text-align:center">pizza</td><td style="text-align:center">153</td><td style="text-align:center">3.06%</td><td style="text-align:center">285</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">donut</td><td style="text-align:center">62</td><td style="text-align:center">1.24%</td><td style="text-align:center">338</td><td style="text-align:center">0.92%</td></tr><tr><td style="text-align:center">cake</td><td style="text-align:center">124</td><td style="text-align:center">2.48%</td><td style="text-align:center">316</td><td style="text-align:center">0.86%</td></tr><tr><td style="text-align:center">chair</td><td style="text-align:center">580</td><td style="text-align:center">11.60%</td><td style="text-align:center">1791</td><td style="text-align:center">4.87%</td></tr><tr><td style="text-align:center">couch</td><td style="text-align:center">195</td><td style="text-align:center">3.90%</td><td style="text-align:center">261</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">potted plant</td><td style="text-align:center">172</td><td style="text-align:center">3.44%</td><td style="text-align:center">343</td><td style="text-align:center">0.93%</td></tr><tr><td style="text-align:center">bed</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">163</td><td style="text-align:center">0.44%</td></tr><tr><td style="text-align:center">dining table</td><td style="text-align:center">501</td><td style="text-align:center">10.02%</td><td style="text-align:center">697</td><td style="text-align:center">1.90%</td></tr><tr><td style="text-align:center">toilet</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">179</td><td style="text-align:center">0.49%</td></tr><tr><td style="text-align:center">tv</td><td style="text-align:center">207</td><td style="text-align:center">4.14%</td><td style="text-align:center">288</td><td style="text-align:center">0.78%</td></tr><tr><td style="text-align:center">laptop</td><td style="text-align:center">183</td><td style="text-align:center">3.66%</td><td style="text-align:center">231</td><td style="text-align:center">0.63%</td></tr><tr><td style="text-align:center">mouse</td><td style="text-align:center">88</td><td style="text-align:center">1.76%</td><td style="text-align:center">106</td><td style="text-align:center">0.29%</td></tr><tr><td style="text-align:center">remote</td><td style="text-align:center">145</td><td style="text-align:center">2.90%</td><td style="text-align:center">283</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">keyboard</td><td style="text-align:center">106</td><td style="text-align:center">2.12%</td><td style="text-align:center">153</td><td style="text-align:center">0.42%</td></tr><tr><td style="text-align:center">cell phone</td><td style="text-align:center">214</td><td style="text-align:center">4.28%</td><td style="text-align:center">262</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">microwave</td><td style="text-align:center">54</td><td style="text-align:center">1.08%</td><td style="text-align:center">55</td><td style="text-align:center">0.15%</td></tr><tr><td style="text-align:center">oven</td><td style="text-align:center">115</td><td style="text-align:center">2.30%</td><td style="text-align:center">143</td><td style="text-align:center">0.39%</td></tr><tr><td style="text-align:center">toaster</td><td style="text-align:center">8</td><td style="text-align:center">0.16%</td><td style="text-align:center">9</td><td style="text-align:center">0.02%</td></tr><tr><td style="text-align:center">sink</td><td style="text-align:center">187</td><td style="text-align:center">3.74%</td><td style="text-align:center">225</td><td style="text-align:center">0.61%</td></tr><tr><td style="text-align:center">refrigerator</td><td style="text-align:center">101</td><td style="text-align:center">2.02%</td><td style="text-align:center">126</td><td style="text-align:center">0.34%</td></tr><tr><td style="text-align:center">book</td><td style="text-align:center">230</td><td style="text-align:center">4.60%</td><td style="text-align:center">1161</td><td style="text-align:center">3.16%</td></tr><tr><td style="text-align:center">clock</td><td style="text-align:center">204</td><td style="text-align:center">4.08%</td><td style="text-align:center">267</td><td style="text-align:center">0.73%</td></tr><tr><td style="text-align:center">vase</td><td style="text-align:center">137</td><td style="text-align:center">2.74%</td><td style="text-align:center">277</td><td style="text-align:center">0.75%</td></tr><tr><td style="text-align:center">scissors</td><td style="text-align:center">28</td><td style="text-align:center">0.56%</td><td style="text-align:center">36</td><td style="text-align:center">0.10%</td></tr><tr><td style="text-align:center">teddy bear</td><td style="text-align:center">94</td><td style="text-align:center">1.88%</td><td style="text-align:center">191</td><td style="text-align:center">0.52%</td></tr><tr><td style="text-align:center">hair drier</td><td style="text-align:center">9</td><td style="text-align:center">0.18%</td><td style="text-align:center">11</td><td style="text-align:center">0.03%</td></tr><tr><td style="text-align:center">toothbrush</td><td style="text-align:center">34</td><td style="text-align:center">0.68%</td><td style="text-align:center">57</td><td style="text-align:center">0.15%</td></tr></tbody></table><p>train2017 分布如下</p><table><thead><tr><th style="text-align:center">类别名</th><th style="text-align:center">图像数</th><th style="text-align:center">类别比例</th><th style="text-align:center">框数</th><th style="text-align:center">框比例</th></tr></thead><tbody><tr><td style="text-align:center">person</td><td style="text-align:center">64115</td><td style="text-align:center">54.20%</td><td style="text-align:center">262465</td><td style="text-align:center">30.52%</td></tr><tr><td style="text-align:center">bicycle</td><td style="text-align:center">3252</td><td style="text-align:center">2.75%</td><td style="text-align:center">7113</td><td style="text-align:center">0.83%</td></tr><tr><td style="text-align:center">car</td><td style="text-align:center">12251</td><td style="text-align:center">10.36%</td><td style="text-align:center">43867</td><td style="text-align:center">5.10%</td></tr><tr><td style="text-align:center">motorcycle</td><td style="text-align:center">3502</td><td style="text-align:center">2.96%</td><td style="text-align:center">8725</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">airplane</td><td style="text-align:center">2986</td><td style="text-align:center">2.52%</td><td style="text-align:center">5135</td><td style="text-align:center">0.60%</td></tr><tr><td style="text-align:center">bus</td><td style="text-align:center">3952</td><td style="text-align:center">3.34%</td><td style="text-align:center">6069</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">train</td><td style="text-align:center">3588</td><td style="text-align:center">3.03%</td><td style="text-align:center">4571</td><td style="text-align:center">0.53%</td></tr><tr><td style="text-align:center">truck</td><td style="text-align:center">6127</td><td style="text-align:center">5.18%</td><td style="text-align:center">9973</td><td style="text-align:center">1.16%</td></tr><tr><td style="text-align:center">boat</td><td style="text-align:center">3025</td><td style="text-align:center">2.56%</td><td style="text-align:center">10759</td><td style="text-align:center">1.25%</td></tr><tr><td style="text-align:center">traffic light</td><td style="text-align:center">4139</td><td style="text-align:center">3.50%</td><td style="text-align:center">12884</td><td style="text-align:center">1.50%</td></tr><tr><td style="text-align:center">fire hydrant</td><td style="text-align:center">1711</td><td style="text-align:center">1.45%</td><td style="text-align:center">1865</td><td style="text-align:center">0.22%</td></tr><tr><td style="text-align:center">stop sign</td><td style="text-align:center">1734</td><td style="text-align:center">1.47%</td><td style="text-align:center">1983</td><td style="text-align:center">0.23%</td></tr><tr><td style="text-align:center">parking meter</td><td style="text-align:center">705</td><td style="text-align:center">0.60%</td><td style="text-align:center">1285</td><td style="text-align:center">0.15%</td></tr><tr><td style="text-align:center">bench</td><td style="text-align:center">5570</td><td style="text-align:center">4.71%</td><td style="text-align:center">9838</td><td style="text-align:center">1.14%</td></tr><tr><td style="text-align:center">bird</td><td style="text-align:center">3237</td><td style="text-align:center">2.74%</td><td style="text-align:center">10806</td><td style="text-align:center">1.26%</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">4114</td><td style="text-align:center">3.48%</td><td style="text-align:center">4768</td><td style="text-align:center">0.55%</td></tr><tr><td style="text-align:center">dog</td><td style="text-align:center">4385</td><td style="text-align:center">3.71%</td><td style="text-align:center">5508</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">horse</td><td style="text-align:center">2941</td><td style="text-align:center">2.49%</td><td style="text-align:center">6587</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">sheep</td><td style="text-align:center">1529</td><td style="text-align:center">1.29%</td><td style="text-align:center">9509</td><td style="text-align:center">1.11%</td></tr><tr><td style="text-align:center">cow</td><td style="text-align:center">1968</td><td style="text-align:center">1.66%</td><td style="text-align:center">8147</td><td style="text-align:center">0.95%</td></tr><tr><td style="text-align:center">elephant</td><td style="text-align:center">2143</td><td style="text-align:center">1.81%</td><td style="text-align:center">5513</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">bear</td><td style="text-align:center">960</td><td style="text-align:center">0.81%</td><td style="text-align:center">1294</td><td style="text-align:center">0.15%</td></tr><tr><td style="text-align:center">zebra</td><td style="text-align:center">1916</td><td style="text-align:center">1.62%</td><td style="text-align:center">5303</td><td style="text-align:center">0.62%</td></tr><tr><td style="text-align:center">giraffe</td><td style="text-align:center">2546</td><td style="text-align:center">2.15%</td><td style="text-align:center">5131</td><td style="text-align:center">0.60%</td></tr><tr><td style="text-align:center">backpack</td><td style="text-align:center">5528</td><td style="text-align:center">4.67%</td><td style="text-align:center">8720</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">umbrella</td><td style="text-align:center">3968</td><td style="text-align:center">3.35%</td><td style="text-align:center">11431</td><td style="text-align:center">1.33%</td></tr><tr><td style="text-align:center">handbag</td><td style="text-align:center">6841</td><td style="text-align:center">5.78%</td><td style="text-align:center">12354</td><td style="text-align:center">1.44%</td></tr><tr><td style="text-align:center">tie</td><td style="text-align:center">3810</td><td style="text-align:center">3.22%</td><td style="text-align:center">6496</td><td style="text-align:center">0.76%</td></tr><tr><td style="text-align:center">suitcase</td><td style="text-align:center">2402</td><td style="text-align:center">2.03%</td><td style="text-align:center">6192</td><td style="text-align:center">0.72%</td></tr><tr><td style="text-align:center">frisbee</td><td style="text-align:center">2184</td><td style="text-align:center">1.85%</td><td style="text-align:center">2682</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">skis</td><td style="text-align:center">3082</td><td style="text-align:center">2.61%</td><td style="text-align:center">6646</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">snowboard</td><td style="text-align:center">1654</td><td style="text-align:center">1.40%</td><td style="text-align:center">2685</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">sports ball</td><td style="text-align:center">4262</td><td style="text-align:center">3.60%</td><td style="text-align:center">6347</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">kite</td><td style="text-align:center">2261</td><td style="text-align:center">1.91%</td><td style="text-align:center">9076</td><td style="text-align:center">1.06%</td></tr><tr><td style="text-align:center">baseball bat</td><td style="text-align:center">2506</td><td style="text-align:center">2.12%</td><td style="text-align:center">3276</td><td style="text-align:center">0.38%</td></tr><tr><td style="text-align:center">baseball glove</td><td style="text-align:center">2629</td><td style="text-align:center">2.22%</td><td style="text-align:center">3747</td><td style="text-align:center">0.44%</td></tr><tr><td style="text-align:center">skateboard</td><td style="text-align:center">3476</td><td style="text-align:center">2.94%</td><td style="text-align:center">5543</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">surfboard</td><td style="text-align:center">3486</td><td style="text-align:center">2.95%</td><td style="text-align:center">6126</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">tennis racket</td><td style="text-align:center">3394</td><td style="text-align:center">2.87%</td><td style="text-align:center">4812</td><td style="text-align:center">0.56%</td></tr><tr><td style="text-align:center">bottle</td><td style="text-align:center">8501</td><td style="text-align:center">7.19%</td><td style="text-align:center">24342</td><td style="text-align:center">2.83%</td></tr><tr><td style="text-align:center">wine glass</td><td style="text-align:center">2533</td><td style="text-align:center">2.14%</td><td style="text-align:center">7913</td><td style="text-align:center">0.92%</td></tr><tr><td style="text-align:center">cup</td><td style="text-align:center">9189</td><td style="text-align:center">7.77%</td><td style="text-align:center">20650</td><td style="text-align:center">2.40%</td></tr><tr><td style="text-align:center">fork</td><td style="text-align:center">3555</td><td style="text-align:center">3.01%</td><td style="text-align:center">5479</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">knife</td><td style="text-align:center">4326</td><td style="text-align:center">3.66%</td><td style="text-align:center">7770</td><td style="text-align:center">0.90%</td></tr><tr><td style="text-align:center">spoon</td><td style="text-align:center">3529</td><td style="text-align:center">2.98%</td><td style="text-align:center">6165</td><td style="text-align:center">0.72%</td></tr><tr><td style="text-align:center">bowl</td><td style="text-align:center">7111</td><td style="text-align:center">6.01%</td><td style="text-align:center">14358</td><td style="text-align:center">1.67%</td></tr><tr><td style="text-align:center">banana</td><td style="text-align:center">2243</td><td style="text-align:center">1.90%</td><td style="text-align:center">9458</td><td style="text-align:center">1.10%</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">1586</td><td style="text-align:center">1.34%</td><td style="text-align:center">5851</td><td style="text-align:center">0.68%</td></tr><tr><td style="text-align:center">sandwich</td><td style="text-align:center">2365</td><td style="text-align:center">2.00%</td><td style="text-align:center">4373</td><td style="text-align:center">0.51%</td></tr><tr><td style="text-align:center">orange</td><td style="text-align:center">1699</td><td style="text-align:center">1.44%</td><td style="text-align:center">6399</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">broccoli</td><td style="text-align:center">1939</td><td style="text-align:center">1.64%</td><td style="text-align:center">7308</td><td style="text-align:center">0.85%</td></tr><tr><td style="text-align:center">carrot</td><td style="text-align:center">1683</td><td style="text-align:center">1.42%</td><td style="text-align:center">7852</td><td style="text-align:center">0.91%</td></tr><tr><td style="text-align:center">hot dog</td><td style="text-align:center">1222</td><td style="text-align:center">1.03%</td><td style="text-align:center">2918</td><td style="text-align:center">0.34%</td></tr><tr><td style="text-align:center">pizza</td><td style="text-align:center">3166</td><td style="text-align:center">2.68%</td><td style="text-align:center">5821</td><td style="text-align:center">0.68%</td></tr><tr><td style="text-align:center">donut</td><td style="text-align:center">1523</td><td style="text-align:center">1.29%</td><td style="text-align:center">7179</td><td style="text-align:center">0.83%</td></tr><tr><td style="text-align:center">cake</td><td style="text-align:center">2925</td><td style="text-align:center">2.47%</td><td style="text-align:center">6353</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">chair</td><td style="text-align:center">12774</td><td style="text-align:center">10.80%</td><td style="text-align:center">38491</td><td style="text-align:center">4.48%</td></tr><tr><td style="text-align:center">couch</td><td style="text-align:center">4423</td><td style="text-align:center">3.74%</td><td style="text-align:center">5779</td><td style="text-align:center">0.67%</td></tr><tr><td style="text-align:center">potted plant</td><td style="text-align:center">4452</td><td style="text-align:center">3.76%</td><td style="text-align:center">8652</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">bed</td><td style="text-align:center">3682</td><td style="text-align:center">3.11%</td><td style="text-align:center">4192</td><td style="text-align:center">0.49%</td></tr><tr><td style="text-align:center">dining table</td><td style="text-align:center">11837</td><td style="text-align:center">10.01%</td><td style="text-align:center">15714</td><td style="text-align:center">1.83%</td></tr><tr><td style="text-align:center">toilet</td><td style="text-align:center">3353</td><td style="text-align:center">2.83%</td><td style="text-align:center">4157</td><td style="text-align:center">0.48%</td></tr><tr><td style="text-align:center">tv</td><td style="text-align:center">4561</td><td style="text-align:center">3.86%</td><td style="text-align:center">5805</td><td style="text-align:center">0.67%</td></tr><tr><td style="text-align:center">laptop</td><td style="text-align:center">3524</td><td style="text-align:center">2.98%</td><td style="text-align:center">4970</td><td style="text-align:center">0.58%</td></tr><tr><td style="text-align:center">mouse</td><td style="text-align:center">1876</td><td style="text-align:center">1.59%</td><td style="text-align:center">2262</td><td style="text-align:center">0.26%</td></tr><tr><td style="text-align:center">remote</td><td style="text-align:center">3076</td><td style="text-align:center">2.60%</td><td style="text-align:center">5703</td><td style="text-align:center">0.66%</td></tr><tr><td style="text-align:center">keyboard</td><td style="text-align:center">2115</td><td style="text-align:center">1.79%</td><td style="text-align:center">2855</td><td style="text-align:center">0.33%</td></tr><tr><td style="text-align:center">cell phone</td><td style="text-align:center">4803</td><td style="text-align:center">4.06%</td><td style="text-align:center">6434</td><td style="text-align:center">0.75%</td></tr><tr><td style="text-align:center">microwave</td><td style="text-align:center">1547</td><td style="text-align:center">1.31%</td><td style="text-align:center">1673</td><td style="text-align:center">0.19%</td></tr><tr><td style="text-align:center">oven</td><td style="text-align:center">2877</td><td style="text-align:center">2.43%</td><td style="text-align:center">3334</td><td style="text-align:center">0.39%</td></tr><tr><td style="text-align:center">toaster</td><td style="text-align:center">217</td><td style="text-align:center">0.18%</td><td style="text-align:center">225</td><td style="text-align:center">0.03%</td></tr><tr><td style="text-align:center">sink</td><td style="text-align:center">4678</td><td style="text-align:center">3.95%</td><td style="text-align:center">5610</td><td style="text-align:center">0.65%</td></tr><tr><td style="text-align:center">refrigerator</td><td style="text-align:center">2360</td><td style="text-align:center">2.00%</td><td style="text-align:center">2637</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">book</td><td style="text-align:center">5332</td><td style="text-align:center">4.51%</td><td style="text-align:center">24715</td><td style="text-align:center">2.87%</td></tr><tr><td style="text-align:center">clock</td><td style="text-align:center">4659</td><td style="text-align:center">3.94%</td><td style="text-align:center">6334</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">vase</td><td style="text-align:center">3593</td><td style="text-align:center">3.04%</td><td style="text-align:center">6613</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">scissors</td><td style="text-align:center">947</td><td style="text-align:center">0.80%</td><td style="text-align:center">1481</td><td style="text-align:center">0.17%</td></tr><tr><td style="text-align:center">teddy bear</td><td style="text-align:center">2140</td><td style="text-align:center">1.81%</td><td style="text-align:center">4793</td><td style="text-align:center">0.56%</td></tr><tr><td style="text-align:center">hair drier</td><td style="text-align:center">189</td><td style="text-align:center">0.16%</td><td style="text-align:center">198</td><td style="text-align:center">0.02%</td></tr><tr><td style="text-align:center">toothbrush</td><td style="text-align:center">1007</td><td style="text-align:center">0.85%</td><td style="text-align:center">1954</td><td style="text-align:center">0.23%</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用 COCO API 来获得数据集中的数据分布信息，参考&lt;a href=&quot;https://blog.csdn.net/gzj2013/article/details/82425954&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN 文章&lt;/a&gt;。
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Linux 安装蓝牙接收器驱动</title>
    <link href="https://blog.patrickcty.cc/2021/01/26/Linux%E5%AE%89%E8%A3%85%E8%93%9D%E7%89%99%E6%8E%A5%E6%94%B6%E5%99%A8%E9%A9%B1%E5%8A%A8/"/>
    <id>https://blog.patrickcty.cc/2021/01/26/Linux安装蓝牙接收器驱动/</id>
    <published>2021-01-26T07:59:00.000Z</published>
    <updated>2021-01-26T08:04:34.482Z</updated>
    
    <content type="html"><![CDATA[<p>买了一个奥睿科的蓝牙驱动器，型号为 BTA-508，官网上介绍是只支持 Windows 系统。本来以为凉了，结果在 <a href="https://ubuntuforums.org/showthread.php?t=2453631&amp;p=14000037&amp;highlight=#post14000037" target="_blank" rel="noopener">Ubuntu 论坛</a>发现可以直接下载对应芯片的驱动。</p><blockquote><p>驱动不支持 Linux 怎么办？-&gt; 查看用的是什么芯片 -&gt; 下载安装对应芯片的驱动</p></blockquote><p>当然对于 Logitech 的 Flow 这种需要软件支持的驱动就不太好使了，硬件的驱动一般都能找到 Linux 版本。这次真的长见识了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;买了一个奥睿科的蓝牙驱动器，型号为 BTA-508，官网上介绍是只支持 Windows 系统。本来以为凉了，结果在 &lt;a href=&quot;https://ubuntuforums.org/showthread.php?t=2453631&amp;amp;p=14000037&amp;amp;h
      
    
    </summary>
    
      <category term="实用教程" scheme="https://blog.patrickcty.cc/categories/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch使用DDP进行分布式训练</title>
    <link href="https://blog.patrickcty.cc/2020/11/18/PyTorch%E4%BD%BF%E7%94%A8DDP%E8%BF%9B%E8%A1%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    <id>https://blog.patrickcty.cc/2020/11/18/PyTorch使用DDP进行分布式训练/</id>
    <published>2020-11-18T12:31:01.000Z</published>
    <updated>2020-11-18T12:35:17.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>DDP 是 DistributedDataParallel 的简写，用来进行分布式训练，可以是单主机多 GPU 也可以是多主机多 GPU，以下均从单主机多 GPU 来介绍。其原理是把模型复制到其他的 GPU 上，然后在训练的过程中汇总梯度，进行迭代，从感知上就像是增大了 N 倍的显存。</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>具体的操作是产生多个进程，每个进程在一个 GPU 上训练，然后结果自动地在主进程中进行汇总。因此启动方式需要通过 <code>torch.distributed.launch</code> 来启动，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=2 main.py</span><br></pre></td></tr></table></figure><p>其中 <code>nproc_per_node</code> 是要使用的总的 GPU 数，如果一台主机上有多个 GPU，但是只想用其中的部分来进行训练，则可以用以下命令来启动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,3,5 python -m torch.distributed.launch --nproc_per_node=3 main.py</span><br></pre></td></tr></table></figure><p>如果想调试分布式的代码，那么用以下方式来启动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NCCL_DEBUG=INFO python -m torch.distributed.launch --nproc_per_node=2 main.py</span><br></pre></td></tr></table></figure><h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><p>为了让程序能很好地与 GPU 交互，<code>torch.distributed.launch</code> 在启动进程的时候会传入 <code>local_rank</code> 参数，用来标识 GPU，因此我们要在训练脚本中加入相应的参数。值得注意的是，<code>local_rank</code> 永远是从零开始。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">'--local_rank'</span>, type=int, default=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="初始化分布式环境"><a href="#初始化分布式环境" class="headerlink" title="初始化分布式环境"></a>初始化分布式环境</h2><p>首先要初始化进程组，对于单主机来说就用下面简单的语句即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"></span><br><span class="line">opt = parser.parse_args()  <span class="comment"># 解析命令行参数</span></span><br><span class="line">torch.cuda.set_device(opt.local_rank)</span><br><span class="line">dist.init_process_group(<span class="string">'nccl'</span>)</span><br><span class="line">device = torch.device(<span class="string">f'cuda:<span class="subst">&#123;opt.local_rank&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><h2 id="构建分布式模型"><a href="#构建分布式模型" class="headerlink" title="构建分布式模型"></a>构建分布式模型</h2><p>分布式环境下默认 BN 是在主 GPU 上进行计算，然后同步到其他 GPU，因此使用普通 BN 的时候不能充分发挥分布式训练中大 batch size 的优势。如果使用 Sync BN 则会解决这个问题，这个转换也可以使用一个函数来完成。</p><p>之后把模型转换为 DDP 模型即可，注意的是每一个进程都会初始化一个 DDP 模型，<code>device_id</code> 指的是当前进程要用到的 GPU 标号列表。因为我们通常一个进程一个 GPU，因此这里使用 <code>[opt.local_rank]</code> 即可，输出的话是会输出到单个设备上，因此就不用转换为 list。需要注意的是，Sync BN 不支持单进程多 GPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line">model = ResNet()</span><br><span class="line">model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)</span><br><span class="line">model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)</span><br></pre></td></tr></table></figure><h2 id="获得分布式-data-loader"><a href="#获得分布式-data-loader" class="headerlink" title="获得分布式 data loader"></a>获得分布式 data loader</h2><p>分布式训练的过程中我们要保证每个 GPU 取到的是不同的数据，因此不能直接使用普通的 Dataloader，要传入一个 sampler 参数，具体也很简单，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"></span><br><span class="line">dataset = SomeDataset(image_root, gt_root, trainsize)</span><br><span class="line"><span class="comment"># 要从原来 dataset 得到一个分布式 sampler</span></span><br><span class="line">sampler = data.distributed.DistributedSampler(dataset)</span><br><span class="line">shuffle = <span class="literal">False</span>  <span class="comment"># sampler 与 shuffle 不兼容</span></span><br><span class="line"></span><br><span class="line">data_loader = data.DataLoader(dataset=dataset,</span><br><span class="line">                              batch_size=batchsize,</span><br><span class="line">                              shuffle=shuffle,</span><br><span class="line">                              num_workers=num_workers,</span><br><span class="line">                              pin_memory=pin_memory,</span><br><span class="line">                              sampler=sampler)</span><br><span class="line"><span class="keyword">return</span> data_loader</span><br></pre></td></tr></table></figure><p>dataloader 中要传入 sampler 作为参数，其他的注意事项在注释中</p><h2 id="获得总的-loss"><a href="#获得总的-loss" class="headerlink" title="获得总的 loss"></a>获得总的 loss</h2><p>总的准备工作都完成了，接下来就是像平常一样训练了。但是每个进程中的 loss 都是通过自己的输入得到的，如果要得到总的 loss 则需要手动同步一下，具体操作如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_tensor</span><span class="params">(tensor: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    rt = tensor.clone()</span><br><span class="line">    dist.all_reduce(rt, op=dist.ReduceOp.SUM)</span><br><span class="line">    rt /= dist.get_world_size()  <span class="comment"># 这是进程的数量</span></span><br><span class="line">    <span class="keyword">return</span> rt</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">reduced_loss1 = reduce_tensor(loss1.data).item()</span><br></pre></td></tr></table></figure><p><code>all_reduce</code> 会自动获取各个进程中同名 tensor，然后通过指定的 op 来进行计算，最后再同步到各个进程当中，也就是说这是一个原地的操作。为了避免可能产生的影响，这里不是直接对原来的 tensor 进行 reduce，而是先取了副本。</p><h2 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h2><p>这里有一个大坑！虽然参数会在各个进程中汇总，但是实际保存的模型的 state_dict 和非分布式的还是有区别的，如果直接载入很可能会出错，解决方法下面会提到。</p><h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>如果你没有掉进上一节的坑里面，那么测试阶段的代码可以和非分布式测试的完全相同。</p><h2 id="打印-log"><a href="#打印-log" class="headerlink" title="打印 log"></a>打印 log</h2><p>因为各个进程代码完全一样，因此打印结果也是打印 N 份，一个简单的解决方法就是判断当前的 <code>local_rank</code>，只有当其为特定值的时候才打印。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> opt.local_rank == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">'some log'</span>)</span><br></pre></td></tr></table></figure><h2 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a>遇到的坑</h2><p>DDP 训练的模型通过非 DDP 进行加载之后结果非常差。</p><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>保存的模型 state_dict 前缀多了一个 <code>module.</code>，这样在 <code>strict=False</code> 下载入参数的时候就相当于载入了个寂寞，因此结果很差</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li>加载 DDP 模型的时候重新构造 state_dict，将二者名称统一<a href="https://discuss.pytorch.org/t/failed-to-load-model-trained-by-ddp-for-inference/84841" target="_blank" rel="noopener">[1]</a>:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_load</span><span class="params">(state_dict)</span>:</span></span><br><span class="line"></span><br><span class="line">    new_state_dict = OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items():</span><br><span class="line">        name = k[<span class="number">7</span>:]  <span class="comment"># remove 'module.' of DataParallel/DistributedDataParallel</span></span><br><span class="line">        new_state_dict[name] = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_state_dict</span><br></pre></td></tr></table></figure><ol start="2"><li>（==推荐==）保存 DDP 模型的时候直接保存不包含 <code>module.</code> 前缀<a href="https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/17" target="_blank" rel="noopener">[2]</a>：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.module.state_dict(), path_to_file)</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>如果你觉得还是讲的不清楚，那就看<a href="https://zhuanlan.zhihu.com/p/145427849" target="_blank" rel="noopener">这篇文章</a>吧，我就是跟着这篇文章来写的。更深入的理解分析就看<a href="https://zhuanlan.zhihu.com/p/250471767" target="_blank" rel="noopener">这个系列</a>。</p><h2 id="总的代码"><a href="#总的代码" class="headerlink" title="总的代码"></a>总的代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> SyncBatchNorm</span><br><span class="line"><span class="keyword">from</span> torch.distributed <span class="keyword">import</span> ReduceOp</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> get_loader</span><br><span class="line"><span class="keyword">from</span> model.CPD_ResNet_models <span class="keyword">import</span> CPD_ResNet</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> clip_gradient, adjust_lr, save_single_plot, get_train_parser, init_workspace</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">parser = get_train_parser(loader_type=<span class="string">'rgb'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--local_rank'</span>, type=int, default=<span class="number">0</span>)</span><br><span class="line">opt = parser.parse_args()</span><br><span class="line">main_proc = <span class="literal">True</span> <span class="keyword">if</span> opt.local_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">basedir = init_workspace(opt, main_proc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># init distribute environment</span></span><br><span class="line">torch.cuda.set_device(opt.local_rank)</span><br><span class="line">dist.init_process_group(<span class="string">'nccl'</span>)</span><br><span class="line">device = torch.device(<span class="string">f'cuda:<span class="subst">&#123;opt.local_rank&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> main_proc:</span><br><span class="line">    print(<span class="string">'Learning Rate: &#123;&#125; Model type: &#123;&#125;'</span>.format(opt.lr, opt.model))</span><br><span class="line"></span><br><span class="line"><span class="comment"># build models</span></span><br><span class="line">model = CPD_ResNet()</span><br><span class="line">model = SyncBatchNorm.convert_sync_batchnorm(model).to(device)</span><br><span class="line">model = DistributedDataParallel(model, device_ids=[opt.local_rank], output_device=opt.local_rank)</span><br><span class="line"><span class="keyword">if</span> opt.print_model <span class="keyword">and</span> main_proc:</span><br><span class="line">    print(model)</span><br><span class="line">    summary(model, (<span class="number">4</span>, opt.trainsize, opt.trainsize))</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), opt.lr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build distribute data loader</span></span><br><span class="line">train_loader = get_loader(opt.train_img_dir, opt.train_gt_dir, loader_type=<span class="string">'rgb'</span>,</span><br><span class="line">                          batchsize=opt.batchsize, trainsize=opt.trainsize, dist=<span class="literal">True</span>)</span><br><span class="line">total_step = len(train_loader)</span><br><span class="line"></span><br><span class="line"><span class="comment"># build loss</span></span><br><span class="line">CE = torch.nn.BCEWithLogitsLoss().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save train results in df</span></span><br><span class="line">df_step = pd.DataFrame(columns=(<span class="string">'loss1'</span>, <span class="string">'loss2'</span>))</span><br><span class="line">df_epoch = pd.DataFrame(columns=(<span class="string">'loss1'</span>, <span class="string">'loss2'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_tensor</span><span class="params">(tensor: torch.Tensor)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    rt = tensor.clone()</span><br><span class="line">    dist.all_reduce(rt, op=dist.ReduceOp.SUM)</span><br><span class="line">    rt /= dist.get_world_size()</span><br><span class="line">    <span class="keyword">return</span> rt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_loader, model, optimizer, epoch)</span>:</span></span><br><span class="line">    model.train()</span><br><span class="line">    epoch_loss1 = []</span><br><span class="line">    epoch_loss2 = []</span><br><span class="line">    <span class="keyword">for</span> i, pack <span class="keyword">in</span> enumerate(train_loader, start=<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># if main_proc:</span></span><br><span class="line">        <span class="comment">#     print('running')</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        images, gts = pack</span><br><span class="line"></span><br><span class="line">        images = images.to(device)</span><br><span class="line">        gts = gts.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if main_proc:</span></span><br><span class="line">        <span class="comment">#     print('data done')</span></span><br><span class="line"></span><br><span class="line">        atts, dets = model(images)</span><br><span class="line"></span><br><span class="line">        loss1 = CE(atts, gts)</span><br><span class="line">        loss2 = CE(dets, gts)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if main_proc:</span></span><br><span class="line">        <span class="comment">#     print('CE done')</span></span><br><span class="line"></span><br><span class="line">        loss = loss1 + loss2</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if main_proc:</span></span><br><span class="line">        <span class="comment">#     print('loss done')</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># save loss results</span></span><br><span class="line">        reduced_loss1 = reduce_tensor(loss1.data).item()</span><br><span class="line">        <span class="comment"># if main_proc:</span></span><br><span class="line">        <span class="comment">#     print('reduce loss1 done')</span></span><br><span class="line">        reduced_loss2 = reduce_tensor(loss2.data).item()</span><br><span class="line">        <span class="comment"># if main_proc:</span></span><br><span class="line">        <span class="comment">#     print('reduce loss2 done')</span></span><br><span class="line">        <span class="keyword">if</span> main_proc:</span><br><span class="line">            epoch_loss1.append(reduced_loss1)</span><br><span class="line">            epoch_loss2.append(reduced_loss2)</span><br><span class="line">            df_step.loc[df_step.shape[<span class="number">0</span>]] = (reduced_loss1, reduced_loss2)</span><br><span class="line"></span><br><span class="line">        clip_gradient(optimizer, opt.clip)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i % <span class="number">400</span> == <span class="number">0</span> <span class="keyword">or</span> i == total_step) <span class="keyword">and</span> main_proc:</span><br><span class="line">            print(<span class="string">'&#123;&#125; Epoch [&#123;:03d&#125;/&#123;:03d&#125;], Step [&#123;:04d&#125;/&#123;:04d&#125;], Loss1: &#123;:.4f&#125; Loss2: &#123;:0.4f&#125;'</span>.</span><br><span class="line">                  format(datetime.now(), epoch, opt.epoch, i, total_step,</span><br><span class="line">                         np.mean(epoch_loss1), np.mean(epoch_loss2)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> main_proc:</span><br><span class="line">        df_epoch.loc[df_epoch.shape[<span class="number">0</span>]] = (np.mean(epoch_loss1), np.mean(epoch_loss2))</span><br><span class="line">        save_path = os.path.join(basedir, <span class="string">'checkpoints'</span>)</span><br><span class="line">        os.makedirs(save_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 注意，这里保存的是 model.module.state_dict()，这样测试的时候就不用做额外的处理</span></span><br><span class="line">            torch.save(model.module.state_dict(), os.path.join(save_path, <span class="string">'CPD_&#123;&#125;.pth'</span>.format(epoch + <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"GPU &#123;&#125;: Let's go!"</span>.format(opt.local_rank))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, opt.epoch + <span class="number">1</span>):</span><br><span class="line">    adjust_lr(optimizer, opt.lr, epoch, opt.decay_rate, opt.decay_epoch)</span><br><span class="line">    train(train_loader, model, optimizer, epoch)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> main_proc:</span><br><span class="line">    df_epoch.to_csv(os.path.join(basedir, <span class="string">'epoch_loss.csv'</span>), index=<span class="literal">False</span>)</span><br><span class="line">    df_step.to_csv(os.path.join(basedir, <span class="string">'step_loss.csv'</span>), index=<span class="literal">False</span>)</span><br><span class="line">    save_single_plot(df_epoch, (<span class="string">'loss1'</span>, <span class="string">'loss2'</span>), <span class="string">'epoch'</span>, <span class="string">'loss'</span>, basedir, <span class="string">'epoch'</span>)</span><br><span class="line">    save_single_plot(df_step, (<span class="string">'loss1'</span>, <span class="string">'loss2'</span>), <span class="string">'step'</span>, <span class="string">'loss'</span>, basedir, <span class="string">'step'</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;p&gt;DDP 是 DistributedDataParallel 的简写，用来进行分布式训练，可以是单主机多 GPU 也可以是多主机多 GPU，以
      
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://blog.patrickcty.cc/tags/PyTorch/"/>
    
      <category term="分布式训练" scheme="https://blog.patrickcty.cc/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
      <category term="DDP" scheme="https://blog.patrickcty.cc/tags/DDP/"/>
    
  </entry>
  
  <entry>
    <title>SimCLR 代码分析</title>
    <link href="https://blog.patrickcty.cc/2020/11/10/simclr%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>https://blog.patrickcty.cc/2020/11/10/simclr代码分析/</id>
    <published>2020-11-10T09:08:49.000Z</published>
    <updated>2020-11-10T11:58:38.866Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章分析 <a href="https://github.com/sthalles/SimCLR" target="_blank" rel="noopener">PyTorch SimCLR: A Simple Framework for Contrastive Learning of Visual Representations</a>。使用自监督的方法来生成特征表示。其中 loss 部分实现得非常巧妙，因此特地拿出来分析。</p><h2 id="SimCLR-data-aug-dataset-wrapper-py"><a href="#SimCLR-data-aug-dataset-wrapper-py" class="headerlink" title="SimCLR/data_aug/dataset_wrapper.py"></a>SimCLR/data_aug/dataset_wrapper.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> SubsetRandomSampler</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> data_aug.gaussian_blur <span class="keyword">import</span> GaussianBlur</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataSetWrapper</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batch_size, num_workers, valid_size, input_shape, s)</span>:</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line">        self.valid_size = valid_size</span><br><span class="line">        self.s = s</span><br><span class="line">        self.input_shape = eval(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data_loaders</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 数据增强</span></span><br><span class="line">        data_augment = self._get_simclr_pipeline_transform()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用官方的 STL 数据集</span></span><br><span class="line">        train_dataset = datasets.STL10(<span class="string">'./data'</span>, split=<span class="string">'train+unlabeled'</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                       transform=SimCLRDataTransform(data_augment))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机划分训练集与验证集</span></span><br><span class="line">        train_loader, valid_loader = self.get_train_validation_data_loaders(train_dataset)</span><br><span class="line">        <span class="keyword">return</span> train_loader, valid_loader</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_simclr_pipeline_transform</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 运用了很多数据增强的方法</span></span><br><span class="line">        <span class="comment"># get a set of data augmentation transformations as described in the SimCLR paper.</span></span><br><span class="line">        color_jitter = transforms.ColorJitter(<span class="number">0.8</span> * self.s, <span class="number">0.8</span> * self.s, <span class="number">0.8</span> * self.s, <span class="number">0.2</span> * self.s)</span><br><span class="line">        data_transforms = transforms.Compose([transforms.RandomResizedCrop(size=self.input_shape[<span class="number">0</span>]),</span><br><span class="line">                                              transforms.RandomHorizontalFlip(),</span><br><span class="line">                                              transforms.RandomApply([color_jitter], p=<span class="number">0.8</span>),</span><br><span class="line">                                              transforms.RandomGrayscale(p=<span class="number">0.2</span>),</span><br><span class="line">                                              GaussianBlur(kernel_size=int(<span class="number">0.1</span> * self.input_shape[<span class="number">0</span>])),</span><br><span class="line">                                              transforms.ToTensor()])</span><br><span class="line">        <span class="keyword">return</span> data_transforms</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_train_validation_data_loaders</span><span class="params">(self, train_dataset)</span>:</span></span><br><span class="line">        <span class="comment"># 随机划分训练集和验证集 </span></span><br><span class="line">        <span class="comment"># obtain training indices that will be used for validation</span></span><br><span class="line">        num_train = len(train_dataset)</span><br><span class="line">        indices = list(range(num_train))</span><br><span class="line">        np.random.shuffle(indices)</span><br><span class="line"></span><br><span class="line">        split = int(np.floor(self.valid_size * num_train))</span><br><span class="line">        train_idx, valid_idx = indices[split:], indices[:split]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># define samplers for obtaining training and validation batches</span></span><br><span class="line">        train_sampler = SubsetRandomSampler(train_idx)</span><br><span class="line">        valid_sampler = SubsetRandomSampler(valid_idx)</span><br><span class="line"></span><br><span class="line">        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=train_sampler,</span><br><span class="line">                                  num_workers=self.num_workers, drop_last=<span class="literal">True</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        valid_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=valid_sampler,</span><br><span class="line">                                  num_workers=self.num_workers, drop_last=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> train_loader, valid_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimCLRDataTransform</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, transform)</span>:</span></span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="comment"># 同一个 sample 增强两次得到两个输入</span></span><br><span class="line">        xi = self.transform(sample)</span><br><span class="line">        xj = self.transform(sample)</span><br><span class="line">        <span class="keyword">return</span> xi, xj</span><br></pre></td></tr></table></figure><p>这部分要注意的主要就是对于一个数据数据得到两个增强的数据。</p><h2 id="SimCLR-models-resnet-simclr-py"><a href="#SimCLR-models-resnet-simclr-py" class="headerlink" title="SimCLR/models/resnet_simclr.py"></a>SimCLR/models/resnet_simclr.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNetSimCLR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, base_model, out_dim)</span>:</span></span><br><span class="line">        super(ResNetSimCLR, self).__init__()</span><br><span class="line">        self.resnet_dict = &#123;<span class="string">"resnet18"</span>: models.resnet18(pretrained=<span class="literal">False</span>),</span><br><span class="line">                            <span class="string">"resnet50"</span>: models.resnet50(pretrained=<span class="literal">False</span>)&#125;</span><br><span class="line"></span><br><span class="line">        resnet = self._get_basemodel(base_model)</span><br><span class="line">        num_ftrs = resnet.fc.in_features</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(*list(resnet.children())[:<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># projection MLP</span></span><br><span class="line">        self.l1 = nn.Linear(num_ftrs, num_ftrs)</span><br><span class="line">        self.l2 = nn.Linear(num_ftrs, out_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_basemodel</span><span class="params">(self, model_name)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            model = self.resnet_dict[model_name]</span><br><span class="line">            print(<span class="string">"Feature extractor:"</span>, model_name)</span><br><span class="line">            <span class="keyword">return</span> model</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">raise</span> (<span class="string">"Invalid model name. Check the config file and pass one of: resnet18 or resnet50"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        h = self.features(x)</span><br><span class="line">        h = h.squeeze()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用了两个 FC 层对特征进行映射</span></span><br><span class="line">        <span class="comment"># 这个实现中设置最终得到 256 维向量</span></span><br><span class="line">        x = self.l1(h)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.l2(x)</span><br><span class="line">        <span class="keyword">return</span> h, x</span><br></pre></td></tr></table></figure><h2 id="SimCLR-loss-nt-xent-py"><a href="#SimCLR-loss-nt-xent-py" class="headerlink" title="SimCLR/loss/nt_xent.py"></a>SimCLR/loss/nt_xent.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NTXentLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, device, batch_size, temperature, use_cosine_similarity)</span>:</span></span><br><span class="line">        super(NTXentLoss, self).__init__()</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.temperature = temperature</span><br><span class="line">        self.device = device</span><br><span class="line">        self.softmax = torch.nn.Softmax(dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># 标注负样本所在的位置</span></span><br><span class="line">        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)</span><br><span class="line">        self.similarity_function = self._get_similarity_function(use_cosine_similarity)</span><br><span class="line">        self.criterion = torch.nn.CrossEntropyLoss(reduction=<span class="string">"sum"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_similarity_function</span><span class="params">(self, use_cosine_similarity)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> use_cosine_similarity:</span><br><span class="line">            self._cosine_similarity = torch.nn.CosineSimilarity(dim=<span class="number">-1</span>)</span><br><span class="line">            <span class="keyword">return</span> self._cosine_simililarity</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._dot_simililarity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_correlated_mask</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 每个图像都经过两次增强，得到 2 * self.batch_size 个输入图像</span></span><br><span class="line">        <span class="comment"># 两两求相似度之后得到 2N * 2N 矩阵</span></span><br><span class="line">        <span class="comment"># 其中对角线以及上 N 下 N 对角线是同一个图像，也就是正样本</span></span><br><span class="line">        diag = np.eye(<span class="number">2</span> * self.batch_size)</span><br><span class="line">        l1 = np.eye((<span class="number">2</span> * self.batch_size), <span class="number">2</span> * self.batch_size, k=-self.batch_size)</span><br><span class="line">        l2 = np.eye((<span class="number">2</span> * self.batch_size), <span class="number">2</span> * self.batch_size, k=self.batch_size)</span><br><span class="line">        mask = torch.from_numpy((diag + l1 + l2))</span><br><span class="line">        <span class="comment"># 取反就是负样本</span></span><br><span class="line">        mask = (<span class="number">1</span> - mask).type(torch.bool)</span><br><span class="line">        <span class="keyword">return</span> mask.to(self.device)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dot_simililarity</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        v = torch.tensordot(x.unsqueeze(<span class="number">1</span>), y.T.unsqueeze(<span class="number">0</span>), dims=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># x shape: (M, 1, C)</span></span><br><span class="line">        <span class="comment"># y shape: (1, C, N)</span></span><br><span class="line">        <span class="comment"># v shape: (M, N)</span></span><br><span class="line">        <span class="comment"># 因为有 batch size，因此得到的相似度是一个矩阵</span></span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_cosine_simililarity</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="comment"># x shape: (M, 1, C)</span></span><br><span class="line">        <span class="comment"># y shape: (1, N, C)</span></span><br><span class="line">        <span class="comment"># v shape: (M, N)</span></span><br><span class="line">        v = self._cosine_similarity(x.unsqueeze(<span class="number">1</span>), y.unsqueeze(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, zis, zjs)</span>:</span></span><br><span class="line">        <span class="comment"># 组合成 2N 维向量，其中第 i 和 i + N 是同一个样本增强结果</span></span><br><span class="line">        representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算相似度，得到 2N * 2N 矩阵</span></span><br><span class="line">        similarity_matrix = self.similarity_function(representations, representations)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到正样本，其中中间对角线是自身相乘的，可以无视掉</span></span><br><span class="line">        <span class="comment"># filter out the scores from the positive samples</span></span><br><span class="line">        l_pos = torch.diag(similarity_matrix, self.batch_size)</span><br><span class="line">        r_pos = torch.diag(similarity_matrix, -self.batch_size)</span><br><span class="line">        <span class="comment"># view 是将向量 reshape，得到一个 2N 维向量</span></span><br><span class="line">        positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 负样本，同样也是 reshape，得到 2N * (2N - 2) 矩阵</span></span><br><span class="line">        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 串联起来之后维度是 2N * (2N - 1)，其中每一列中第一个都是正样本，其他为负样本</span></span><br><span class="line">        logits = torch.cat((positives, negatives), dim=<span class="number">1</span>)</span><br><span class="line">        logits /= self.temperature</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造一个维度为 2N 的标签，0 对应相面的正样本</span></span><br><span class="line">        labels = torch.zeros(<span class="number">2</span> * self.batch_size).to(self.device).long()</span><br><span class="line">        <span class="comment"># 交叉熵展开之后就是文中 loss 的形式</span></span><br><span class="line">        loss = self.criterion(logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss / (<span class="number">2</span> * self.batch_size)</span><br></pre></td></tr></table></figure><p>这是代码中最关键的部分，这里在一个 batch 中构造正负样本，通过相似矩阵的形似很巧妙地挖掘出了正负样本，最终也巧妙用交叉熵实现了以下 loss 的形式。</p><p><img src="https://static.jnugeek.cn/blog/loss_contrastive.png" alt="Contrastive loss"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;这篇文章分析 &lt;a href=&quot;https://github.com/sthalles/SimCLR&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
    
      <category term="Contrastive Learning" scheme="https://blog.patrickcty.cc/tags/Contrastive-Learning/"/>
    
      <category term="Deep Leanring" scheme="https://blog.patrickcty.cc/tags/Deep-Leanring/"/>
    
  </entry>
  
  <entry>
    <title>keras 中 dense 层输入秩大于二</title>
    <link href="https://blog.patrickcty.cc/2020/11/03/keras%E4%B8%ADdense%E5%B1%82%E8%BE%93%E5%85%A5%E7%A7%A9%E5%A4%A7%E4%BA%8E%E4%BA%8C/"/>
    <id>https://blog.patrickcty.cc/2020/11/03/keras中dense层输入秩大于二/</id>
    <published>2020-11-03T10:36:16.000Z</published>
    <updated>2020-11-03T10:51:44.888Z</updated>
    
    <content type="html"><![CDATA[<p>通常情况下输入到 Dense 层（又或者叫 FC 层）的张量是一个 (batch_size, length) 的秩为 2 的形式。比如 AlexNet 和 VGG，输出的特征图都会经过 flatten 操作降维到 (None, 1024)，然后才输入到 Dense 层中。</p><p>但是今天我在看 MaskX R-CNN 的时候发现输入并不是一个二维矩阵，而是一个三维的张量。Keras 文档中在处理 Dense 秩大于二的时候会将其通过一个矩阵乘法来改变输出最后一维的长度（秩不变）。这样处理不是真正意义上所有神经元全连接，参数上也比 flatten 再还原要小很多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = layers.Input((<span class="number">81</span>, <span class="number">1024</span>))  <span class="comment"># (None, 81, 1024)</span></span><br><span class="line">y = layers.Dense(<span class="number">256</span>)  <span class="comment"># y shape: (None, 81, 256)</span></span><br><span class="line"><span class="comment"># 参数 W shape: (1024, 256) </span></span><br><span class="line"><span class="comment"># 参数 b shape: (256)</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;通常情况下输入到 Dense 层（又或者叫 FC 层）的张量是一个 (batch_size, length) 的秩为 2 的形式。比如 AlexNet 和 VGG，输出的特征图都会经过 flatten 操作降维到 (None, 1024)，然后才输入到 Dense 层中。&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>一个例子加深Python元类与描述符类理解</title>
    <link href="https://blog.patrickcty.cc/2020/10/27/%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E5%8A%A0%E6%B7%B1Python%E5%85%83%E7%B1%BB%E4%B8%8E%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%B1%BB%E7%90%86%E8%A7%A3/"/>
    <id>https://blog.patrickcty.cc/2020/10/27/一个例子加深Python元类与描述符类理解/</id>
    <published>2020-10-27T01:10:02.000Z</published>
    <updated>2020-10-27T03:14:40.447Z</updated>
    
    <content type="html"><![CDATA[<h2 id="通过函数注解来实现方法重载"><a href="#通过函数注解来实现方法重载" class="headerlink" title="通过函数注解来实现方法重载"></a>通过函数注解来实现方法重载</h2><p>最近在看 《Python Cookbook》，9.20 的示例涉及到非常多高级用法，有必要专门拿出来整理一下避免遗忘。这一节的目的是通过函数注解来实现方法重载，由于 Python 中对参数类型是没有硬性要求的，因此在 Python 中也没有方法重载这一特性。虽然函数注解能提示用户输入变量应该是什么类型，但实际上并没有类型检查与硬性的约束。在这一节之中就是用元类 + 描述符来实现这个功能。</p><h3 id="定义描述符类"><a href="#定义描述符类" class="headerlink" title="定义描述符类"></a>定义描述符类</h3><p>首先定义一个描述符类，用来将参数类型与对应的函数引用进行绑定，同时也可以通过传入的参数获取到函数引用。前者是使用一个注册函数来实现的，传入的是一个函数，得到这个函数可能的参数列表，然后将参数列表与函数绑定到一个字典中；后者是通过重写 <code>__call__</code> 方法来从输出参数找到对应函数，重写 <code>__get__</code> 方法将函数绑定 <code>self</code> 参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> types</span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiMethod</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Represents a single multimethod.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self._methods = &#123;&#125;  <span class="comment"># 绑定参数类型与函数引用</span></span><br><span class="line">        self.__name__ = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(self, meth)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Register a new method as a multimethod</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        sig = inspect.signature(meth)  <span class="comment"># 用来获取函数的参数信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Build a type-signature from the method's annotations</span></span><br><span class="line">        types = []</span><br><span class="line">        <span class="keyword">for</span> name, parm <span class="keyword">in</span> sig.parameters.items():</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">'self'</span>:  <span class="comment"># 忽视掉 self 参数</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> parm.annotation <span class="keyword">is</span> inspect.Parameter.empty:  <span class="comment"># 必须要有函数注解</span></span><br><span class="line">                <span class="keyword">raise</span> TypeError(</span><br><span class="line">                    <span class="string">'Argument &#123;&#125; must be annotated with a type'</span>.format(name)</span><br><span class="line">                    )</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> isinstance(parm.annotation, type):  <span class="comment"># 函数注解必须要是类型</span></span><br><span class="line">                <span class="keyword">raise</span> TypeError(</span><br><span class="line">                    <span class="string">'Argument &#123;&#125; annotation must be a type'</span>.format(name)</span><br><span class="line">                    )</span><br><span class="line">            <span class="comment"># 如果遇到有默认值的参数，那么在输入的时候不带这一项也可以</span></span><br><span class="line">            <span class="comment"># 因此每一个都要单独作为一个参数类型的入口</span></span><br><span class="line">            <span class="keyword">if</span> parm.default <span class="keyword">is</span> <span class="keyword">not</span> inspect.Parameter.empty:</span><br><span class="line">                self._methods[tuple(types)] = meth</span><br><span class="line">            <span class="comment"># 因为不支持关键字参数，因此一旦传入了某个参数</span></span><br><span class="line">            <span class="comment"># 其前面所有参数都得传入</span></span><br><span class="line">            types.append(parm.annotation)</span><br><span class="line"></span><br><span class="line">        self._methods[tuple(types)] = meth</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Call a method based on type signature of the arguments</span></span><br><span class="line"><span class="string">        这样创建实例之后实例直接就是一个可调用对象了。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 首先将传入参数转换为类型元组，忽视掉 self 参数</span></span><br><span class="line">        types = tuple(type(arg) <span class="keyword">for</span> arg <span class="keyword">in</span> args[<span class="number">1</span>:])</span><br><span class="line">        <span class="comment"># 然后通过元组来获取对应的方法引用</span></span><br><span class="line">        meth = self._methods.get(types, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 找到了就表明是支持的参数列表</span></span><br><span class="line">        <span class="keyword">if</span> meth:</span><br><span class="line">            <span class="keyword">return</span> meth(*args)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 否则类型就不对应</span></span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'No matching method for types &#123;&#125;'</span>.format(types))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, instance, cls)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Descriptor method needed to make calls work in a class</span></span><br><span class="line"><span class="string">        这里主要是为了绑定 self，不然直接调用会提示少一个参数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">if</span> instance <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> types.MethodType(self, instance)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><h3 id="定义元类"><a href="#定义元类" class="headerlink" title="定义元类"></a>定义元类</h3><p>接下来就是要使用元类把上面的集成到类中，最好的方法就是在创建的时候能通过描述符来绑定方法。这可以通过元类中的 clsdict 来实现。本文中的实现方法是修改 clsdict 的行为，在绑定方法的时候合并同名不同参数的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiDict</span><span class="params">(dict)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Special dictionary to build multimethods in a metaclass</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, key, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self:</span><br><span class="line">            <span class="comment"># If key already exists, it must be a multimethod or callable</span></span><br><span class="line">            current_value = self[key]</span><br><span class="line">            <span class="keyword">if</span> isinstance(current_value, MultiMethod):</span><br><span class="line">                <span class="comment"># 某个名字的方法出现第三次，这次就直接注册了</span></span><br><span class="line">                current_value.register(value)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 某个名字的方法出现第二次，那么首先先创建一个描述符</span></span><br><span class="line">                mvalue = MultiMethod(key)</span><br><span class="line">                <span class="comment"># 分别注册这两个方法</span></span><br><span class="line">                mvalue.register(current_value)</span><br><span class="line">                mvalue.register(value)</span><br><span class="line">                <span class="comment"># 将描述符绑定到类上</span></span><br><span class="line">                super().__setitem__(key, mvalue)</span><br><span class="line">        <span class="keyword">else</span>:  </span><br><span class="line">            <span class="comment"># 如果是第一次见到的，那直接设置属性</span></span><br><span class="line">            <span class="comment"># 因为这个时候不会出现同名方法</span></span><br><span class="line">            super().__setitem__(key, value)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultipleMeta</span><span class="params">(type)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Metaclass that allows multiple dispatch of methods</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, clsname, bases, clsdict)</span>:</span></span><br><span class="line">        <span class="comment"># 这里的 clsdict 就是下面的 MultiDict</span></span><br><span class="line">        <span class="comment"># 在 __new__ 中会创建类，也就是会将方法绑定到 clsdict 中</span></span><br><span class="line">        <span class="comment"># 在绑定的时候，由于 clsdict 重写了 __setitem__</span></span><br><span class="line">        <span class="comment"># 因此不会直接绑定方法，而是会绑定到描述符类</span></span><br><span class="line">        <span class="keyword">return</span> type.__new__(cls, clsname, bases, dict(clsdict))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__prepare__</span><span class="params">(cls, clsname, bases)</span>:</span></span><br><span class="line">        <span class="comment"># 这个函数在调用 __new__ 之前调用，返回一个映射对象</span></span><br><span class="line">        <span class="keyword">return</span> MultiDict()</span><br></pre></td></tr></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Some example classes that use multiple dispatch</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spam</span><span class="params">(metaclass=MultipleMeta)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, x:int, y:int)</span>:</span></span><br><span class="line">        print(<span class="string">'Bar 1:'</span>, x, y)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, s:str, n:int = <span class="number">0</span>)</span>:</span></span><br><span class="line">        print(<span class="string">'Bar 2:'</span>, s, n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example: overloaded __init__</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Date</span><span class="params">(metaclass=MultipleMeta)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, year: int, month:int, day:int)</span>:</span></span><br><span class="line">        self.year = year</span><br><span class="line">        self.month = month</span><br><span class="line">        self.day = day</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        t = time.localtime()</span><br><span class="line">        self.__init__(t.tm_year, t.tm_mon, t.tm_mday)</span><br><span class="line"></span><br><span class="line">s = Spam()</span><br><span class="line">s.bar(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">s.bar(<span class="string">'hello'</span>)</span><br><span class="line">s.bar(<span class="string">'hello'</span>, <span class="number">5</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    s.bar(<span class="number">2</span>, <span class="string">'hello'</span>)</span><br><span class="line"><span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">    print(e)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Overloaded __init__</span></span><br><span class="line">d = Date(<span class="number">2012</span>, <span class="number">12</span>, <span class="number">21</span>)</span><br><span class="line">print(d.year, d.month, d.day)</span><br><span class="line"><span class="comment"># Get today's date</span></span><br><span class="line">e = Date()</span><br><span class="line">print(e.year, e.month, e.day)</span><br></pre></td></tr></table></figure><p>输出结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Bar 1: 2 3</span><br><span class="line">Bar 2: hello 0</span><br><span class="line">Bar 2: hello 5</span><br><span class="line">No matching method for types (&lt;class &apos;int&apos;&gt;, &lt;class &apos;str&apos;&gt;)</span><br><span class="line">2012 12 21</span><br><span class="line">2020 10 27</span><br></pre></td></tr></table></figure></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; b = s.bar</span><br><span class="line">&gt;&gt;&gt; b  # 绑定方法</span><br><span class="line">&lt;bound method bar of &lt;example1.Spam object at 0x10aa00d30&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; b.__self__  # 类实例对象</span><br><span class="line">&lt;example1.Spam object at 0x10aa00d30&gt;</span><br><span class="line">&gt;&gt;&gt; b.__func__  # 实际的函数与描述符绑定</span><br><span class="line">&lt;example1.MultiMethod object at 0x10aaa8850&gt;</span><br></pre></td></tr></table></figure><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>正如作者所说，这种实现方法还是存在很多问题的，比如不支持关键字参数，对于继承也支持有限。因此在 Python 中还是使用更简单的方法，比如取不同的名字来实现比较好，不然也违背了 Python 设计的初衷。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="描述符"><a href="#描述符" class="headerlink" title="描述符"></a>描述符</h3><p>描述符是 Python 中的一类对象，它重写了 <code>__get__</code>, <code>__set__</code>, <code>__delete__</code> 中的一个或者多个。一般用于自定义的数据类型，可以在获取或者设置属性的时候加一些特殊的操作，比如类型检查、输出 log 等。在这里主要是通过 <code>__call__</code> 来从输入参数映射到不同的方法。通常情况下描述符和装饰器是可以相互转换的，在这一节中也给出了示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">multimethod</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        self._methods = &#123;&#125;</span><br><span class="line">        self.__name__ = func.__name__</span><br><span class="line">        self._default = func</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 装饰器函数，这里传入函数不需要知道参数</span></span><br><span class="line">    <span class="comment"># 但是装饰器需要传入类别作为参数，因此只有两层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">match</span><span class="params">(self, *types)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(func)</span>:</span></span><br><span class="line">            ndefaults = len(func.__defaults__) <span class="keyword">if</span> func.__defaults__ <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(ndefaults+<span class="number">1</span>):</span><br><span class="line">                <span class="comment"># 目的和上面一样，也是为了处理默认参数</span></span><br><span class="line">                self._methods[types[:len(types) - n]] = func</span><br><span class="line">            <span class="keyword">return</span> self</span><br><span class="line">        <span class="keyword">return</span> register</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="comment"># 还是要忽视掉 self 参数</span></span><br><span class="line">        types = tuple(type(arg) <span class="keyword">for</span> arg <span class="keyword">in</span> args[<span class="number">1</span>:])</span><br><span class="line">        meth = self._methods.get(types, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">if</span> meth:</span><br><span class="line">            <span class="keyword">return</span> meth(*args)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self._default(*args)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, instance, cls)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> instance <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> types.MethodType(self, instance)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example use</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spam</span>:</span></span><br><span class="line">    <span class="comment"># 相当于先初始化实例</span></span><br><span class="line">    <span class="comment"># 这里定义好像不能指定函数参数类别</span></span><br><span class="line">    <span class="comment"># 因此定义一个接受任意参数的函数作为缺省值来报错</span></span><br><span class="line"><span class="meta">    @multimethod  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="comment"># Default method called if no match</span></span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'No matching method for bar'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后再通过类型进行绑定</span></span><br><span class="line">    <span class="comment"># 这里就不用手动从参数转化到类型了</span></span><br><span class="line"><span class="meta">    @bar.match(int, int)  </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        print(<span class="string">'Bar 1:'</span>, x, y)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @bar.match(str, int)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, s, n = <span class="number">0</span>)</span>:</span></span><br><span class="line">        print(<span class="string">'Bar 2:'</span>, s, n)</span><br></pre></td></tr></table></figure><h2 id="元类"><a href="#元类" class="headerlink" title="元类"></a>元类</h2><p>元类是用来创建类的，可以在其中定义创建类时候的各种操作，比如这里就修改了绑定方法的地方，不直接将方法绑定到类中，而是将方法绑定到描述符中，然后通过描述符再绑定到类中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;通过函数注解来实现方法重载&quot;&gt;&lt;a href=&quot;#通过函数注解来实现方法重载&quot; class=&quot;headerlink&quot; title=&quot;通过函数注解来实现方法重载&quot;&gt;&lt;/a&gt;通过函数注解来实现方法重载&lt;/h2&gt;&lt;p&gt;最近在看 《Python Cookbook》，9.2
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用mmsegmentation训练自己的数据集</title>
    <link href="https://blog.patrickcty.cc/2020/10/14/%E4%BD%BF%E7%94%A8mmsegmentation%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://blog.patrickcty.cc/2020/10/14/使用mmsegmentation训练自己的数据集/</id>
    <published>2020-10-14T12:01:44.000Z</published>
    <updated>2020-10-14T12:02:35.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><ul><li>安装</li><li>注册数据集</li><li>编写配置文件</li><li>运行</li><li>测试</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install mmcv</span><br><span class="line"># 注意只是 clone 是不行的，还要 install 一下产生版本文件</span><br><span class="line">pip install git+https://github.com/open-mmlab/mmsegmentation.git # install the master branch</span><br></pre></td></tr></table></figure><p>更多安装方法参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/install.md" target="_blank" rel="noopener">官方文档</a></p><h2 id="注册数据集"><a href="#注册数据集" class="headerlink" title="注册数据集"></a>注册数据集</h2><ul><li>在 <code>mmseg/datasets</code> 目录下添加自己的数据集的 .py 文件，这里主要是让框架知道模型的类别，下面 suffix 根据自己实际情况修改</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .builder <span class="keyword">import</span> DATASETS</span><br><span class="line"><span class="keyword">from</span> .custom <span class="keyword">import</span> CustomDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@DATASETS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SatelliteDataset</span><span class="params">(CustomDataset)</span>:</span></span><br><span class="line">    <span class="string">"""Satellite dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``img_suffix`` is fixed to '.tif' and ``seg_map_suffix`` is</span></span><br><span class="line"><span class="string">    fixed to '.png'.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    CLASSES = (<span class="string">'ford'</span>, <span class="string">'transportation'</span>, <span class="string">'building'</span>, <span class="string">'farmland'</span>, <span class="string">'grassland'</span>,</span><br><span class="line">               <span class="string">'woodland'</span>, <span class="string">'bare_soil'</span>,  <span class="string">'others'</span>)</span><br><span class="line"></span><br><span class="line">    PALETTE = [[<span class="number">120</span>, <span class="number">120</span>, <span class="number">120</span>], [<span class="number">180</span>, <span class="number">120</span>, <span class="number">120</span>], [<span class="number">6</span>, <span class="number">230</span>, <span class="number">230</span>], [<span class="number">80</span>, <span class="number">50</span>, <span class="number">50</span>],</span><br><span class="line">               [<span class="number">4</span>, <span class="number">200</span>, <span class="number">3</span>], [<span class="number">120</span>, <span class="number">120</span>, <span class="number">80</span>], [<span class="number">140</span>, <span class="number">140</span>, <span class="number">140</span>], [<span class="number">204</span>, <span class="number">5</span>, <span class="number">255</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></span><br><span class="line">        super(SatelliteDataset, self).__init__(</span><br><span class="line">            img_suffix=<span class="string">'.tif'</span>,</span><br><span class="line">            seg_map_suffix=<span class="string">'.png'</span>,</span><br><span class="line">            reduce_zero_label=<span class="literal">False</span>,</span><br><span class="line">            **kwargs)</span><br><span class="line">        <span class="keyword">assert</span> osp.exists(self.img_dir)</span><br></pre></td></tr></table></figure><ul><li>在 <code>mmseg/datasets/__init__.py</code> 中导入你自定义的类，并在 <code>__all__</code> 变量中添加你的类名</li></ul><h2 id="编写配置文件"><a href="#编写配置文件" class="headerlink" title="编写配置文件"></a>编写配置文件</h2><p>在 <code>configs/你要用的方法/</code> 下创建一个 .py 文件，配置文件主要由四部分组成：</p><ul><li>使用模型</li><li>数据集及数据处理流程</li><li>模型调度方法</li><li>runtime 配置</li></ul><p>可以引入已有的配置，如果要修改配置就新建一个 dict 覆盖掉原来的配置项（不需要全部字段都有），如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = dict(</span><br><span class="line">    # 修改类别</span><br><span class="line">    decode_head=dict(num_classes=8, norm_cfg=norm_cfg),</span><br><span class="line">    auxiliary_head=dict(num_classes=8, norm_cfg=norm_cfg),</span><br><span class="line">    # 修改预训练路径</span><br><span class="line">    pretrained=&apos;open-mmlab://resnet101_v1c&apos;,</span><br><span class="line">    # 修改训练 backbone</span><br><span class="line">    backbone=dict(depth=101, norm_cfg=norm_cfg)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>放一个完整的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">_base_ = [</span><br><span class="line">    &apos;../_base_/models/pspnet_r50-d8.py&apos;,</span><br><span class="line">    &apos;../_base_/default_runtime.py&apos;,</span><br><span class="line">    &apos;../_base_/schedules/schedule_40k.py&apos;</span><br><span class="line">]</span><br><span class="line"># norm_cfg = dict(type=&apos;BN&apos;, requires_grad=True)</span><br><span class="line">norm_cfg = dict(type=&apos;SyncBN&apos;, requires_grad=True)</span><br><span class="line">model = dict(</span><br><span class="line">    decode_head=dict(num_classes=8, norm_cfg=norm_cfg),</span><br><span class="line">    auxiliary_head=dict(num_classes=8, norm_cfg=norm_cfg),</span><br><span class="line">    pretrained=&apos;open-mmlab://resnet101_v1c&apos;,</span><br><span class="line">    backbone=dict(depth=101, norm_cfg=norm_cfg)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">dataset_type = &apos;SatelliteDataset&apos;</span><br><span class="line">data_root = &apos;/home/sse/data4T/common_datasets/satelite_dataset/&apos;</span><br><span class="line">img_norm_cfg = dict(</span><br><span class="line">    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)</span><br><span class="line">img_scale = (256, 256)</span><br><span class="line"># crop_size = (224, 224)</span><br><span class="line"></span><br><span class="line">train_pipeline = [</span><br><span class="line">    dict(type=&apos;LoadImageFromFile&apos;),</span><br><span class="line">    dict(type=&apos;LoadAnnotations&apos;),</span><br><span class="line">    dict(type=&apos;RandomFlip&apos;, flip_ratio=0.5),</span><br><span class="line">    dict(type=&apos;PhotoMetricDistortion&apos;),</span><br><span class="line">    dict(type=&apos;Normalize&apos;, **img_norm_cfg),</span><br><span class="line">    dict(type=&apos;DefaultFormatBundle&apos;),</span><br><span class="line">    dict(type=&apos;Collect&apos;, keys=[&apos;img&apos;, &apos;gt_semantic_seg&apos;]),</span><br><span class="line">]</span><br><span class="line">test_pipeline = [</span><br><span class="line">    dict(type=&apos;LoadImageFromFile&apos;),</span><br><span class="line">    dict(</span><br><span class="line">        type=&apos;MultiScaleFlipAug&apos;,</span><br><span class="line">        img_scale=img_scale,</span><br><span class="line">        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],</span><br><span class="line">        flip=False,</span><br><span class="line">        transforms=[</span><br><span class="line">            dict(type=&apos;Normalize&apos;, **img_norm_cfg),</span><br><span class="line">            dict(type=&apos;ImageToTensor&apos;, keys=[&apos;img&apos;]),</span><br><span class="line">            dict(type=&apos;Collect&apos;, keys=[&apos;img&apos;]),</span><br><span class="line">        ])</span><br><span class="line">]</span><br><span class="line">data = dict(</span><br><span class="line">    samples_per_gpu=12,</span><br><span class="line">    workers_per_gpu=0,</span><br><span class="line">    train=dict(</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        data_root=data_root,</span><br><span class="line">        img_dir=&apos;img_dir/train&apos;,</span><br><span class="line">        ann_dir=&apos;ann_dir/train&apos;,</span><br><span class="line">        # split=&apos;ImageSets/SegmentationContext/train.txt&apos;,</span><br><span class="line">        pipeline=train_pipeline),</span><br><span class="line">    val=dict(  # 训练到一定轮次会自动验证</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        data_root=data_root,</span><br><span class="line">        img_dir=&apos;img_dir/test&apos;,</span><br><span class="line">        ann_dir=&apos;ann_dir/test&apos;,</span><br><span class="line">        # split=&apos;ImageSets/SegmentationContext/val.txt&apos;,</span><br><span class="line">        pipeline=test_pipeline),</span><br><span class="line">    test=dict(  # 测试的时候才用到</span><br><span class="line">        type=dataset_type,</span><br><span class="line">        data_root=data_root,</span><br><span class="line">        img_dir=&apos;image_A/image_A_9&apos;,</span><br><span class="line">        # ann_dir=&apos;ann_dir/test&apos;,</span><br><span class="line">        # split=&apos;ImageSets/SegmentationContext/val.txt&apos;,</span><br><span class="line">        pipeline=test_pipeline))</span><br><span class="line"></span><br><span class="line">total_iters = 100000</span><br><span class="line">checkpoint_config = dict(by_epoch=False, interval=4000)</span><br><span class="line">evaluation = dict(interval=4000, metric=&apos;mIoU&apos;)</span><br><span class="line"></span><br><span class="line"># 训练结果保存路径</span><br><span class="line">work_dir = &apos;/home/sse/mmsegmentation/run/satellite-10-12&apos;</span><br></pre></td></tr></table></figure><p>详细配置还是参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/config.md" target="_blank" rel="noopener">官方文档</a></p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>单卡运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/train.py 配置文件路径名</span><br></pre></td></tr></table></figure><p>分布式运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tools/dist_train.sh 配置文件路径名 GPU数 [optional arguments]</span><br></pre></td></tr></table></figure><p>从已有参数从头开始运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tools/dist_train.sh 配置文件路径名 GPU数 --load-from 参数路径</span><br></pre></td></tr></table></figure><p>从已有参数从头继续运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tools/dist_train.sh 配置文件路径名 GPU数 --resume-from 参数路径</span><br></pre></td></tr></table></figure><p>更详细的依然参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/getting_started.md" target="_blank" rel="noopener">官方文档</a></p><h3 id="运行时候的-一个坑"><a href="#运行时候的-一个坑" class="headerlink" title="运行时候的==一个坑=="></a>运行时候的==一个坑==</h3><p>验证数据集在验证时会把所有的预测结果和 GT 保存在内存中，如果验证集太大很可能进程会挂掉，测试同</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的时候默认使用配置文件中指定的测试集</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># single-gpu testing</span><br><span class="line">python tools/test.py $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; [--out $&#123;RESULT_FILE&#125;] [--eval $&#123;EVAL_METRICS&#125;] [--show]</span><br><span class="line"></span><br><span class="line"># multi-gpu testing</span><br><span class="line">./tools/dist_test.sh $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; $&#123;GPU_NUM&#125; [--out $&#123;RESULT_FILE&#125;] [--eval $&#123;EVAL_METRICS&#125;]</span><br></pre></td></tr></table></figure><p>可选参数</p><ul><li>RESULT_FILE: pickle 文件，结果保存在其中</li><li>EVAL_METRICS: 评价指标，制定之后需要标签文件</li><li>–show: 结果会在新窗口中打开</li><li>–show-dir: 可视化结果保存到指定文件夹中，注意保存的不是神经网络出来的结果，而是经过 RGB 调色之后和原图叠加的结果</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;总体流程&quot;&gt;&lt;a href=&quot;#总体流程&quot; class=&quot;headerlink&quot; title=&quot;总体流程&quot;&gt;&lt;/a&gt;总体流程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;安装&lt;/li&gt;
&lt;li&gt;注册数据集&lt;/li&gt;
&lt;li&gt;编写配置文件&lt;/li&gt;
&lt;li&gt;运行&lt;/li&gt;
&lt;li&gt;测
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割" scheme="https://blog.patrickcty.cc/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>【一个坑】exFAT 不支持软链接与硬链接</title>
    <link href="https://blog.patrickcty.cc/2020/10/10/exFAT%E4%B8%8D%E6%94%AF%E6%8C%81%E8%BD%AF%E9%93%BE%E6%8E%A5%E4%B8%8E%E7%A1%AC%E9%93%BE%E6%8E%A5/"/>
    <id>https://blog.patrickcty.cc/2020/10/10/exFAT不支持软链接与硬链接/</id>
    <published>2020-10-10T14:47:13.000Z</published>
    <updated>2020-10-10T14:53:48.276Z</updated>
    
    <content type="html"><![CDATA[<p>今天在跑 mmseg 的代码的时候报了一个 OSError，function not implemented。在终端使用 <code>ln</code> 命令也报了同样的问题，后来经过<a href="https://superuser.com/questions/1256530/linux-links-shortcuts-in-exfat-filesystem/1256536" target="_blank" rel="noopener">查找</a>之后发现 exFAT 文件系统不支持软链接与硬链接。</p><p>P.S. 创建软链接方法：<code>ln -s src_path dst_symlink_path</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天在跑 mmseg 的代码的时候报了一个 OSError，function not implemented。在终端使用 &lt;code&gt;ln&lt;/code&gt; 命令也报了同样的问题，后来经过&lt;a href=&quot;https://superuser.com/questions/1256
      
    
    </summary>
    
    
      <category term="踩坑" scheme="https://blog.patrickcty.cc/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>keras自定义训练流程</title>
    <link href="https://blog.patrickcty.cc/2020/10/07/keras%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/"/>
    <id>https://blog.patrickcty.cc/2020/10/07/keras自定义训练流程/</id>
    <published>2020-10-07T12:48:50.000Z</published>
    <updated>2020-10-07T12:51:09.771Z</updated>
    
    <content type="html"><![CDATA[<h2 id="标准流程"><a href="#标准流程" class="headerlink" title="标准流程"></a>标准流程</h2><p>keras 的 api 集成度都非常高，在你没有额外需求的时候的时候能非常轻松地完成整个训练流程：</p><ul><li>加载数据<ul><li>可以选择 generator </li><li>也可以直接传入内存的数据</li><li>还可以按照一定格式组织成文件夹然后直接传文件夹名</li></ul></li><li>构造模型</li><li>编译模型<ul><li>指定优化器</li><li>指定损失函数</li><li>指定评价标准</li></ul></li><li>训练模型<ul><li>指定训练轮次（epoch）</li><li>指定回调</li></ul></li></ul><h2 id="自定义流程"><a href="#自定义流程" class="headerlink" title="自定义流程"></a>自定义流程</h2><p>标准流程在大多数情况下都能满足需求，但是对于一些需要获取网络中细节的情况下就需要自定义流程了。自定义主要也是对训练步骤进行处理，基本步骤如下：</p><ul><li>定义一个 step 的操作：<ul><li>取出这个 batch 的数据</li><li>传入网络得到输出</li><li>计算 loss</li><li>计算梯度</li><li>梯度下降</li></ul></li></ul><p>写成代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">train_loss = tf.keras.metrics.Mean(<span class="string">'train_loss'</span>, dtype=tf.float32)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, (x_train, y_train) <span class="keyword">in</span> enumerate(train_gen):</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        predictions = model(x_train, training=<span class="literal">True</span>)  <span class="comment"># 传入网络得到输出</span></span><br><span class="line">        loss = loss_object(y_train, predictions)  <span class="comment"># 计算 loss</span></span><br><span class="line">    grads = tape.gradient(loss, model.trainable_variables)  <span class="comment"># 计算梯度</span></span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))  <span class="comment"># 梯度下降</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 一些指标</span></span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(y_train, predictions)</span><br></pre></td></tr></table></figure><h2 id="和-TensorBoard-一起作用"><a href="#和-TensorBoard-一起作用" class="headerlink" title="和 TensorBoard 一起作用"></a>和 TensorBoard 一起作用</h2><p>尽管 keras 的 callback 里面也有 tensorboard，但是默认情况下它只能每个 ep 来保存评价指标和直方图，不能看一个 step 中的变化情况，也不能将参数或者梯度来画成图表。在这里我们将参数和梯度的 l2 范数变化情况画成图表，并且原本就有的直方图也不落下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a tensorboard file writer</span></span><br><span class="line">summary_writer = tf.summary.create_file_writer(some_path)</span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">train_loss = tf.keras.metrics.Mean(<span class="string">'train_loss'</span>, dtype=tf.float32)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> idx, (x_train, y_train) <span class="keyword">in</span> enumerate(train_gen):</span><br><span class="line">        n_iter = (epoch - <span class="number">1</span>) * len(train_gen) + idx + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            predictions = model(x_train, training=<span class="literal">True</span>)  <span class="comment"># 传入网络得到输出</span></span><br><span class="line">            loss = loss_object(y_train, predictions)  <span class="comment"># 计算 loss</span></span><br><span class="line">        gradients = tape.gradient(loss, model.trainable_variables)  <span class="comment"># 计算梯度</span></span><br><span class="line">        optimizer.apply_gradients(zip(grads, model.trainable_variables))  <span class="comment"># 梯度下降</span></span><br><span class="line">        trainable_vars = model.trainable_variables</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 一些指标，都是标量</span></span><br><span class="line">        train_loss(loss)</span><br><span class="line">        train_accuracy(y_train, predictions)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">            <span class="comment"># 写入评价指标</span></span><br><span class="line">            tf.summary.scalar(<span class="string">'loss'</span>, train_loss.result(), step=n_iter)</span><br><span class="line">            tf.summary.scalar(<span class="string">'accuracy'</span>, train_accuracy.result(), step=n_iter)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> var, grad <span class="keyword">in</span> zip(trainable_vars, gradients)</span><br><span class="line">                <span class="comment"># 写入各个可训练元素的直方图、梯度和参数</span></span><br><span class="line">                tf.summary.histograme(var.name, var, n_iter)</span><br><span class="line">                tf.summary.scalar(<span class="string">'Grads:'</span> + var.name, tf.norm(grad), n_iter)</span><br><span class="line">                tf.summart.scalar(<span class="string">'Weights'</span> + var.name, ty.norm(var), n_iter)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Epoch &#123;:03d&#125; finished.'</span>.format(epoch))</span><br></pre></td></tr></table></figure><p>TensorBoard 里面的数据不能导出来，也可以单独将其写入 csv 来方便后续的处理。</p><h2 id="参考教程"><a href="#参考教程" class="headerlink" title="参考教程"></a>参考教程</h2><ul><li><a href="https://keras.io/guides/customizing_what_happens_in_fit/" target="_blank" rel="noopener">https://keras.io/guides/customizing_what_happens_in_fit/</a></li><li><a href="https://www.tensorflow.org/tensorboard/get_started" target="_blank" rel="noopener">https://www.tensorflow.org/tensorboard/get_started</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;标准流程&quot;&gt;&lt;a href=&quot;#标准流程&quot; class=&quot;headerlink&quot; title=&quot;标准流程&quot;&gt;&lt;/a&gt;标准流程&lt;/h2&gt;&lt;p&gt;keras 的 api 集成度都非常高，在你没有额外需求的时候的时候能非常轻松地完成整个训练流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li
      
    
    </summary>
    
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Keras" scheme="https://blog.patrickcty.cc/tags/Keras/"/>
    
      <category term="TensorFlow" scheme="https://blog.patrickcty.cc/tags/TensorFlow/"/>
    
      <category term="TensorBoard" scheme="https://blog.patrickcty.cc/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>keras中BatchNormalization在迁移学习中的坑</title>
    <link href="https://blog.patrickcty.cc/2020/05/13/Keras%E4%B8%ADBatchNormalization%E5%9C%A8%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/05/13/Keras中BatchNormalization在迁移学习中的坑/</id>
    <published>2020-05-13T02:38:13.000Z</published>
    <updated>2020-10-08T08:03:09.029Z</updated>
    
    <content type="html"><![CDATA[<h2 id="注"><a href="#注" class="headerlink" title="注"></a>注</h2><p>这个问题已经在 TF 2.0 中修复了，见<a href="https://tensorflow.google.cn/api_docs/python/tf/keras/layers/BatchNormalization?hl=zh-cn" target="_blank" rel="noopener">文档</a>。</p><blockquote><p>However, in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p><p>This behavior has been introduced in TensorFlow 2.0, in order to enable layer.trainable = False to produce the most commonly expected behavior in the convnet fine-tuning use case.</p></blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近 Keras 的文档更新了，我发现了一个迁移学习的文档，结果进去之后发现一个奇怪的地方：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">base_model = keras.applications.Xception(</span><br><span class="line">    weights=&quot;imagenet&quot;,  # Load weights pre-trained on ImageNet.</span><br><span class="line">    input_shape=(150, 150, 3),</span><br><span class="line">    include_top=False,</span><br><span class="line">)  # Do not include the ImageNet classifier at the top.</span><br><span class="line"></span><br><span class="line"># Freeze the base_model</span><br><span class="line">base_model.trainable = False</span><br><span class="line"></span><br><span class="line"># Create new model on top</span><br><span class="line">inputs = keras.Input(shape=(150, 150, 3))</span><br><span class="line">x = data_augmentation(inputs)  # Apply random data augmentation</span><br><span class="line">x = keras.layers.experimental.preprocessing.Rescaling(1.0 / 255.0)(</span><br><span class="line">    x</span><br><span class="line">)  # Scale inputs to [0. 1]</span><br><span class="line"># The base model contains batchnorm layers. We want to keep them in inference mode</span><br><span class="line"># when we unfreeze the base model for fine-tuning, so we make sure that the</span><br><span class="line"># base_model is running in inference mode here.</span><br><span class="line">x = base_model(x, training=False)</span><br><span class="line">x = keras.layers.GlobalAveragePooling2D()(x)</span><br><span class="line">x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout</span><br><span class="line">outputs = keras.layers.Dense(1)(x)</span><br><span class="line">model = keras.Model(inputs, outputs)</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><p>这里是说迁移学习的时候，使用其他的模型，然后冻结之，再增加新的层进行训练。其中 <code>x = base_model(x, training=False)</code> 以及上面的注释引起了我的注意。在这里设置 <code>training=False</code> 是为了让 backbone 处于 inference 状态，这个状态主要是对 BN 起作用，那就是不更新 BN 的参数，即使 unfreeze 之后也不更新。</p><p>这个 inference 状态和 training 状态有什么用呢？我们知道，有一些层在训练和测试的时候表现是不同的，比如 BN 和 Dropout。其中 BN 在训练的时候使用 mini-batch 的数据来进行归一化，同时更新 moving mean 和 moving variance，在测试的时候就使用上面的 moving mean 和 moving variance 来进行归一化。这里的 training 是用来控制这些层的表现。</p><p>这个 training 出现在 keras Layer 和 Model 的 call 方法中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def call(self, inputs, training=False):</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure><p>当调用层的时候可以指定，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = BatchNormalization()(x, training=False)</span><br><span class="line">model = Xception(input_shape=(150, 150, 3))(x, training=True)</span><br></pre></td></tr></table></figure><p>本着想更深入地理解这个参数，就查了一下，结果发现了一个 <a href="https://github.com/keras-team/keras/issues/7177" target="_blank" rel="noopener">issue</a> 和一个 <a href="https://github.com/keras-team/keras/pull/9965" target="_blank" rel="noopener">PR</a>，这里面就描述了 keras BatchNormalization 在迁移学习中的坑。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>大佬的<a href="http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/" target="_blank" rel="noopener">博客</a>清楚地解释了问题，我这里再重新复述一下。</p><p>上面提到过了，BN 层在训练状态和测试状态下的表现是不同的，一个是使用 mini-batch 的数据，另一个是使用积累下来的 moving mean 和 moving variance。而在迁移学习中，我们通常会把 backbone 直接 freeze，训练新加的层，再 unfreeze backbone，然后一起训练。</p><p>不过如果不按照上面那样设置 <code>x = base_model(x, training=False)</code>，而是直接像下面这样使用已有的模型然后进行训练（实际上大多数数据增强都不会整合到 model 中，因为只有训练的时候才需要，也就是说，基本不会出现上面这种调用形式），那么就会出现问题。（以下代码来自提到的博客）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from tensorflow.keras.datasets import cifar10</span><br><span class="line"> </span><br><span class="line">from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><br><span class="line">from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input</span><br><span class="line">from tensorflow.keras.models import Model, load_model</span><br><span class="line">from tensorflow.keras.layers import Dense, Flatten</span><br><span class="line">from tensorflow.keras import backend as K</span><br><span class="line"></span><br><span class="line">seed = 42</span><br><span class="line">epochs = 10</span><br><span class="line">records_per_class = 100</span><br><span class="line"></span><br><span class="line"># We take only 2 classes from CIFAR10 and a very small sample to intentionally overfit the model.</span><br><span class="line"># We will also use the same data for train/test and expect that Keras will give the same accuracy.</span><br><span class="line">(x, y), _ = cifar10.load_data()</span><br><span class="line"> </span><br><span class="line">def filter_resize(category):</span><br><span class="line">   # We do the preprocessing here instead in the Generator to get around a bug on Keras 2.1.5.</span><br><span class="line">   return [preprocess_input(img) for img in x[y.flatten()==category][:records_per_class]]</span><br><span class="line"> </span><br><span class="line">x = np.stack(filter_resize(3)+filter_resize(5))</span><br><span class="line">records_per_class = x.shape[0] // 2</span><br><span class="line">y = np.array([[1,0]]*records_per_class + [[0,1]]*records_per_class)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># We will use a pre-trained model and finetune the top layers.</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">base_model = ResNet50(weights=&apos;imagenet&apos;, include_top=False, input_shape=(32, 32, 3))</span><br><span class="line">l = Flatten()(base_model.output)</span><br><span class="line">predictions = Dense(2, activation=&apos;softmax&apos;)(l)</span><br><span class="line">model = Model(inputs=base_model.input, outputs=predictions)</span><br><span class="line"> </span><br><span class="line"># for layer in model.layers[:140]:</span><br><span class="line">#    layer.trainable = False</span><br><span class="line"> </span><br><span class="line"># for layer in model.layers[140:]:</span><br><span class="line">#    layer.trainable = True</span><br><span class="line">base_model.trainable = False</span><br><span class="line"> </span><br><span class="line">model.compile(optimizer=&apos;sgd&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])</span><br><span class="line">model.fit_generator(ImageDataGenerator().flow(x, y, seed=42), </span><br><span class="line">                    steps_per_epoch=7,</span><br><span class="line">                    epochs=epochs, </span><br><span class="line">                    validation_data=ImageDataGenerator().flow(x, y, seed=42),</span><br><span class="line">                    validation_steps=7</span><br><span class="line">                    )</span><br><span class="line"> </span><br><span class="line"># Store the model on disk</span><br><span class="line">model.save(&apos;tmp.h5&apos;)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># In every test we will clear the session and reload the model to force Learning_Phase values to change.</span><br><span class="line">print(&apos;DYNAMIC LEARNING_PHASE&apos;)</span><br><span class="line">K.clear_session()</span><br><span class="line">model = load_model(&apos;tmp.h5&apos;)</span><br><span class="line"># This accuracy should match exactly the one of the validation set on the last iteration.</span><br><span class="line">print(model.evaluate(ImageDataGenerator().flow(x, y, seed=42), steps=7))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">print(&apos;STATIC LEARNING_PHASE = 0&apos;)</span><br><span class="line">K.clear_session()</span><br><span class="line">K.set_learning_phase(0)</span><br><span class="line">model = load_model(&apos;tmp.h5&apos;)</span><br><span class="line"># Again the accuracy should match the above.</span><br><span class="line">print(model.evaluate(ImageDataGenerator().flow(x, y, seed=42), steps=7))</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">print(&apos;STATIC LEARNING_PHASE = 1&apos;)</span><br><span class="line">K.clear_session()</span><br><span class="line">K.set_learning_phase(1)</span><br><span class="line">model = load_model(&apos;tmp.h5&apos;)</span><br><span class="line"># The accuracy will be close to the one of the training set on the last iteration.</span><br><span class="line">print(model.evaluate(ImageDataGenerator().flow(x, y, seed=42), steps=7))</span><br></pre></td></tr></table></figure><p>运行上面的代码，其中训练集和验证集是同一个数据集。我们可以看到这两者的结果截然不同，训练的结果远好于验证的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 1.1314 - acc: 0.5298Epoch 1/10</span><br><span class="line">7/7 [==============================] - 3s 394ms/step - loss: 2.0678 - acc: 0.5700</span><br><span class="line">7/7 [==============================] - 5s 760ms/step - loss: 1.2129 - acc: 0.5300 - val_loss: 2.0678 - val_acc: 0.5700</span><br><span class="line">Epoch 2/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.9528 - acc: 0.6012Epoch 1/10</span><br><span class="line">7/7 [==============================] - 2s 265ms/step - loss: 1.4357 - acc: 0.5600</span><br><span class="line">7/7 [==============================] - 4s 558ms/step - loss: 0.8973 - acc: 0.6150 - val_loss: 1.4357 - val_acc: 0.5600</span><br><span class="line">Epoch 3/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.7655 - acc: 0.6667Epoch 1/10</span><br><span class="line">7/7 [==============================] - 2s 215ms/step - loss: 1.4113 - acc: 0.5950</span><br><span class="line">7/7 [==============================] - 4s 535ms/step - loss: 0.8119 - acc: 0.6550 - val_loss: 1.4113 - val_acc: 0.5950</span><br><span class="line">Epoch 4/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.7548 - acc: 0.7440Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 151ms/step - loss: 1.9380 - acc: 0.5800</span><br><span class="line">7/7 [==============================] - 2s 331ms/step - loss: 0.7230 - acc: 0.7350 - val_loss: 1.9380 - val_acc: 0.5800</span><br><span class="line">Epoch 5/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.5866 - acc: 0.7202Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 150ms/step - loss: 1.8147 - acc: 0.6000</span><br><span class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.5802 - acc: 0.7150 - val_loss: 1.8147 - val_acc: 0.6000</span><br><span class="line">Epoch 6/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.3704 - acc: 0.8095Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 151ms/step - loss: 1.5603 - acc: 0.6450</span><br><span class="line">7/7 [==============================] - 2s 321ms/step - loss: 0.3881 - acc: 0.7950 - val_loss: 1.5603 - val_acc: 0.6450</span><br><span class="line">Epoch 7/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.5056 - acc: 0.7738Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 151ms/step - loss: 1.9539 - acc: 0.6250</span><br><span class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.5618 - acc: 0.7400 - val_loss: 1.9539 - val_acc: 0.6250</span><br><span class="line">Epoch 8/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.5849 - acc: 0.7976Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 153ms/step - loss: 1.4035 - acc: 0.6600</span><br><span class="line">7/7 [==============================] - 2s 323ms/step - loss: 0.5465 - acc: 0.8050 - val_loss: 1.4035 - val_acc: 0.6600</span><br><span class="line">Epoch 9/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.4055 - acc: 0.8512Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 147ms/step - loss: 1.0538 - acc: 0.6650</span><br><span class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.3984 - acc: 0.8450 - val_loss: 1.0538 - val_acc: 0.6650</span><br><span class="line">Epoch 10/10</span><br><span class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.4082 - acc: 0.8452Epoch 1/10</span><br><span class="line">7/7 [==============================] - 1s 152ms/step - loss: 1.8019 - acc: 0.6000</span><br><span class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.4177 - acc: 0.8400 - val_loss: 1.8019 - val_acc: 0.6000</span><br></pre></td></tr></table></figure><p>再看看最后输出的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DYNAMIC LEARNING_PHASE</span><br><span class="line">7/7 [==============================] - 2s 256ms/step - loss: 2.0028 - acc: 0.6000</span><br><span class="line">[2.002779943602426, 0.6]</span><br><span class="line">STATIC LEARNING_PHASE = 0</span><br><span class="line">7/7 [==============================] - 1s 204ms/step - loss: 2.0028 - acc: 0.6000</span><br><span class="line">[2.002779943602426, 0.6]</span><br><span class="line">STATIC LEARNING_PHASE = 1</span><br><span class="line">7/7 [==============================] - 1s 212ms/step - loss: 0.3017 - acc: 0.8650</span><br><span class="line">[0.30170093051024843, 0.865]</span><br></pre></td></tr></table></figure><p>第一个结果是 keras 直接自动设置运行状态，第二个结果是手动设定运行状态为测试状态，第三个结果是手动设定运行结果为训练状态。可以看出来，keras 在测试的时候自动设置为测试状态，但这个时候结果出现了明显的下滑，而设置为训练状态的时候结果很正常。</p><p>其原因在于，在训练的时候，虽然 freeze 了 BN 的参数，但是 keras 仍然认为 BN 是在训练状态，因此会使用 mini-batch 的数据来标准化。也就是说，这时候后层网络学习到的是 mini-batch（训练数据集） 的分布。但是当测试的时候，BN 使用 moving mean 和 moving variance 来标准化，这两个参数由于没更新，是来自于原来数据集的。因为二者分布偏差很大，因此在测试模式下得到的结果非常差。</p><p>这个 PR 的改进就是当 freeze BN 的时候，就让 BN 层按照测试状态来进行，而不使用 mini-batch 的数据。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>看了半天，keras 官方好像没有改这个 bug，但是 TF 2.0 版本已经修改了这个 bug 了，以下是在 TF 2.0 下运行同样代码的结果，可以看到训练和验证的结果是相差不大的。另外值得一提的是，改进过后收敛速度明显快了很多，loss 从 0.3 直接降到了 0.01。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/10</span><br><span class="line">7/7 [==============================] - 2s 332ms/step - loss: 7.3916 - accuracy: 0.4700 - val_loss: 3.1501 - val_accuracy: 0.6500</span><br><span class="line">Epoch 2/10</span><br><span class="line">7/7 [==============================] - 1s 207ms/step - loss: 2.8816 - accuracy: 0.6700 - val_loss: 8.4492 - val_accuracy: 0.5100</span><br><span class="line">Epoch 3/10</span><br><span class="line">7/7 [==============================] - 1s 206ms/step - loss: 4.1846 - accuracy: 0.6750 - val_loss: 11.3409 - val_accuracy: 0.5600</span><br><span class="line">Epoch 4/10</span><br><span class="line">7/7 [==============================] - 1s 204ms/step - loss: 3.4036 - accuracy: 0.7800 - val_loss: 0.4167 - val_accuracy: 0.8650</span><br><span class="line">Epoch 5/10</span><br><span class="line">7/7 [==============================] - 1s 210ms/step - loss: 0.8244 - accuracy: 0.8150 - val_loss: 9.1833 - val_accuracy: 0.5400</span><br><span class="line">Epoch 6/10</span><br><span class="line">7/7 [==============================] - 1s 210ms/step - loss: 2.3888 - accuracy: 0.7600 - val_loss: 0.7993 - val_accuracy: 0.8100</span><br><span class="line">Epoch 7/10</span><br><span class="line">7/7 [==============================] - 1s 207ms/step - loss: 0.5801 - accuracy: 0.8600 - val_loss: 2.9707 - val_accuracy: 0.6700</span><br><span class="line">Epoch 8/10</span><br><span class="line">7/7 [==============================] - 1s 205ms/step - loss: 4.2250 - accuracy: 0.6050 - val_loss: 1.0646 - val_accuracy: 0.8500</span><br><span class="line">Epoch 9/10</span><br><span class="line">7/7 [==============================] - 1s 206ms/step - loss: 0.4886 - accuracy: 0.8900 - val_loss: 0.0866 - val_accuracy: 0.9800</span><br><span class="line">Epoch 10/10</span><br><span class="line">7/7 [==============================] - 1s 206ms/step - loss: 0.0969 - accuracy: 0.9700 - val_loss: 0.0109 - val_accuracy: 1.0000</span><br><span class="line">DYNAMIC LEARNING_PHASE</span><br><span class="line">7/7 [==============================] - 1s 95ms/step - loss: 0.0118 - accuracy: 1.0000</span><br><span class="line">[0.011801988817751408, 1.0]</span><br><span class="line">STATIC LEARNING_PHASE = 0</span><br><span class="line">7/7 [==============================] - 1s 94ms/step - loss: 0.0118 - accuracy: 1.0000</span><br><span class="line">[0.011801988817751408, 1.0]</span><br><span class="line">STATIC LEARNING_PHASE = 1</span><br><span class="line">7/7 [==============================] - 1s 92ms/step - loss: 0.0118 - accuracy: 1.0000</span><br><span class="line">[0.011801988817751408, 1.0]</span><br></pre></td></tr></table></figure><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>这种问题真的是防不胜防，毕竟很少人会去训练集和验证集使用同一个数据集，训练集和验证集相差大大家也只会怪罪到过拟合头上去。所以平常对于一些关键的东西还是得把他摸透，并且要多看官方文档，遇到问题多思考（所以深度学习就是这一点不好，出了问题有太多可能的原因，很难定位到问题所在）。</p><p>P.S. 今天在训练 SOD 的时候并没有出现这个问题，其原因可能在于：</p><ul><li>我的模型在 backbone 之外增加了很多的参数，减弱了 BN 的影响，因此结果是差不多的。（回头再多做一点实验）</li><li>DUTS 的数据本来就来自 ImageNet Detection，因此分布非常接近。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;注&quot;&gt;&lt;a href=&quot;#注&quot; class=&quot;headerlink&quot; title=&quot;注&quot;&gt;&lt;/a&gt;注&lt;/h2&gt;&lt;p&gt;这个问题已经在 TF 2.0 中修复了，见&lt;a href=&quot;https://tensorflow.google.cn/api_docs/python
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>keras处理任意大小输入</title>
    <link href="https://blog.patrickcty.cc/2020/04/11/keras%E5%A4%84%E7%90%86%E4%BB%BB%E6%84%8F%E5%A4%A7%E5%B0%8F%E8%BE%93%E5%85%A5/"/>
    <id>https://blog.patrickcty.cc/2020/04/11/keras处理任意大小输入/</id>
    <published>2020-04-11T07:56:32.000Z</published>
    <updated>2020-04-11T09:28:25.319Z</updated>
    
    <content type="html"><![CDATA[<p>让 keras 处理任意大小输入其实很简单，只需：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># for rgb</span><br><span class="line">main_input = layers.Input(shape=(None, None, 3))</span><br><span class="line"># for gray</span><br><span class="line">main_input = layers.Input(shape=(None, None, 1))</span><br></pre></td></tr></table></figure><p>对于 U-Net 这种又有下采样又有上采样的就要注意一下了，下采样再上采样得到的结果可能与原来的不同，比如 25 下采样是 12，再上采样就是 24，这样往往会出现一些问题，因此最好让输入经过下采样时不会出现除不尽的情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;让 keras 处理任意大小输入其实很简单，只需：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span cla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tf.keras重命名模型层名</title>
    <link href="https://blog.patrickcty.cc/2020/04/01/tfkeras%E9%87%8D%E5%91%BD%E5%90%8D%E6%A8%A1%E5%9E%8B/"/>
    <id>https://blog.patrickcty.cc/2020/04/01/tfkeras重命名模型/</id>
    <published>2020-04-01T13:08:27.000Z</published>
    <updated>2020-04-03T11:57:57.714Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前因后果"><a href="#前因后果" class="headerlink" title="前因后果"></a>前因后果</h2><p>为什么要重新命名模型的层名呢，目前做的是一个多任务的网络，两个网络用的是分开的 backbone，如果就这样并在一起作为一个模型的话就会有重复的层名，这个是不允许的，因此必须要重命名层名。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="方法一：初始化模型的时候设置不同名字"><a href="#方法一：初始化模型的时候设置不同名字" class="headerlink" title="方法一：初始化模型的时候设置不同名字"></a>方法一：初始化模型的时候设置不同名字</h3><p>其实比较好的解决方法就是在模型的层名中加一个 prefix，对于不同的任务可以定义不同的 prefix，不过这样有一个问题，那就是载入参数的时候不能使用 <code>by_name=True</code>。</p><h3 id="方法二：改层名"><a href="#方法二：改层名" class="headerlink" title="方法二：改层名"></a>方法二：改层名</h3><p>不过因为我是直接用的官方的库，不想自己改 backbone，所以就只能退而求其次修改层名了，修改方法为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.layers[idx]._name = &apos;aux_&apos; + l.name</span><br></pre></td></tr></table></figure><p>层的 name 属性是一个 property，不能修改，如果要修改的话使用 _name 属性，不过 _name 是一个 protected 属性，这么修改可能不是最好的方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前因后果&quot;&gt;&lt;a href=&quot;#前因后果&quot; class=&quot;headerlink&quot; title=&quot;前因后果&quot;&gt;&lt;/a&gt;前因后果&lt;/h2&gt;&lt;p&gt;为什么要重新命名模型的层名呢，目前做的是一个多任务的网络，两个网络用的是分开的 backbone，如果就这样并在一起作为一个
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>C语言指针的一些理解</title>
    <link href="https://blog.patrickcty.cc/2020/03/18/C%E8%AF%AD%E8%A8%80%E6%8C%87%E9%92%88%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"/>
    <id>https://blog.patrickcty.cc/2020/03/18/C语言指针的一些理解/</id>
    <published>2020-03-18T15:17:54.000Z</published>
    <updated>2020-03-18T15:41:05.558Z</updated>
    
    <content type="html"><![CDATA[<p>今天在 C 语言一个指针的问题上卡了很久……虽然平常不用 C 语言，但是还是记录一下，不能让时间白白的被浪费了。</p><h2 id="指针加减偏移量"><a href="#指针加减偏移量" class="headerlink" title="指针加减偏移量"></a>指针加减偏移量</h2><p>首先是指针加一个整数，加了之后偏移的字节数等于数据类型的字节数乘以整数的数值，例如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> *a; <span class="comment">// 假设 int 是四个字节，a 地址为 1000</span></span><br><span class="line">a = a + <span class="number">3</span>  <span class="comment">// 地址为 1000 + 3 * 4 = 1012</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 假设 SOMESTRUCT 结构体是 52 字节</span></span><br><span class="line"><span class="comment">// b 地址为 10000</span></span><br><span class="line">SOMESTRUCT *b;</span><br><span class="line">b += <span class="number">44</span>; <span class="comment">// 地址为 10000 + 44 * 52 = 12288</span></span><br></pre></td></tr></table></figure><h2 id="非整数倍的偏移量"><a href="#非整数倍的偏移量" class="headerlink" title="非整数倍的偏移量"></a>非整数倍的偏移量</h2><p>如果不想偏移数据类型的整数倍怎么办？很简单，用下标取地址，比如：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> *a;</span><br><span class="line">&amp;a[<span class="number">7</span>]</span><br></pre></td></tr></table></figure><p>不过要注意的是这种方法只适用于 char 型的指针，其他类型的还是不行。</p><h2 id="文件流的偏移量"><a href="#文件流的偏移量" class="headerlink" title="文件流的偏移量"></a>文件流的偏移量</h2><p>如果要取文件流的偏移量，那就使用 stdio.h 中的 <a href="https://www.runoob.com/cprogramming/c-function-fseek.html" target="_blank" rel="noopener">fseek</a> 和 <a href="https://www.runoob.com/cprogramming/c-function-fread.html" target="_blank" rel="noopener">fread</a> 方法。</p><p>fseek 会将文件流指针偏移指定大小，fread 则可以将文件流的数据读取到特定的指针之中。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">FILE *fp;</span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">char</span> *p;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 fp 指针置于文件开头的第十个字节</span></span><br><span class="line">fseek(fp, <span class="number">10</span>, SEEK_SET);</span><br><span class="line"><span class="comment">// 将以下量的数据写入 p 指针中</span></span><br><span class="line"><span class="comment">// 每个元素大小为 100 字节，一共有一个元素</span></span><br><span class="line"><span class="comment">// 写成 fread(p, 1, 100, fp) 也是相同效果</span></span><br><span class="line">fread(p, <span class="number">100</span>, <span class="number">1</span>, fp);</span><br></pre></td></tr></table></figure><p>另外，<a href="https://www.runoob.com/cprogramming/c-function-ftell.html" target="_blank" rel="noopener">ftell</a> 可以让你知道给定文件流当前指针的位置，和 fseek 配合使用可以知道文件流的大小。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fseek(fp, <span class="number">0</span>, SEEK_END);</span><br><span class="line"><span class="keyword">int</span> len = ftell(fp);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天在 C 语言一个指针的问题上卡了很久……虽然平常不用 C 语言，但是还是记录一下，不能让时间白白的被浪费了。&lt;/p&gt;
&lt;h2 id=&quot;指针加减偏移量&quot;&gt;&lt;a href=&quot;#指针加减偏移量&quot; class=&quot;headerlink&quot; title=&quot;指针加减偏移量&quot;&gt;&lt;/a&gt;指
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>理解多分类中的 mAP</title>
    <link href="https://blog.patrickcty.cc/2020/03/11/%E7%90%86%E8%A7%A3%E5%A4%9A%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84-k/"/>
    <id>https://blog.patrickcty.cc/2020/03/11/理解多分类中的-k/</id>
    <published>2020-03-11T02:33:33.000Z</published>
    <updated>2020-03-11T04:18:57.213Z</updated>
    
    <content type="html"><![CDATA[<p>在多分类任务中，有时候会用 mAP（Mean Average Precision）来表示分类的准确程度，如 VOC。其原因在于 mAP 能很好地评价分类的排序，而通常用的 accuracy 则往往会被最多的那个类别支配。</p><p>要计算多分类的 mAP，则要先计算各个类别的 AP。</p><h2 id="VOC-中-AP-的计算方法"><a href="#VOC-中-AP-的计算方法" class="headerlink" title="VOC 中 AP 的计算方法"></a>VOC 中 AP 的计算方法</h2><p>总的来说，就是对于某个类别，得到 n 个对该类别的预测概率，按照概率从大到小的顺序进行排列，然后对于 k∈1~n，求每个 k 对应的 Precision 和 Recall 值，对于每个 Recall 值，得到一个 Precision 值（==保证 P-R 曲线单调非递增==），将 n 个 Precision 取平均之后即为 AP 的值。要注意的是，这里不涉及到@k，因为总是为计算所有 n 个预测的结果。</p><p>对于一个四分类问题，已知标签和预测结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_true = np.array([[2], [1], [0], [3], [0], [1]]).astype(np.int64)</span><br><span class="line">y_pred = np.array([[0.1, 0.2, 0.6, 0.1],</span><br><span class="line">                   [0.8, 0.05, 0.1, 0.05],</span><br><span class="line">                   [0.3, 0.4, 0.1, 0.2],</span><br><span class="line">                   [0.6, 0.25, 0.1, 0.05],</span><br><span class="line">                   [0.1, 0.2, 0.6, 0.1],</span><br><span class="line">                   [0.9, 0.0, 0.03, 0.07]]).astype(np.float32)</span><br></pre></td></tr></table></figure><p>以类别 3 为例，六次预测给出的概率经过排序后为<code>[0.2  0.1  0.1  0.07 0.05 0.05]</code>，对应位置预测结果为<code>[0. 0. 0. 0. 0. 1.]</code>，0 表示预测错误，1 表示预测正确，那么可以列出来一个表：</p><table><thead><tr><th style="text-align:center">top-k</th><th style="text-align:center">Precision</th><th style="text-align:center">Recall</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0/1</td><td style="text-align:center">0/1</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0/2</td><td style="text-align:center">0/1</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0/3</td><td style="text-align:center">0/1</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0/4</td><td style="text-align:center">0/1</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">0/5</td><td style="text-align:center">0/1</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">1/6</td><td style="text-align:center">1/1</td></tr></tbody></table><p>在确保 P-R 曲线单调递减的情况下，求各个 Recall 对应的 Precision 的均值，在这里是 AP = (1/6) / 1 = 1/6</p><p>具体的内容参考<a href="https://link.zhihu.com/?target=http%3A//blog.sina.com.cn/s/blog_9db078090102whzw.html" target="_blank" rel="noopener">这一篇</a>。</p><p>同理可以求出来各个类别的 AP 为：<code>[1/3, 1/3, 1.0, 1/6]</code>，求均值后得到 MAP = 0.458。</p><p>一个 numpy 的实现为，来自这个 <a href="https://github.com/broadinstitute/keras-rcnn/issues/6" target="_blank" rel="noopener">issue</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y_true is one-hot</span></span><br><span class="line"> _, classes = y_true.shape</span><br><span class="line">    </span><br><span class="line">average_precisions = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(classes):</span><br><span class="line">        <span class="comment"># 得到从大到小排序后的标签索引</span></span><br><span class="line">        row_indices_sorted = numpy.argsort(-y_pred[:, index])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重新排列后的标签和预测结果</span></span><br><span class="line">        y_true_cls = y_true[row_indices_sorted, index]</span><br><span class="line">        y_pred_cls = y_pred[row_indices_sorted, index]</span><br><span class="line"></span><br><span class="line">        tp = (y_true_cls == <span class="number">1</span>)</span><br><span class="line">        fp = (y_true_cls == <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个位置是 top-i 的 fp 和 tp</span></span><br><span class="line">        fp = numpy.cumsum(fp)</span><br><span class="line">        tp = numpy.cumsum(tp)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 一共有多少预测正确的标签</span></span><br><span class="line">        npos = numpy.sum(y_true_cls)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># top-i 的 recall</span></span><br><span class="line">        rec = tp*<span class="number">1.0</span> / npos</span><br><span class="line"></span><br><span class="line">        <span class="comment"># avoid divide by zero in case the first detection matches a difficult</span></span><br><span class="line">        <span class="comment"># ground truth</span></span><br><span class="line">        <span class="comment"># top-i 的 precision</span></span><br><span class="line">        prec = tp*<span class="number">1.0</span> / numpy.maximum((tp + fp), numpy.finfo(numpy.float64).eps)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加上头和尾</span></span><br><span class="line">        mrec = numpy.concatenate(([<span class="number">0.</span>], rec, [<span class="number">1.</span>]))</span><br><span class="line">        mpre = numpy.concatenate(([<span class="number">0.</span>], prec, [<span class="number">0.</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute the precision envelope</span></span><br><span class="line">        <span class="comment"># 保证 P-R 曲线单调递减</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(mpre.size - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            mpre[i - <span class="number">1</span>] = numpy.maximum(mpre[i - <span class="number">1</span>], mpre[i])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># to calculate area under PR curve, look for points</span></span><br><span class="line">        <span class="comment"># where X axis (recall) changes value</span></span><br><span class="line">        i = numpy.where(mrec[<span class="number">1</span>:] != mrec[:<span class="number">-1</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># and sum (\Delta recall) * prec</span></span><br><span class="line">        <span class="comment"># 相当于求每个 recall 值对应的 precision 的均值</span></span><br><span class="line">        average_precisions.append(numpy.sum((mrec[i + <span class="number">1</span>] - mrec[i]) * mpre[i + <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h2 id="TnesorFlow-中的-AP"><a href="#TnesorFlow-中的-AP" class="headerlink" title="TnesorFlow 中的 AP"></a>TnesorFlow 中的 AP</h2><p>TensorFlow 中使用以下方法来计算 mAP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.compat.v1.metrics.average_precision_at_k(</span><br><span class="line">    labels, predictions, k, weights=None, metrics_collections=None,</span><br><span class="line">    updates_collections=None, name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>但是这里并不是按照分类的标准类计算 mAP，而是对于检索来计算 mAP，而对于检索来说，总是==要考虑@k==，也就是考虑检索出来前 k 个的结果。</p><p>还是对于上面的例子，TF 中将 pred 看成了 6 次检索，每次检索有一个待检索对象（真值标签），检索产生四个概率值，这四个概率值的和为一。不过要注意的是，通常检索的时候待检索对象往往大于一，并且检索所产生的多个概率值的和不一定为一。</p><p>对于以下这一个预测结果，当 k = 1 的时候，0.8 对应的是标签 0，因此 Precision = 0/1，Recall = 0/1；k = 2，0.1 对应的是标签 2，因此 Precision = 0/2，Recall = 0/1；k = 3，0.05 对应的是标签 1，因此 Precision = 1/3，Recall = 1/1；k = 4，此时已经全部检索到了，因此 Precision = 1/3，Recall = 1。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred = [0.8, 0.05, 0.1, 0.05]</span><br><span class="line">true = [1]</span><br></pre></td></tr></table></figure><p>其他预测结果同理可求，因此 mAP@3 = (1 + 1/3 + 1/2 + 0 + 1/3 + 0) / 6 = 0.3611。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在多分类任务中，有时候会用 mAP（Mean Average Precision）来表示分类的准确程度，如 VOC。其原因在于 mAP 能很好地评价分类的排序，而通常用的 accuracy 则往往会被最多的那个类别支配。&lt;/p&gt;
&lt;p&gt;要计算多分类的 mAP，则要先计算各个
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow遇到的坑</title>
    <link href="https://blog.patrickcty.cc/2020/03/09/tensorflow%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/03/09/tensorflow遇到的坑/</id>
    <published>2020-03-08T16:09:12.000Z</published>
    <updated>2020-03-08T16:22:16.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="IO-的坑"><a href="#IO-的坑" class="headerlink" title="IO 的坑"></a>IO 的坑</h2><p>对于<code>tf.io.decode_image()</code>，如果指定 <code>dtype=tf.float32</code>，那么会默认把 tensor 的值除以 255。</p><p>我之前就在这里直接指定了，后面还减掉了 ImageNet 的均值（[123.68, 116.779, 103.939]），所以对于任何的图，其 RGB 通道上的值都基本上是 [-123.68, -116.779, -103.939]，也难怪网络的预测结果都是数量最多的那两个标签……</p><p>如果要像我这么做的话，就不指定 dtype，然后再用 <code>tf.cast()</code> 将 tensor 转化为浮点型，这时候就只转换数据类型不改变值的大小了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;IO-的坑&quot;&gt;&lt;a href=&quot;#IO-的坑&quot; class=&quot;headerlink&quot; title=&quot;IO 的坑&quot;&gt;&lt;/a&gt;IO 的坑&lt;/h2&gt;&lt;p&gt;对于&lt;code&gt;tf.io.decode_image()&lt;/code&gt;，如果指定 &lt;code&gt;dtype=tf.fl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>colab遇到的坑</title>
    <link href="https://blog.patrickcty.cc/2020/02/28/colab%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/02/28/colab遇到的坑/</id>
    <published>2020-02-28T10:11:35.000Z</published>
    <updated>2020-02-28T10:19:15.662Z</updated>
    
    <content type="html"><![CDATA[<h2 id="训练特别慢"><a href="#训练特别慢" class="headerlink" title="训练特别慢"></a>训练特别慢</h2><h3 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h3><p>训练的数据集不要放在 google driver 里面，读取里面的文件特别慢，把 google driver 的文件拷贝到 colab 里面也特别慢。（不过把 colab 文件拷贝过去特别快）</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>直接再把相关的文件下载一遍，记得提前写好相关操作的脚本，每次直接执行一下。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!wget http://www.cs.bu.edu/groups/ivc/data/SOS/ESOS.zip;mkdir data;mkdir data/ESOS;mv ESOS.zip data/ESOS;unzip ESOS.zip &gt; zip.log</span><br><span class="line">from utils.dataset_utils.SOS import create_csv</span><br><span class="line">create_csv(&apos;/content/data/ESOS&apos;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;训练特别慢&quot;&gt;&lt;a href=&quot;#训练特别慢&quot; class=&quot;headerlink&quot; title=&quot;训练特别慢&quot;&gt;&lt;/a&gt;训练特别慢&lt;/h2&gt;&lt;h3 id=&quot;坑&quot;&gt;&lt;a href=&quot;#坑&quot; class=&quot;headerlink&quot; title=&quot;坑&quot;&gt;&lt;/a&gt;坑&lt;/h
      
    
    </summary>
    
    
  </entry>
  
</feed>
