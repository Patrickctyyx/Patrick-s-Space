<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Patrick&#39;s Space</title>
  <subtitle>Stay hungry, stay foolish!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.patrickcty.cc/"/>
  <updated>2020-11-18T12:35:17.473Z</updated>
  <id>https://blog.patrickcty.cc/</id>
  
  <author>
    <name>Patrick</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch使用DDP进行分布式训练</title>
    <link href="https://blog.patrickcty.cc/2020/11/18/PyTorch%E4%BD%BF%E7%94%A8DDP%E8%BF%9B%E8%A1%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    <id>https://blog.patrickcty.cc/2020/11/18/PyTorch使用DDP进行分布式训练/</id>
    <published>2020-11-18T12:31:01.000Z</published>
    <updated>2020-11-18T12:35:17.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>DDP 是 DistributedDataParallel 的简写，用来进行分布式训练，可以是单主机多 GPU 也可以是多主机多 GPU，以下均从单主机多 GPU 来介绍。其原理是把模型复制到其他的 GPU 上，然后在训练的过程中汇总梯度，进行迭代，从感知上就像是增大了 N 倍的显存。</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>具体的操作是产生多个进程，每个进程在一个 GPU 上训练，然后结果自动地在主进程中进行汇总。因此启动方式需要通过 <code>torch.distributed.launch</code> 来启动，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python -m torch.distributed.launch --nproc_per_node=2 main.py</div></pre></td></tr></table></figure>
<p>其中 <code>nproc_per_node</code> 是要使用的总的 GPU 数，如果一台主机上有多个 GPU，但是只想用其中的部分来进行训练，则可以用以下命令来启动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CUDA_VISIBLE_DEVICES=1,3,5 python -m torch.distributed.launch --nproc_per_node=3 main.py</div></pre></td></tr></table></figure>
<p>如果想调试分布式的代码，那么用以下方式来启动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">NCCL_DEBUG=INFO python -m torch.distributed.launch --nproc_per_node=2 main.py</div></pre></td></tr></table></figure>
<h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><p>为了让程序能很好地与 GPU 交互，<code>torch.distributed.launch</code> 在启动进程的时候会传入 <code>local_rank</code> 参数，用来标识 GPU，因此我们要在训练脚本中加入相应的参数。值得注意的是，<code>local_rank</code> 永远是从零开始。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">parser.add_argument(<span class="string">'--local_rank'</span>, type=int, default=<span class="number">0</span>)</div></pre></td></tr></table></figure>
<h2 id="初始化分布式环境"><a href="#初始化分布式环境" class="headerlink" title="初始化分布式环境"></a>初始化分布式环境</h2><p>首先要初始化进程组，对于单主机来说就用下面简单的语句即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</div><div class="line"></div><div class="line">opt = parser.parse_args()  <span class="comment"># 解析命令行参数</span></div><div class="line">torch.cuda.set_device(opt.local_rank)</div><div class="line">dist.init_process_group(<span class="string">'nccl'</span>)</div><div class="line">device = torch.device(f<span class="string">'cuda:&#123;opt.local_rank&#125;'</span>)</div></pre></td></tr></table></figure>
<h2 id="构建分布式模型"><a href="#构建分布式模型" class="headerlink" title="构建分布式模型"></a>构建分布式模型</h2><p>分布式环境下默认 BN 是在主 GPU 上进行计算，然后同步到其他 GPU，因此使用普通 BN 的时候不能充分发挥分布式训练中大 batch size 的优势。如果使用 Sync BN 则会解决这个问题，这个转换也可以使用一个函数来完成。</p>
<p>之后把模型转换为 DDP 模型即可，注意的是每一个进程都会初始化一个 DDP 模型，<code>device_id</code> 指的是当前进程要用到的 GPU 标号列表。因为我们通常一个进程一个 GPU，因此这里使用 <code>[opt.local_rank]</code> 即可，输出的话是会输出到单个设备上，因此就不用转换为 list。需要注意的是，Sync BN 不支持单进程多 GPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</div><div class="line"></div><div class="line">model = ResNet()</div><div class="line">model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)</div><div class="line">model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)</div></pre></td></tr></table></figure>
<h2 id="获得分布式-data-loader"><a href="#获得分布式-data-loader" class="headerlink" title="获得分布式 data loader"></a>获得分布式 data loader</h2><p>分布式训练的过程中我们要保证每个 GPU 取到的是不同的数据，因此不能直接使用普通的 Dataloader，要传入一个 sampler 参数，具体也很简单，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</div><div class="line"></div><div class="line">dataset = SomeDataset(image_root, gt_root, trainsize)</div><div class="line"><span class="comment"># 要从原来 dataset 得到一个分布式 sampler</span></div><div class="line">sampler = data.distributed.DistributedSampler(dataset)</div><div class="line">shuffle = <span class="keyword">False</span>  <span class="comment"># sampler 与 shuffle 不兼容</span></div><div class="line"></div><div class="line">data_loader = data.DataLoader(dataset=dataset,</div><div class="line">                              batch_size=batchsize,</div><div class="line">                              shuffle=shuffle,</div><div class="line">                              num_workers=num_workers,</div><div class="line">                              pin_memory=pin_memory,</div><div class="line">                              sampler=sampler)</div><div class="line"><span class="keyword">return</span> data_loader</div></pre></td></tr></table></figure>
<p>dataloader 中要传入 sampler 作为参数，其他的注意事项在注释中</p>
<h2 id="获得总的-loss"><a href="#获得总的-loss" class="headerlink" title="获得总的 loss"></a>获得总的 loss</h2><p>总的准备工作都完成了，接下来就是像平常一样训练了。但是每个进程中的 loss 都是通过自己的输入得到的，如果要得到总的 loss 则需要手动同步一下，具体操作如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_tensor</span><span class="params">(tensor: torch.Tensor)</span> -&gt; torch.Tensor:</span></div><div class="line">    rt = tensor.clone()</div><div class="line">    dist.all_reduce(rt, op=dist.ReduceOp.SUM)</div><div class="line">    rt /= dist.get_world_size()  <span class="comment"># 这是进程的数量</span></div><div class="line">    <span class="keyword">return</span> rt</div><div class="line">    </div><div class="line">    </div><div class="line">reduced_loss1 = reduce_tensor(loss1.data).item()</div></pre></td></tr></table></figure>
<p><code>all_reduce</code> 会自动获取各个进程中同名 tensor，然后通过指定的 op 来进行计算，最后再同步到各个进程当中，也就是说这是一个原地的操作。为了避免可能产生的影响，这里不是直接对原来的 tensor 进行 reduce，而是先取了副本。</p>
<h2 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h2><p>这里有一个大坑！虽然参数会在各个进程中汇总，但是实际保存的模型的 state_dict 和非分布式的还是有区别的，如果直接载入很可能会出错，解决方法下面会提到。</p>
<h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>如果你没有掉进上一节的坑里面，那么测试阶段的代码可以和非分布式测试的完全相同。</p>
<h2 id="打印-log"><a href="#打印-log" class="headerlink" title="打印 log"></a>打印 log</h2><p>因为各个进程代码完全一样，因此打印结果也是打印 N 份，一个简单的解决方法就是判断当前的 <code>local_rank</code>，只有当其为特定值的时候才打印。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> opt.local_rank == <span class="number">0</span>:</div><div class="line">    print(<span class="string">'some log'</span>)</div></pre></td></tr></table></figure>
<h2 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a>遇到的坑</h2><p>DDP 训练的模型通过非 DDP 进行加载之后结果非常差。</p>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>保存的模型 state_dict 前缀多了一个 <code>module.</code>，这样在 <code>strict=False</code> 下载入参数的时候就相当于载入了个寂寞，因此结果很差</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol>
<li>加载 DDP 模型的时候重新构造 state_dict，将二者名称统一<a href="https://discuss.pytorch.org/t/failed-to-load-model-trained-by-ddp-for-inference/84841" target="_blank" rel="external">[1]</a>:</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_load</span><span class="params">(state_dict)</span>:</span></div><div class="line"></div><div class="line">    new_state_dict = OrderedDict()</div><div class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items():</div><div class="line">        name = k[<span class="number">7</span>:]  <span class="comment"># remove 'module.' of DataParallel/DistributedDataParallel</span></div><div class="line">        new_state_dict[name] = v</div><div class="line"></div><div class="line">    <span class="keyword">return</span> new_state_dict</div></pre></td></tr></table></figure>
<ol>
<li>（==推荐==）保存 DDP 模型的时候直接保存不包含 <code>module.</code> 前缀<a href="https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/17" target="_blank" rel="external">[2]</a>：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch.save(model.module.state_dict(), path_to_file)</div></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>如果你觉得还是讲的不清楚，那就看<a href="https://zhuanlan.zhihu.com/p/145427849" target="_blank" rel="external">这篇文章</a>吧，我就是跟着这篇文章来写的。更深入的理解分析就看<a href="https://zhuanlan.zhihu.com/p/250471767" target="_blank" rel="external">这个系列</a>。</p>
<h2 id="总的代码"><a href="#总的代码" class="headerlink" title="总的代码"></a>总的代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</div><div class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</div><div class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</div><div class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> SyncBatchNorm</div><div class="line"><span class="keyword">from</span> torch.distributed <span class="keyword">import</span> ReduceOp</div><div class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel</div><div class="line"></div><div class="line"><span class="keyword">from</span> data <span class="keyword">import</span> get_loader</div><div class="line"><span class="keyword">from</span> model.CPD_ResNet_models <span class="keyword">import</span> CPD_ResNet</div><div class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> clip_gradient, adjust_lr, save_single_plot, get_train_parser, init_workspace</div><div class="line"></div><div class="line"></div><div class="line">parser = get_train_parser(loader_type=<span class="string">'rgb'</span>)</div><div class="line">parser.add_argument(<span class="string">'--local_rank'</span>, type=int, default=<span class="number">0</span>)</div><div class="line">opt = parser.parse_args()</div><div class="line">main_proc = <span class="keyword">True</span> <span class="keyword">if</span> opt.local_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">False</span></div><div class="line">basedir = init_workspace(opt, main_proc)</div><div class="line"></div><div class="line"><span class="comment"># init distribute environment</span></div><div class="line">torch.cuda.set_device(opt.local_rank)</div><div class="line">dist.init_process_group(<span class="string">'nccl'</span>)</div><div class="line">device = torch.device(f<span class="string">'cuda:&#123;opt.local_rank&#125;'</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> main_proc:</div><div class="line">    print(<span class="string">'Learning Rate: &#123;&#125; Model type: &#123;&#125;'</span>.format(opt.lr, opt.model))</div><div class="line"></div><div class="line"><span class="comment"># build models</span></div><div class="line">model = CPD_ResNet()</div><div class="line">model = SyncBatchNorm.convert_sync_batchnorm(model).to(device)</div><div class="line">model = DistributedDataParallel(model, device_ids=[opt.local_rank], output_device=opt.local_rank)</div><div class="line"><span class="keyword">if</span> opt.print_model <span class="keyword">and</span> main_proc:</div><div class="line">    print(model)</div><div class="line">    summary(model, (<span class="number">4</span>, opt.trainsize, opt.trainsize))</div><div class="line"></div><div class="line">optimizer = torch.optim.Adam(model.parameters(), opt.lr)</div><div class="line"></div><div class="line"><span class="comment"># build distribute data loader</span></div><div class="line">train_loader = get_loader(opt.train_img_dir, opt.train_gt_dir, loader_type=<span class="string">'rgb'</span>,</div><div class="line">                          batchsize=opt.batchsize, trainsize=opt.trainsize, dist=<span class="keyword">True</span>)</div><div class="line">total_step = len(train_loader)</div><div class="line"></div><div class="line"><span class="comment"># build loss</span></div><div class="line">CE = torch.nn.BCEWithLogitsLoss().to(device)</div><div class="line"></div><div class="line"><span class="comment"># save train results in df</span></div><div class="line">df_step = pd.DataFrame(columns=(<span class="string">'loss1'</span>, <span class="string">'loss2'</span>))</div><div class="line">df_epoch = pd.DataFrame(columns=(<span class="string">'loss1'</span>, <span class="string">'loss2'</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_tensor</span><span class="params">(tensor: torch.Tensor)</span> -&gt; torch.Tensor:</span></div><div class="line">    rt = tensor.clone()</div><div class="line">    dist.all_reduce(rt, op=dist.ReduceOp.SUM)</div><div class="line">    rt /= dist.get_world_size()</div><div class="line">    <span class="keyword">return</span> rt</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_loader, model, optimizer, epoch)</span>:</span></div><div class="line">    model.train()</div><div class="line">    epoch_loss1 = []</div><div class="line">    epoch_loss2 = []</div><div class="line">    <span class="keyword">for</span> i, pack <span class="keyword">in</span> enumerate(train_loader, start=<span class="number">1</span>):</div><div class="line">        <span class="comment"># if main_proc:</span></div><div class="line">        <span class="comment">#     print('running')</span></div><div class="line">        optimizer.zero_grad()</div><div class="line">        images, gts = pack</div><div class="line"></div><div class="line">        images = images.to(device)</div><div class="line">        gts = gts.to(device)</div><div class="line"></div><div class="line">        <span class="comment"># if main_proc:</span></div><div class="line">        <span class="comment">#     print('data done')</span></div><div class="line"></div><div class="line">        atts, dets = model(images)</div><div class="line"></div><div class="line">        loss1 = CE(atts, gts)</div><div class="line">        loss2 = CE(dets, gts)</div><div class="line"></div><div class="line">        <span class="comment"># if main_proc:</span></div><div class="line">        <span class="comment">#     print('CE done')</span></div><div class="line"></div><div class="line">        loss = loss1 + loss2</div><div class="line">        loss.backward()</div><div class="line"></div><div class="line">        <span class="comment"># if main_proc:</span></div><div class="line">        <span class="comment">#     print('loss done')</span></div><div class="line"></div><div class="line">        <span class="comment"># save loss results</span></div><div class="line">        reduced_loss1 = reduce_tensor(loss1.data).item()</div><div class="line">        <span class="comment"># if main_proc:</span></div><div class="line">        <span class="comment">#     print('reduce loss1 done')</span></div><div class="line">        reduced_loss2 = reduce_tensor(loss2.data).item()</div><div class="line">        <span class="comment"># if main_proc:</span></div><div class="line">        <span class="comment">#     print('reduce loss2 done')</span></div><div class="line">        <span class="keyword">if</span> main_proc:</div><div class="line">            epoch_loss1.append(reduced_loss1)</div><div class="line">            epoch_loss2.append(reduced_loss2)</div><div class="line">            df_step.loc[df_step.shape[<span class="number">0</span>]] = (reduced_loss1, reduced_loss2)</div><div class="line"></div><div class="line">        clip_gradient(optimizer, opt.clip)</div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (i % <span class="number">400</span> == <span class="number">0</span> <span class="keyword">or</span> i == total_step) <span class="keyword">and</span> main_proc:</div><div class="line">            print(<span class="string">'&#123;&#125; Epoch [&#123;:03d&#125;/&#123;:03d&#125;], Step [&#123;:04d&#125;/&#123;:04d&#125;], Loss1: &#123;:.4f&#125; Loss2: &#123;:0.4f&#125;'</span>.</div><div class="line">                  format(datetime.now(), epoch, opt.epoch, i, total_step,</div><div class="line">                         np.mean(epoch_loss1), np.mean(epoch_loss2)))</div><div class="line"></div><div class="line">    <span class="keyword">if</span> main_proc:</div><div class="line">        df_epoch.loc[df_epoch.shape[<span class="number">0</span>]] = (np.mean(epoch_loss1), np.mean(epoch_loss2))</div><div class="line">        save_path = os.path.join(basedir, <span class="string">'checkpoints'</span>)</div><div class="line">        os.makedirs(save_path, exist_ok=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</div><div class="line">            <span class="comment"># 注意，这里保存的是 model.module.state_dict()，这样测试的时候就不用做额外的处理</span></div><div class="line">            torch.save(model.module.state_dict(), os.path.join(save_path, <span class="string">'CPD_&#123;&#125;.pth'</span>.format(epoch + <span class="number">1</span>)))</div><div class="line"></div><div class="line"></div><div class="line">print(<span class="string">"GPU &#123;&#125;: Let's go!"</span>.format(opt.local_rank))</div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, opt.epoch + <span class="number">1</span>):</div><div class="line">    adjust_lr(optimizer, opt.lr, epoch, opt.decay_rate, opt.decay_epoch)</div><div class="line">    train(train_loader, model, optimizer, epoch)</div><div class="line"></div><div class="line"><span class="keyword">if</span> main_proc:</div><div class="line">    df_epoch.to_csv(os.path.join(basedir, <span class="string">'epoch_loss.csv'</span>), index=<span class="keyword">False</span>)</div><div class="line">    df_step.to_csv(os.path.join(basedir, <span class="string">'step_loss.csv'</span>), index=<span class="keyword">False</span>)</div><div class="line">    save_single_plot(df_epoch, (<span class="string">'loss1'</span>, <span class="string">'loss2'</span>), <span class="string">'epoch'</span>, <span class="string">'loss'</span>, basedir, <span class="string">'epoch'</span>)</div><div class="line">    save_single_plot(df_step, (<span class="string">'loss1'</span>, <span class="string">'loss2'</span>), <span class="string">'step'</span>, <span class="string">'loss'</span>, basedir, <span class="string">'step'</span>)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;p&gt;DDP 是 DistributedDataParallel 的简写，用来进行分布式训练，可以是单主机多 GPU 也可以是多主机多 GPU，以
    
    </summary>
    
    
      <category term="PyTorch" scheme="https://blog.patrickcty.cc/tags/PyTorch/"/>
    
      <category term="分布式训练" scheme="https://blog.patrickcty.cc/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
      <category term="DDP" scheme="https://blog.patrickcty.cc/tags/DDP/"/>
    
  </entry>
  
  <entry>
    <title>SimCLR 代码分析</title>
    <link href="https://blog.patrickcty.cc/2020/11/10/simclr%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>https://blog.patrickcty.cc/2020/11/10/simclr代码分析/</id>
    <published>2020-11-10T09:08:49.000Z</published>
    <updated>2020-11-10T11:58:38.866Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章分析 <a href="https://github.com/sthalles/SimCLR" target="_blank" rel="external">PyTorch SimCLR: A Simple Framework for Contrastive Learning of Visual Representations</a>。使用自监督的方法来生成特征表示。其中 loss 部分实现得非常巧妙，因此特地拿出来分析。</p>
<h2 id="SimCLR-data-aug-dataset-wrapper-py"><a href="#SimCLR-data-aug-dataset-wrapper-py" class="headerlink" title="SimCLR/data_aug/dataset_wrapper.py"></a>SimCLR/data_aug/dataset_wrapper.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</div><div class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> SubsetRandomSampler</div><div class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</div><div class="line"><span class="keyword">from</span> data_aug.gaussian_blur <span class="keyword">import</span> GaussianBlur</div><div class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</div><div class="line"></div><div class="line">np.random.seed(<span class="number">0</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataSetWrapper</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, batch_size, num_workers, valid_size, input_shape, s)</span>:</span></div><div class="line">        self.batch_size = batch_size</div><div class="line">        self.num_workers = num_workers</div><div class="line">        self.valid_size = valid_size</div><div class="line">        self.s = s</div><div class="line">        self.input_shape = eval(input_shape)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data_loaders</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 数据增强</span></div><div class="line">        data_augment = self._get_simclr_pipeline_transform()</div><div class="line"></div><div class="line">        <span class="comment"># 使用官方的 STL 数据集</span></div><div class="line">        train_dataset = datasets.STL10(<span class="string">'./data'</span>, split=<span class="string">'train+unlabeled'</span>, download=<span class="keyword">True</span>,</div><div class="line">                                       transform=SimCLRDataTransform(data_augment))</div><div class="line"></div><div class="line">        <span class="comment"># 随机划分训练集与验证集</span></div><div class="line">        train_loader, valid_loader = self.get_train_validation_data_loaders(train_dataset)</div><div class="line">        <span class="keyword">return</span> train_loader, valid_loader</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_simclr_pipeline_transform</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 运用了很多数据增强的方法</span></div><div class="line">        <span class="comment"># get a set of data augmentation transformations as described in the SimCLR paper.</span></div><div class="line">        color_jitter = transforms.ColorJitter(<span class="number">0.8</span> * self.s, <span class="number">0.8</span> * self.s, <span class="number">0.8</span> * self.s, <span class="number">0.2</span> * self.s)</div><div class="line">        data_transforms = transforms.Compose([transforms.RandomResizedCrop(size=self.input_shape[<span class="number">0</span>]),</div><div class="line">                                              transforms.RandomHorizontalFlip(),</div><div class="line">                                              transforms.RandomApply([color_jitter], p=<span class="number">0.8</span>),</div><div class="line">                                              transforms.RandomGrayscale(p=<span class="number">0.2</span>),</div><div class="line">                                              GaussianBlur(kernel_size=int(<span class="number">0.1</span> * self.input_shape[<span class="number">0</span>])),</div><div class="line">                                              transforms.ToTensor()])</div><div class="line">        <span class="keyword">return</span> data_transforms</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_train_validation_data_loaders</span><span class="params">(self, train_dataset)</span>:</span></div><div class="line">        <span class="comment"># 随机划分训练集和验证集 </span></div><div class="line">        <span class="comment"># obtain training indices that will be used for validation</span></div><div class="line">        num_train = len(train_dataset)</div><div class="line">        indices = list(range(num_train))</div><div class="line">        np.random.shuffle(indices)</div><div class="line"></div><div class="line">        split = int(np.floor(self.valid_size * num_train))</div><div class="line">        train_idx, valid_idx = indices[split:], indices[:split]</div><div class="line"></div><div class="line">        <span class="comment"># define samplers for obtaining training and validation batches</span></div><div class="line">        train_sampler = SubsetRandomSampler(train_idx)</div><div class="line">        valid_sampler = SubsetRandomSampler(valid_idx)</div><div class="line"></div><div class="line">        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=train_sampler,</div><div class="line">                                  num_workers=self.num_workers, drop_last=<span class="keyword">True</span>, shuffle=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">        valid_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=valid_sampler,</div><div class="line">                                  num_workers=self.num_workers, drop_last=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> train_loader, valid_loader</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimCLRDataTransform</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, transform)</span>:</span></div><div class="line">        self.transform = transform</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></div><div class="line">        <span class="comment"># 同一个 sample 增强两次得到两个输入</span></div><div class="line">        xi = self.transform(sample)</div><div class="line">        xj = self.transform(sample)</div><div class="line">        <span class="keyword">return</span> xi, xj</div></pre></td></tr></table></figure>
<p>这部分要注意的主要就是对于一个数据数据得到两个增强的数据。</p>
<h2 id="SimCLR-models-resnet-simclr-py"><a href="#SimCLR-models-resnet-simclr-py" class="headerlink" title="SimCLR/models/resnet_simclr.py"></a>SimCLR/models/resnet_simclr.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNetSimCLR</span><span class="params">(nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, base_model, out_dim)</span>:</span></div><div class="line">        super(ResNetSimCLR, self).__init__()</div><div class="line">        self.resnet_dict = &#123;<span class="string">"resnet18"</span>: models.resnet18(pretrained=<span class="keyword">False</span>),</div><div class="line">                            <span class="string">"resnet50"</span>: models.resnet50(pretrained=<span class="keyword">False</span>)&#125;</div><div class="line"></div><div class="line">        resnet = self._get_basemodel(base_model)</div><div class="line">        num_ftrs = resnet.fc.in_features</div><div class="line"></div><div class="line">        self.features = nn.Sequential(*list(resnet.children())[:<span class="number">-1</span>])</div><div class="line"></div><div class="line">        <span class="comment"># projection MLP</span></div><div class="line">        self.l1 = nn.Linear(num_ftrs, num_ftrs)</div><div class="line">        self.l2 = nn.Linear(num_ftrs, out_dim)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_basemodel</span><span class="params">(self, model_name)</span>:</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            model = self.resnet_dict[model_name]</div><div class="line">            print(<span class="string">"Feature extractor:"</span>, model_name)</div><div class="line">            <span class="keyword">return</span> model</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            <span class="keyword">raise</span> (<span class="string">"Invalid model name. Check the config file and pass one of: resnet18 or resnet50"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        h = self.features(x)</div><div class="line">        h = h.squeeze()</div><div class="line"></div><div class="line">        <span class="comment"># 用了两个 FC 层对特征进行映射</span></div><div class="line">        <span class="comment"># 这个实现中设置最终得到 256 维向量</span></div><div class="line">        x = self.l1(h)</div><div class="line">        x = F.relu(x)</div><div class="line">        x = self.l2(x)</div><div class="line">        <span class="keyword">return</span> h, x</div></pre></td></tr></table></figure>
<h2 id="SimCLR-loss-nt-xent-py"><a href="#SimCLR-loss-nt-xent-py" class="headerlink" title="SimCLR/loss/nt_xent.py"></a>SimCLR/loss/nt_xent.py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NTXentLoss</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, device, batch_size, temperature, use_cosine_similarity)</span>:</span></div><div class="line">        super(NTXentLoss, self).__init__()</div><div class="line">        self.batch_size = batch_size</div><div class="line">        self.temperature = temperature</div><div class="line">        self.device = device</div><div class="line">        self.softmax = torch.nn.Softmax(dim=<span class="number">-1</span>)</div><div class="line">        <span class="comment"># 标注负样本所在的位置</span></div><div class="line">        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)</div><div class="line">        self.similarity_function = self._get_similarity_function(use_cosine_similarity)</div><div class="line">        self.criterion = torch.nn.CrossEntropyLoss(reduction=<span class="string">"sum"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_similarity_function</span><span class="params">(self, use_cosine_similarity)</span>:</span></div><div class="line">        <span class="keyword">if</span> use_cosine_similarity:</div><div class="line">            self._cosine_similarity = torch.nn.CosineSimilarity(dim=<span class="number">-1</span>)</div><div class="line">            <span class="keyword">return</span> self._cosine_simililarity</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self._dot_simililarity</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_correlated_mask</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># 每个图像都经过两次增强，得到 2 * self.batch_size 个输入图像</span></div><div class="line">        <span class="comment"># 两两求相似度之后得到 2N * 2N 矩阵</span></div><div class="line">        <span class="comment"># 其中对角线以及上 N 下 N 对角线是同一个图像，也就是正样本</span></div><div class="line">        diag = np.eye(<span class="number">2</span> * self.batch_size)</div><div class="line">        l1 = np.eye((<span class="number">2</span> * self.batch_size), <span class="number">2</span> * self.batch_size, k=-self.batch_size)</div><div class="line">        l2 = np.eye((<span class="number">2</span> * self.batch_size), <span class="number">2</span> * self.batch_size, k=self.batch_size)</div><div class="line">        mask = torch.from_numpy((diag + l1 + l2))</div><div class="line">        <span class="comment"># 取反就是负样本</span></div><div class="line">        mask = (<span class="number">1</span> - mask).type(torch.bool)</div><div class="line">        <span class="keyword">return</span> mask.to(self.device)</div><div class="line"></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_dot_simililarity</span><span class="params">(x, y)</span>:</span></div><div class="line">        v = torch.tensordot(x.unsqueeze(<span class="number">1</span>), y.T.unsqueeze(<span class="number">0</span>), dims=<span class="number">2</span>)</div><div class="line">        <span class="comment"># x shape: (M, 1, C)</span></div><div class="line">        <span class="comment"># y shape: (1, C, N)</span></div><div class="line">        <span class="comment"># v shape: (M, N)</span></div><div class="line">        <span class="comment"># 因为有 batch size，因此得到的相似度是一个矩阵</span></div><div class="line">        <span class="keyword">return</span> v</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_cosine_simililarity</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        <span class="comment"># x shape: (M, 1, C)</span></div><div class="line">        <span class="comment"># y shape: (1, N, C)</span></div><div class="line">        <span class="comment"># v shape: (M, N)</span></div><div class="line">        v = self._cosine_similarity(x.unsqueeze(<span class="number">1</span>), y.unsqueeze(<span class="number">0</span>))</div><div class="line">        <span class="keyword">return</span> v</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, zis, zjs)</span>:</span></div><div class="line">        <span class="comment"># 组合成 2N 维向量，其中第 i 和 i + N 是同一个样本增强结果</span></div><div class="line">        representations = torch.cat([zjs, zis], dim=<span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="comment"># 计算相似度，得到 2N * 2N 矩阵</span></div><div class="line">        similarity_matrix = self.similarity_function(representations, representations)</div><div class="line"></div><div class="line">        <span class="comment"># 找到正样本，其中中间对角线是自身相乘的，可以无视掉</span></div><div class="line">        <span class="comment"># filter out the scores from the positive samples</span></div><div class="line">        l_pos = torch.diag(similarity_matrix, self.batch_size)</div><div class="line">        r_pos = torch.diag(similarity_matrix, -self.batch_size)</div><div class="line">        <span class="comment"># view 是将向量 reshape，得到一个 2N 维向量</span></div><div class="line">        positives = torch.cat([l_pos, r_pos]).view(<span class="number">2</span> * self.batch_size, <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># 负样本，同样也是 reshape，得到 2N * (2N - 2) 矩阵</span></div><div class="line">        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="number">2</span> * self.batch_size, <span class="number">-1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># 串联起来之后维度是 2N * (2N - 1)，其中每一列中第一个都是正样本，其他为负样本</span></div><div class="line">        logits = torch.cat((positives, negatives), dim=<span class="number">1</span>)</div><div class="line">        logits /= self.temperature</div><div class="line"></div><div class="line">        <span class="comment"># 构造一个维度为 2N 的标签，0 对应相面的正样本</span></div><div class="line">        labels = torch.zeros(<span class="number">2</span> * self.batch_size).to(self.device).long()</div><div class="line">        <span class="comment"># 交叉熵展开之后就是文中 loss 的形式</span></div><div class="line">        loss = self.criterion(logits, labels)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> loss / (<span class="number">2</span> * self.batch_size)</div></pre></td></tr></table></figure>
<p>这是代码中最关键的部分，这里在一个 batch 中构造正负样本，通过相似矩阵的形似很巧妙地挖掘出了正负样本，最终也巧妙用交叉熵实现了以下 loss 的形式。</p>
<p><img src="https://static.jnugeek.cn/blog/loss_contrastive.png" alt="Contrastive loss"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;这篇文章分析 &lt;a href=&quot;https://github.com/sthalles/SimCLR&quot; target=&quot;_blank&quot; re
    
    </summary>
    
    
      <category term="Contrastive Learning" scheme="https://blog.patrickcty.cc/tags/Contrastive-Learning/"/>
    
      <category term="Deep Leanring" scheme="https://blog.patrickcty.cc/tags/Deep-Leanring/"/>
    
  </entry>
  
  <entry>
    <title>keras 中 dense 层输入秩大于二</title>
    <link href="https://blog.patrickcty.cc/2020/11/03/keras%E4%B8%ADdense%E5%B1%82%E8%BE%93%E5%85%A5%E7%A7%A9%E5%A4%A7%E4%BA%8E%E4%BA%8C/"/>
    <id>https://blog.patrickcty.cc/2020/11/03/keras中dense层输入秩大于二/</id>
    <published>2020-11-03T10:36:16.000Z</published>
    <updated>2020-11-03T10:51:44.888Z</updated>
    
    <content type="html"><![CDATA[<p>通常情况下输入到 Dense 层（又或者叫 FC 层）的张量是一个 (batch_size, length) 的秩为 2 的形式。比如 AlexNet 和 VGG，输出的特征图都会经过 flatten 操作降维到 (None, 1024)，然后才输入到 Dense 层中。</p>
<p>但是今天我在看 MaskX R-CNN 的时候发现输入并不是一个二维矩阵，而是一个三维的张量。Keras 文档中在处理 Dense 秩大于二的时候会将其通过一个矩阵乘法来改变输出最后一维的长度（秩不变）。这样处理不是真正意义上所有神经元全连接，参数上也比 flatten 再还原要小很多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x = layers.Input((<span class="number">81</span>, <span class="number">1024</span>))  <span class="comment"># (None, 81, 1024)</span></div><div class="line">y = layers.Dense(<span class="number">256</span>)  <span class="comment"># y shape: (None, 81, 256)</span></div><div class="line"><span class="comment"># 参数 W shape: (1024, 256) </span></div><div class="line"><span class="comment"># 参数 b shape: (256)</span></div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通常情况下输入到 Dense 层（又或者叫 FC 层）的张量是一个 (batch_size, length) 的秩为 2 的形式。比如 AlexNet 和 VGG，输出的特征图都会经过 flatten 操作降维到 (None, 1024)，然后才输入到 Dense 层中。&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>一个例子加深Python元类与描述符类理解</title>
    <link href="https://blog.patrickcty.cc/2020/10/27/%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E5%8A%A0%E6%B7%B1Python%E5%85%83%E7%B1%BB%E4%B8%8E%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%B1%BB%E7%90%86%E8%A7%A3/"/>
    <id>https://blog.patrickcty.cc/2020/10/27/一个例子加深Python元类与描述符类理解/</id>
    <published>2020-10-27T01:10:02.000Z</published>
    <updated>2020-10-27T03:14:40.447Z</updated>
    
    <content type="html"><![CDATA[<h2 id="通过函数注解来实现方法重载"><a href="#通过函数注解来实现方法重载" class="headerlink" title="通过函数注解来实现方法重载"></a>通过函数注解来实现方法重载</h2><p>最近在看 《Python Cookbook》，9.20 的示例涉及到非常多高级用法，有必要专门拿出来整理一下避免遗忘。这一节的目的是通过函数注解来实现方法重载，由于 Python 中对参数类型是没有硬性要求的，因此在 Python 中也没有方法重载这一特性。虽然函数注解能提示用户输入变量应该是什么类型，但实际上并没有类型检查与硬性的约束。在这一节之中就是用元类 + 描述符来实现这个功能。</p>
<h3 id="定义描述符类"><a href="#定义描述符类" class="headerlink" title="定义描述符类"></a>定义描述符类</h3><p>首先定义一个描述符类，用来将参数类型与对应的函数引用进行绑定，同时也可以通过传入的参数获取到函数引用。前者是使用一个注册函数来实现的，传入的是一个函数，得到这个函数可能的参数列表，然后将参数列表与函数绑定到一个字典中；后者是通过重写 <code>__call__</code> 方法来从输出参数找到对应函数，重写 <code>__get__</code> 方法将函数绑定 <code>self</code> 参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> types</div><div class="line"><span class="keyword">import</span> inspect</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiMethod</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    Represents a single multimethod.</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></div><div class="line">        self._methods = &#123;&#125;  <span class="comment"># 绑定参数类型与函数引用</span></div><div class="line">        self.__name__ = name</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(self, meth)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line">        Register a new method as a multimethod</div><div class="line">        '''</div><div class="line">        sig = inspect.signature(meth)  <span class="comment"># 用来获取函数的参数信息</span></div><div class="line"></div><div class="line">        <span class="comment"># Build a type-signature from the method's annotations</span></div><div class="line">        types = []</div><div class="line">        <span class="keyword">for</span> name, parm <span class="keyword">in</span> sig.parameters.items():</div><div class="line">            <span class="keyword">if</span> name == <span class="string">'self'</span>:  <span class="comment"># 忽视掉 self 参数</span></div><div class="line">                <span class="keyword">continue</span></div><div class="line">            <span class="keyword">if</span> parm.annotation <span class="keyword">is</span> inspect.Parameter.empty:  <span class="comment"># 必须要有函数注解</span></div><div class="line">                <span class="keyword">raise</span> TypeError(</div><div class="line">                    <span class="string">'Argument &#123;&#125; must be annotated with a type'</span>.format(name)</div><div class="line">                    )</div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> isinstance(parm.annotation, type):  <span class="comment"># 函数注解必须要是类型</span></div><div class="line">                <span class="keyword">raise</span> TypeError(</div><div class="line">                    <span class="string">'Argument &#123;&#125; annotation must be a type'</span>.format(name)</div><div class="line">                    )</div><div class="line">            <span class="comment"># 如果遇到有默认值的参数，那么在输入的时候不带这一项也可以</span></div><div class="line">            <span class="comment"># 因此每一个都要单独作为一个参数类型的入口</span></div><div class="line">            <span class="keyword">if</span> parm.default <span class="keyword">is</span> <span class="keyword">not</span> inspect.Parameter.empty:</div><div class="line">                self._methods[tuple(types)] = meth</div><div class="line">            <span class="comment"># 因为不支持关键字参数，因此一旦传入了某个参数</span></div><div class="line">            <span class="comment"># 其前面所有参数都得传入</span></div><div class="line">            types.append(parm.annotation)</div><div class="line"></div><div class="line">        self._methods[tuple(types)] = meth</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line">        Call a method based on type signature of the arguments</div><div class="line">        这样创建实例之后实例直接就是一个可调用对象了。</div><div class="line">        '''</div><div class="line">        <span class="comment"># 首先将传入参数转换为类型元组，忽视掉 self 参数</span></div><div class="line">        types = tuple(type(arg) <span class="keyword">for</span> arg <span class="keyword">in</span> args[<span class="number">1</span>:])</div><div class="line">        <span class="comment"># 然后通过元组来获取对应的方法引用</span></div><div class="line">        meth = self._methods.get(types, <span class="keyword">None</span>)</div><div class="line">        <span class="comment"># 找到了就表明是支持的参数列表</span></div><div class="line">        <span class="keyword">if</span> meth:</div><div class="line">            <span class="keyword">return</span> meth(*args)</div><div class="line">        <span class="keyword">else</span>:  <span class="comment"># 否则类型就不对应</span></div><div class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'No matching method for types &#123;&#125;'</span>.format(types))</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, instance, cls)</span>:</span></div><div class="line">        <span class="string">'''</span></div><div class="line">        Descriptor method needed to make calls work in a class</div><div class="line">        这里主要是为了绑定 self，不然直接调用会提示少一个参数</div><div class="line">        '''</div><div class="line">        <span class="keyword">if</span> instance <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> types.MethodType(self, instance)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self</div></pre></td></tr></table></figure>
<h3 id="定义元类"><a href="#定义元类" class="headerlink" title="定义元类"></a>定义元类</h3><p>接下来就是要使用元类把上面的集成到类中，最好的方法就是在创建的时候能通过描述符来绑定方法。这可以通过元类中的 clsdict 来实现。本文中的实现方法是修改 clsdict 的行为，在绑定方法的时候合并同名不同参数的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiDict</span><span class="params">(dict)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    Special dictionary to build multimethods in a metaclass</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, key, value)</span>:</span></div><div class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self:</div><div class="line">            <span class="comment"># If key already exists, it must be a multimethod or callable</span></div><div class="line">            current_value = self[key]</div><div class="line">            <span class="keyword">if</span> isinstance(current_value, MultiMethod):</div><div class="line">                <span class="comment"># 某个名字的方法出现第三次，这次就直接注册了</span></div><div class="line">                current_value.register(value)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                <span class="comment"># 某个名字的方法出现第二次，那么首先先创建一个描述符</span></div><div class="line">                mvalue = MultiMethod(key)</div><div class="line">                <span class="comment"># 分别注册这两个方法</span></div><div class="line">                mvalue.register(current_value)</div><div class="line">                mvalue.register(value)</div><div class="line">                <span class="comment"># 将描述符绑定到类上</span></div><div class="line">                super().__setitem__(key, mvalue)</div><div class="line">        <span class="keyword">else</span>:  </div><div class="line">            <span class="comment"># 如果是第一次见到的，那直接设置属性</span></div><div class="line">            <span class="comment"># 因为这个时候不会出现同名方法</span></div><div class="line">            super().__setitem__(key, value)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultipleMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    Metaclass that allows multiple dispatch of methods</div><div class="line">    '''</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, clsname, bases, clsdict)</span>:</span></div><div class="line">        <span class="comment"># 这里的 clsdict 就是下面的 MultiDict</span></div><div class="line">        <span class="comment"># 在 __new__ 中会创建类，也就是会将方法绑定到 clsdict 中</span></div><div class="line">        <span class="comment"># 在绑定的时候，由于 clsdict 重写了 __setitem__</span></div><div class="line">        <span class="comment"># 因此不会直接绑定方法，而是会绑定到描述符类</span></div><div class="line">        <span class="keyword">return</span> type.__new__(cls, clsname, bases, dict(clsdict))</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__prepare__</span><span class="params">(cls, clsname, bases)</span>:</span></div><div class="line">        <span class="comment"># 这个函数在调用 __new__ 之前调用，返回一个映射对象</span></div><div class="line">        <span class="keyword">return</span> MultiDict()</div></pre></td></tr></table></figure>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Some example classes that use multiple dispatch</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spam</span><span class="params">(metaclass=MultipleMeta)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, x:int, y:int)</span>:</span></div><div class="line">        print(<span class="string">'Bar 1:'</span>, x, y)</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, s:str, n:int = <span class="number">0</span>)</span>:</span></div><div class="line">        print(<span class="string">'Bar 2:'</span>, s, n)</div><div class="line"></div><div class="line"><span class="comment"># Example: overloaded __init__</span></div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Date</span><span class="params">(metaclass=MultipleMeta)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, year: int, month:int, day:int)</span>:</span></div><div class="line">        self.year = year</div><div class="line">        self.month = month</div><div class="line">        self.day = day</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        t = time.localtime()</div><div class="line">        self.__init__(t.tm_year, t.tm_mon, t.tm_mday)</div><div class="line"></div><div class="line">s = Spam()</div><div class="line">s.bar(<span class="number">2</span>, <span class="number">3</span>)</div><div class="line">s.bar(<span class="string">'hello'</span>)</div><div class="line">s.bar(<span class="string">'hello'</span>, <span class="number">5</span>)</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    s.bar(<span class="number">2</span>, <span class="string">'hello'</span>)</div><div class="line"><span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</div><div class="line">    print(e)</div><div class="line"></div><div class="line"><span class="comment"># Overloaded __init__</span></div><div class="line">d = Date(<span class="number">2012</span>, <span class="number">12</span>, <span class="number">21</span>)</div><div class="line">print(d.year, d.month, d.day)</div><div class="line"><span class="comment"># Get today's date</span></div><div class="line">e = Date()</div><div class="line">print(e.year, e.month, e.day)</div></pre></td></tr></table></figure>
<p>输出结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Bar 1: 2 3</div><div class="line">Bar 2: hello 0</div><div class="line">Bar 2: hello 5</div><div class="line">No matching method for types (&lt;class &apos;int&apos;&gt;, &lt;class &apos;str&apos;&gt;)</div><div class="line">2012 12 21</div><div class="line">2020 10 27</div></pre></td></tr></table></figure></p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; b = s.bar</div><div class="line">&gt;&gt;&gt; b  # 绑定方法</div><div class="line">&lt;bound method bar of &lt;example1.Spam object at 0x10aa00d30&gt;&gt;</div><div class="line">&gt;&gt;&gt; b.__self__  # 类实例对象</div><div class="line">&lt;example1.Spam object at 0x10aa00d30&gt;</div><div class="line">&gt;&gt;&gt; b.__func__  # 实际的函数与描述符绑定</div><div class="line">&lt;example1.MultiMethod object at 0x10aaa8850&gt;</div></pre></td></tr></table></figure>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>正如作者所说，这种实现方法还是存在很多问题的，比如不支持关键字参数，对于继承也支持有限。因此在 Python 中还是使用更简单的方法，比如取不同的名字来实现比较好，不然也违背了 Python 设计的初衷。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="描述符"><a href="#描述符" class="headerlink" title="描述符"></a>描述符</h3><p>描述符是 Python 中的一类对象，它重写了 <code>__get__</code>, <code>__set__</code>, <code>__delete__</code> 中的一个或者多个。一般用于自定义的数据类型，可以在获取或者设置属性的时候加一些特殊的操作，比如类型检查、输出 log 等。在这里主要是通过 <code>__call__</code> 来从输入参数映射到不同的方法。通常情况下描述符和装饰器是可以相互转换的，在这一节中也给出了示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">multimethod</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></div><div class="line">        self._methods = &#123;&#125;</div><div class="line">        self.__name__ = func.__name__</div><div class="line">        self._default = func</div><div class="line"></div><div class="line">    <span class="comment"># 装饰器函数，这里传入函数不需要知道参数</span></div><div class="line">    <span class="comment"># 但是装饰器需要传入类别作为参数，因此只有两层</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">match</span><span class="params">(self, *types)</span>:</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">register</span><span class="params">(func)</span>:</span></div><div class="line">            ndefaults = len(func.__defaults__) <span class="keyword">if</span> func.__defaults__ <span class="keyword">else</span> <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(ndefaults+<span class="number">1</span>):</div><div class="line">                <span class="comment"># 目的和上面一样，也是为了处理默认参数</span></div><div class="line">                self._methods[types[:len(types) - n]] = func</div><div class="line">            <span class="keyword">return</span> self</div><div class="line">        <span class="keyword">return</span> register</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args)</span>:</span></div><div class="line">        <span class="comment"># 还是要忽视掉 self 参数</span></div><div class="line">        types = tuple(type(arg) <span class="keyword">for</span> arg <span class="keyword">in</span> args[<span class="number">1</span>:])</div><div class="line">        meth = self._methods.get(types, <span class="keyword">None</span>)</div><div class="line">        <span class="keyword">if</span> meth:</div><div class="line">            <span class="keyword">return</span> meth(*args)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self._default(*args)</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get__</span><span class="params">(self, instance, cls)</span>:</span></div><div class="line">        <span class="keyword">if</span> instance <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> types.MethodType(self, instance)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self</div><div class="line"></div><div class="line"><span class="comment"># Example use</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spam</span>:</span></div><div class="line">    <span class="comment"># 相当于先初始化实例</span></div><div class="line">    <span class="comment"># 这里定义好像不能指定函数参数类别</span></div><div class="line">    <span class="comment"># 因此定义一个接受任意参数的函数作为缺省值来报错</span></div><div class="line"><span class="meta">    @multimethod  </span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, *args)</span>:</span></div><div class="line">        <span class="comment"># Default method called if no match</span></div><div class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'No matching method for bar'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 然后再通过类型进行绑定</span></div><div class="line">    <span class="comment"># 这里就不用手动从参数转化到类型了</span></div><div class="line"><span class="meta">    @bar.match(int, int)  </span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        print(<span class="string">'Bar 1:'</span>, x, y)</div><div class="line"></div><div class="line"><span class="meta">    @bar.match(str, int)</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self, s, n = <span class="number">0</span>)</span>:</span></div><div class="line">        print(<span class="string">'Bar 2:'</span>, s, n)</div></pre></td></tr></table></figure>
<h2 id="元类"><a href="#元类" class="headerlink" title="元类"></a>元类</h2><p>元类是用来创建类的，可以在其中定义创建类时候的各种操作，比如这里就修改了绑定方法的地方，不直接将方法绑定到类中，而是将方法绑定到描述符中，然后通过描述符再绑定到类中。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;通过函数注解来实现方法重载&quot;&gt;&lt;a href=&quot;#通过函数注解来实现方法重载&quot; class=&quot;headerlink&quot; title=&quot;通过函数注解来实现方法重载&quot;&gt;&lt;/a&gt;通过函数注解来实现方法重载&lt;/h2&gt;&lt;p&gt;最近在看 《Python Cookbook》，9.2
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用mmsegmentation训练自己的数据集</title>
    <link href="https://blog.patrickcty.cc/2020/10/14/%E4%BD%BF%E7%94%A8mmsegmentation%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://blog.patrickcty.cc/2020/10/14/使用mmsegmentation训练自己的数据集/</id>
    <published>2020-10-14T12:01:44.000Z</published>
    <updated>2020-10-14T12:02:35.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><ul>
<li>安装</li>
<li>注册数据集</li>
<li>编写配置文件</li>
<li>运行</li>
<li>测试</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">pip install mmcv</div><div class="line"># 注意只是 clone 是不行的，还要 install 一下产生版本文件</div><div class="line">pip install git+https://github.com/open-mmlab/mmsegmentation.git # install the master branch</div></pre></td></tr></table></figure>
<p>更多安装方法参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/install.md" target="_blank" rel="external">官方文档</a></p>
<h2 id="注册数据集"><a href="#注册数据集" class="headerlink" title="注册数据集"></a>注册数据集</h2><ul>
<li>在 <code>mmseg/datasets</code> 目录下添加自己的数据集的 .py 文件，这里主要是让框架知道模型的类别，下面 suffix 根据自己实际情况修改</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</div><div class="line"></div><div class="line"><span class="keyword">from</span> .builder <span class="keyword">import</span> DATASETS</div><div class="line"><span class="keyword">from</span> .custom <span class="keyword">import</span> CustomDataset</div><div class="line"></div><div class="line"></div><div class="line"><span class="meta">@DATASETS.register_module()</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SatelliteDataset</span><span class="params">(CustomDataset)</span>:</span></div><div class="line">    <span class="string">"""Satellite dataset.</span></div><div class="line"></div><div class="line">    The ``img_suffix`` is fixed to '.tif' and ``seg_map_suffix`` is</div><div class="line">    fixed to '.png'.</div><div class="line">    """</div><div class="line"></div><div class="line">    CLASSES = (<span class="string">'ford'</span>, <span class="string">'transportation'</span>, <span class="string">'building'</span>, <span class="string">'farmland'</span>, <span class="string">'grassland'</span>,</div><div class="line">               <span class="string">'woodland'</span>, <span class="string">'bare_soil'</span>,  <span class="string">'others'</span>)</div><div class="line"></div><div class="line">    PALETTE = [[<span class="number">120</span>, <span class="number">120</span>, <span class="number">120</span>], [<span class="number">180</span>, <span class="number">120</span>, <span class="number">120</span>], [<span class="number">6</span>, <span class="number">230</span>, <span class="number">230</span>], [<span class="number">80</span>, <span class="number">50</span>, <span class="number">50</span>],</div><div class="line">               [<span class="number">4</span>, <span class="number">200</span>, <span class="number">3</span>], [<span class="number">120</span>, <span class="number">120</span>, <span class="number">80</span>], [<span class="number">140</span>, <span class="number">140</span>, <span class="number">140</span>], [<span class="number">204</span>, <span class="number">5</span>, <span class="number">255</span>]]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></div><div class="line">        super(SatelliteDataset, self).__init__(</div><div class="line">            img_suffix=<span class="string">'.tif'</span>,</div><div class="line">            seg_map_suffix=<span class="string">'.png'</span>,</div><div class="line">            reduce_zero_label=<span class="keyword">False</span>,</div><div class="line">            **kwargs)</div><div class="line">        <span class="keyword">assert</span> osp.exists(self.img_dir)</div></pre></td></tr></table></figure>
<ul>
<li>在 <code>mmseg/datasets/__init__.py</code> 中导入你自定义的类，并在 <code>__all__</code> 变量中添加你的类名</li>
</ul>
<h2 id="编写配置文件"><a href="#编写配置文件" class="headerlink" title="编写配置文件"></a>编写配置文件</h2><p>在 <code>configs/你要用的方法/</code> 下创建一个 .py 文件，配置文件主要由四部分组成：</p>
<ul>
<li>使用模型</li>
<li>数据集及数据处理流程</li>
<li>模型调度方法</li>
<li>runtime 配置</li>
</ul>
<p>可以引入已有的配置，如果要修改配置就新建一个 dict 覆盖掉原来的配置项（不需要全部字段都有），如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">model = dict(</div><div class="line">    # 修改类别</div><div class="line">    decode_head=dict(num_classes=8, norm_cfg=norm_cfg),</div><div class="line">    auxiliary_head=dict(num_classes=8, norm_cfg=norm_cfg),</div><div class="line">    # 修改预训练路径</div><div class="line">    pretrained=&apos;open-mmlab://resnet101_v1c&apos;,</div><div class="line">    # 修改训练 backbone</div><div class="line">    backbone=dict(depth=101, norm_cfg=norm_cfg)</div><div class="line">)</div></pre></td></tr></table></figure>
<p>放一个完整的配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">_base_ = [</div><div class="line">    &apos;../_base_/models/pspnet_r50-d8.py&apos;,</div><div class="line">    &apos;../_base_/default_runtime.py&apos;,</div><div class="line">    &apos;../_base_/schedules/schedule_40k.py&apos;</div><div class="line">]</div><div class="line"># norm_cfg = dict(type=&apos;BN&apos;, requires_grad=True)</div><div class="line">norm_cfg = dict(type=&apos;SyncBN&apos;, requires_grad=True)</div><div class="line">model = dict(</div><div class="line">    decode_head=dict(num_classes=8, norm_cfg=norm_cfg),</div><div class="line">    auxiliary_head=dict(num_classes=8, norm_cfg=norm_cfg),</div><div class="line">    pretrained=&apos;open-mmlab://resnet101_v1c&apos;,</div><div class="line">    backbone=dict(depth=101, norm_cfg=norm_cfg)</div><div class="line">)</div><div class="line"></div><div class="line">dataset_type = &apos;SatelliteDataset&apos;</div><div class="line">data_root = &apos;/home/sse/data4T/common_datasets/satelite_dataset/&apos;</div><div class="line">img_norm_cfg = dict(</div><div class="line">    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)</div><div class="line">img_scale = (256, 256)</div><div class="line"># crop_size = (224, 224)</div><div class="line"></div><div class="line">train_pipeline = [</div><div class="line">    dict(type=&apos;LoadImageFromFile&apos;),</div><div class="line">    dict(type=&apos;LoadAnnotations&apos;),</div><div class="line">    dict(type=&apos;RandomFlip&apos;, flip_ratio=0.5),</div><div class="line">    dict(type=&apos;PhotoMetricDistortion&apos;),</div><div class="line">    dict(type=&apos;Normalize&apos;, **img_norm_cfg),</div><div class="line">    dict(type=&apos;DefaultFormatBundle&apos;),</div><div class="line">    dict(type=&apos;Collect&apos;, keys=[&apos;img&apos;, &apos;gt_semantic_seg&apos;]),</div><div class="line">]</div><div class="line">test_pipeline = [</div><div class="line">    dict(type=&apos;LoadImageFromFile&apos;),</div><div class="line">    dict(</div><div class="line">        type=&apos;MultiScaleFlipAug&apos;,</div><div class="line">        img_scale=img_scale,</div><div class="line">        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],</div><div class="line">        flip=False,</div><div class="line">        transforms=[</div><div class="line">            dict(type=&apos;Normalize&apos;, **img_norm_cfg),</div><div class="line">            dict(type=&apos;ImageToTensor&apos;, keys=[&apos;img&apos;]),</div><div class="line">            dict(type=&apos;Collect&apos;, keys=[&apos;img&apos;]),</div><div class="line">        ])</div><div class="line">]</div><div class="line">data = dict(</div><div class="line">    samples_per_gpu=12,</div><div class="line">    workers_per_gpu=0,</div><div class="line">    train=dict(</div><div class="line">        type=dataset_type,</div><div class="line">        data_root=data_root,</div><div class="line">        img_dir=&apos;img_dir/train&apos;,</div><div class="line">        ann_dir=&apos;ann_dir/train&apos;,</div><div class="line">        # split=&apos;ImageSets/SegmentationContext/train.txt&apos;,</div><div class="line">        pipeline=train_pipeline),</div><div class="line">    val=dict(  # 训练到一定轮次会自动验证</div><div class="line">        type=dataset_type,</div><div class="line">        data_root=data_root,</div><div class="line">        img_dir=&apos;img_dir/test&apos;,</div><div class="line">        ann_dir=&apos;ann_dir/test&apos;,</div><div class="line">        # split=&apos;ImageSets/SegmentationContext/val.txt&apos;,</div><div class="line">        pipeline=test_pipeline),</div><div class="line">    test=dict(  # 测试的时候才用到</div><div class="line">        type=dataset_type,</div><div class="line">        data_root=data_root,</div><div class="line">        img_dir=&apos;image_A/image_A_9&apos;,</div><div class="line">        # ann_dir=&apos;ann_dir/test&apos;,</div><div class="line">        # split=&apos;ImageSets/SegmentationContext/val.txt&apos;,</div><div class="line">        pipeline=test_pipeline))</div><div class="line"></div><div class="line">total_iters = 100000</div><div class="line">checkpoint_config = dict(by_epoch=False, interval=4000)</div><div class="line">evaluation = dict(interval=4000, metric=&apos;mIoU&apos;)</div><div class="line"></div><div class="line"># 训练结果保存路径</div><div class="line">work_dir = &apos;/home/sse/mmsegmentation/run/satellite-10-12&apos;</div></pre></td></tr></table></figure>
<p>详细配置还是参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/config.md" target="_blank" rel="external">官方文档</a></p>
<h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>单卡运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python tools/train.py 配置文件路径名</div></pre></td></tr></table></figure>
<p>分布式运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./tools/dist_train.sh 配置文件路径名 GPU数 [optional arguments]</div></pre></td></tr></table></figure>
<p>从已有参数从头开始运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./tools/dist_train.sh 配置文件路径名 GPU数 --load-from 参数路径</div></pre></td></tr></table></figure>
<p>从已有参数从头继续运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./tools/dist_train.sh 配置文件路径名 GPU数 --resume-from 参数路径</div></pre></td></tr></table></figure>
<p>更详细的依然参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/getting_started.md" target="_blank" rel="external">官方文档</a></p>
<h3 id="运行时候的-一个坑"><a href="#运行时候的-一个坑" class="headerlink" title="运行时候的==一个坑=="></a>运行时候的==一个坑==</h3><p>验证数据集在验证时会把所有的预测结果和 GT 保存在内存中，如果验证集太大很可能进程会挂掉，测试同</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的时候默认使用配置文件中指定的测试集</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># single-gpu testing</div><div class="line">python tools/test.py $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; [--out $&#123;RESULT_FILE&#125;] [--eval $&#123;EVAL_METRICS&#125;] [--show]</div><div class="line"></div><div class="line"># multi-gpu testing</div><div class="line">./tools/dist_test.sh $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; $&#123;GPU_NUM&#125; [--out $&#123;RESULT_FILE&#125;] [--eval $&#123;EVAL_METRICS&#125;]</div></pre></td></tr></table></figure>
<p>可选参数</p>
<ul>
<li>RESULT_FILE: pickle 文件，结果保存在其中</li>
<li>EVAL_METRICS: 评价指标，制定之后需要标签文件</li>
<li>–show: 结果会在新窗口中打开</li>
<li>–show-dir: 可视化结果保存到指定文件夹中，注意保存的不是神经网络出来的结果，而是经过 RGB 调色之后和原图叠加的结果</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;总体流程&quot;&gt;&lt;a href=&quot;#总体流程&quot; class=&quot;headerlink&quot; title=&quot;总体流程&quot;&gt;&lt;/a&gt;总体流程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;安装&lt;/li&gt;
&lt;li&gt;注册数据集&lt;/li&gt;
&lt;li&gt;编写配置文件&lt;/li&gt;
&lt;li&gt;运行&lt;/li&gt;
&lt;li&gt;测
    
    </summary>
    
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割" scheme="https://blog.patrickcty.cc/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>【一个坑】exFAT 不支持软链接与硬链接</title>
    <link href="https://blog.patrickcty.cc/2020/10/10/exFAT%E4%B8%8D%E6%94%AF%E6%8C%81%E8%BD%AF%E9%93%BE%E6%8E%A5%E4%B8%8E%E7%A1%AC%E9%93%BE%E6%8E%A5/"/>
    <id>https://blog.patrickcty.cc/2020/10/10/exFAT不支持软链接与硬链接/</id>
    <published>2020-10-10T14:47:13.000Z</published>
    <updated>2020-10-10T14:53:48.276Z</updated>
    
    <content type="html"><![CDATA[<p>今天在跑 mmseg 的代码的时候报了一个 OSError，function not implemented。在终端使用 <code>ln</code> 命令也报了同样的问题，后来经过<a href="https://superuser.com/questions/1256530/linux-links-shortcuts-in-exfat-filesystem/1256536" target="_blank" rel="external">查找</a>之后发现 exFAT 文件系统不支持软链接与硬链接。</p>
<p>P.S. 创建软链接方法：<code>ln -s src_path dst_symlink_path</code></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在跑 mmseg 的代码的时候报了一个 OSError，function not implemented。在终端使用 &lt;code&gt;ln&lt;/code&gt; 命令也报了同样的问题，后来经过&lt;a href=&quot;https://superuser.com/questions/1256
    
    </summary>
    
    
      <category term="踩坑" scheme="https://blog.patrickcty.cc/tags/%E8%B8%A9%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>keras自定义训练流程</title>
    <link href="https://blog.patrickcty.cc/2020/10/07/keras%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/"/>
    <id>https://blog.patrickcty.cc/2020/10/07/keras自定义训练流程/</id>
    <published>2020-10-07T12:48:50.000Z</published>
    <updated>2020-10-07T12:51:09.771Z</updated>
    
    <content type="html"><![CDATA[<h2 id="标准流程"><a href="#标准流程" class="headerlink" title="标准流程"></a>标准流程</h2><p>keras 的 api 集成度都非常高，在你没有额外需求的时候的时候能非常轻松地完成整个训练流程：</p>
<ul>
<li>加载数据<ul>
<li>可以选择 generator </li>
<li>也可以直接传入内存的数据</li>
<li>还可以按照一定格式组织成文件夹然后直接传文件夹名</li>
</ul>
</li>
<li>构造模型</li>
<li>编译模型<ul>
<li>指定优化器</li>
<li>指定损失函数</li>
<li>指定评价标准</li>
</ul>
</li>
<li>训练模型<ul>
<li>指定训练轮次（epoch）</li>
<li>指定回调</li>
</ul>
</li>
</ul>
<h2 id="自定义流程"><a href="#自定义流程" class="headerlink" title="自定义流程"></a>自定义流程</h2><p>标准流程在大多数情况下都能满足需求，但是对于一些需要获取网络中细节的情况下就需要自定义流程了。自定义主要也是对训练步骤进行处理，基本步骤如下：</p>
<ul>
<li>定义一个 step 的操作：<ul>
<li>取出这个 batch 的数据</li>
<li>传入网络得到输出</li>
<li>计算 loss</li>
<li>计算梯度</li>
<li>梯度下降</li>
</ul>
</li>
</ul>
<p>写成代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</div><div class="line">train_loss = tf.keras.metrics.Mean(<span class="string">'train_loss'</span>, dtype=tf.float32)</div><div class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(<span class="string">'train_accuracy'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> idx, (x_train, y_train) <span class="keyword">in</span> enumerate(train_gen):</div><div class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</div><div class="line">        predictions = model(x_train, training=<span class="keyword">True</span>)  <span class="comment"># 传入网络得到输出</span></div><div class="line">        loss = loss_object(y_train, predictions)  <span class="comment"># 计算 loss</span></div><div class="line">    grads = tape.gradient(loss, model.trainable_variables)  <span class="comment"># 计算梯度</span></div><div class="line">    optimizer.apply_gradients(zip(grads, model.trainable_variables))  <span class="comment"># 梯度下降</span></div><div class="line"></div><div class="line">    <span class="comment"># 一些指标</span></div><div class="line">    train_loss(loss)</div><div class="line">    train_accuracy(y_train, predictions)</div></pre></td></tr></table></figure>
<h2 id="和-TensorBoard-一起作用"><a href="#和-TensorBoard-一起作用" class="headerlink" title="和 TensorBoard 一起作用"></a>和 TensorBoard 一起作用</h2><p>尽管 keras 的 callback 里面也有 tensorboard，但是默认情况下它只能每个 ep 来保存评价指标和直方图，不能看一个 step 中的变化情况，也不能将参数或者梯度来画成图表。在这里我们将参数和梯度的 l2 范数变化情况画成图表，并且原本就有的直方图也不落下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># create a tensorboard file writer</span></div><div class="line">summary_writer = tf.summary.create_file_writer(some_path)</div><div class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</div><div class="line">train_loss = tf.keras.metrics.Mean(<span class="string">'train_loss'</span>, dtype=tf.float32)</div><div class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(<span class="string">'train_accuracy'</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</div><div class="line">    <span class="keyword">for</span> idx, (x_train, y_train) <span class="keyword">in</span> enumerate(train_gen):</div><div class="line">        n_iter = (epoch - <span class="number">1</span>) * len(train_gen) + idx + <span class="number">1</span></div><div class="line">        </div><div class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</div><div class="line">            predictions = model(x_train, training=<span class="keyword">True</span>)  <span class="comment"># 传入网络得到输出</span></div><div class="line">            loss = loss_object(y_train, predictions)  <span class="comment"># 计算 loss</span></div><div class="line">        gradients = tape.gradient(loss, model.trainable_variables)  <span class="comment"># 计算梯度</span></div><div class="line">        optimizer.apply_gradients(zip(grads, model.trainable_variables))  <span class="comment"># 梯度下降</span></div><div class="line">        trainable_vars = model.trainable_variables</div><div class="line">        </div><div class="line">        <span class="comment"># 一些指标，都是标量</span></div><div class="line">        train_loss(loss)</div><div class="line">        train_accuracy(y_train, predictions)</div><div class="line">        </div><div class="line">        <span class="keyword">with</span> summary_writer.as_default():</div><div class="line">            <span class="comment"># 写入评价指标</span></div><div class="line">            tf.summary.scalar(<span class="string">'loss'</span>, train_loss.result(), step=n_iter)</div><div class="line">            tf.summary.scalar(<span class="string">'accuracy'</span>, train_accuracy.result(), step=n_iter)</div><div class="line">            </div><div class="line">            <span class="keyword">for</span> var, grad <span class="keyword">in</span> zip(trainable_vars, gradients)</div><div class="line">                <span class="comment"># 写入各个可训练元素的直方图、梯度和参数</span></div><div class="line">                tf.summary.histograme(var.name, var, n_iter)</div><div class="line">                tf.summary.scalar(<span class="string">'Grads:'</span> + var.name, tf.norm(grad), n_iter)</div><div class="line">                tf.summart.scalar(<span class="string">'Weights'</span> + var.name, ty.norm(var), n_iter)</div><div class="line"></div><div class="line">    print(<span class="string">'Epoch &#123;:03d&#125; finished.'</span>.format(epoch))</div></pre></td></tr></table></figure>
<p>TensorBoard 里面的数据不能导出来，也可以单独将其写入 csv 来方便后续的处理。</p>
<h2 id="参考教程"><a href="#参考教程" class="headerlink" title="参考教程"></a>参考教程</h2><ul>
<li><a href="https://keras.io/guides/customizing_what_happens_in_fit/" target="_blank" rel="external">https://keras.io/guides/customizing_what_happens_in_fit/</a></li>
<li><a href="https://www.tensorflow.org/tensorboard/get_started" target="_blank" rel="external">https://www.tensorflow.org/tensorboard/get_started</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;标准流程&quot;&gt;&lt;a href=&quot;#标准流程&quot; class=&quot;headerlink&quot; title=&quot;标准流程&quot;&gt;&lt;/a&gt;标准流程&lt;/h2&gt;&lt;p&gt;keras 的 api 集成度都非常高，在你没有额外需求的时候的时候能非常轻松地完成整个训练流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li
    
    </summary>
    
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Keras" scheme="https://blog.patrickcty.cc/tags/Keras/"/>
    
      <category term="TensorFlow" scheme="https://blog.patrickcty.cc/tags/TensorFlow/"/>
    
      <category term="TensorBoard" scheme="https://blog.patrickcty.cc/tags/TensorBoard/"/>
    
  </entry>
  
  <entry>
    <title>keras中BatchNormalization在迁移学习中的坑</title>
    <link href="https://blog.patrickcty.cc/2020/05/13/Keras%E4%B8%ADBatchNormalization%E5%9C%A8%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/05/13/Keras中BatchNormalization在迁移学习中的坑/</id>
    <published>2020-05-13T02:38:13.000Z</published>
    <updated>2020-10-08T08:03:09.029Z</updated>
    
    <content type="html"><![CDATA[<h2 id="注"><a href="#注" class="headerlink" title="注"></a>注</h2><p>这个问题已经在 TF 2.0 中修复了，见<a href="https://tensorflow.google.cn/api_docs/python/tf/keras/layers/BatchNormalization?hl=zh-cn" target="_blank" rel="external">文档</a>。</p>
<blockquote>
<p>However, in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p>
<p>This behavior has been introduced in TensorFlow 2.0, in order to enable layer.trainable = False to produce the most commonly expected behavior in the convnet fine-tuning use case.</p>
</blockquote>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近 Keras 的文档更新了，我发现了一个迁移学习的文档，结果进去之后发现一个奇怪的地方：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">base_model = keras.applications.Xception(</div><div class="line">    weights=&quot;imagenet&quot;,  # Load weights pre-trained on ImageNet.</div><div class="line">    input_shape=(150, 150, 3),</div><div class="line">    include_top=False,</div><div class="line">)  # Do not include the ImageNet classifier at the top.</div><div class="line"></div><div class="line"># Freeze the base_model</div><div class="line">base_model.trainable = False</div><div class="line"></div><div class="line"># Create new model on top</div><div class="line">inputs = keras.Input(shape=(150, 150, 3))</div><div class="line">x = data_augmentation(inputs)  # Apply random data augmentation</div><div class="line">x = keras.layers.experimental.preprocessing.Rescaling(1.0 / 255.0)(</div><div class="line">    x</div><div class="line">)  # Scale inputs to [0. 1]</div><div class="line"># The base model contains batchnorm layers. We want to keep them in inference mode</div><div class="line"># when we unfreeze the base model for fine-tuning, so we make sure that the</div><div class="line"># base_model is running in inference mode here.</div><div class="line">x = base_model(x, training=False)</div><div class="line">x = keras.layers.GlobalAveragePooling2D()(x)</div><div class="line">x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout</div><div class="line">outputs = keras.layers.Dense(1)(x)</div><div class="line">model = keras.Model(inputs, outputs)</div><div class="line"></div><div class="line">model.summary()</div></pre></td></tr></table></figure>
<p>这里是说迁移学习的时候，使用其他的模型，然后冻结之，再增加新的层进行训练。其中 <code>x = base_model(x, training=False)</code> 以及上面的注释引起了我的注意。在这里设置 <code>training=False</code> 是为了让 backbone 处于 inference 状态，这个状态主要是对 BN 起作用，那就是不更新 BN 的参数，即使 unfreeze 之后也不更新。</p>
<p>这个 inference 状态和 training 状态有什么用呢？我们知道，有一些层在训练和测试的时候表现是不同的，比如 BN 和 Dropout。其中 BN 在训练的时候使用 mini-batch 的数据来进行归一化，同时更新 moving mean 和 moving variance，在测试的时候就使用上面的 moving mean 和 moving variance 来进行归一化。这里的 training 是用来控制这些层的表现。</p>
<p>这个 training 出现在 keras Layer 和 Model 的 call 方法中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def call(self, inputs, training=False):</div><div class="line">    pass</div></pre></td></tr></table></figure>
<p>当调用层的时候可以指定，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = BatchNormalization()(x, training=False)</div><div class="line">model = Xception(input_shape=(150, 150, 3))(x, training=True)</div></pre></td></tr></table></figure>
<p>本着想更深入地理解这个参数，就查了一下，结果发现了一个 <a href="https://github.com/keras-team/keras/issues/7177" target="_blank" rel="external">issue</a> 和一个 <a href="https://github.com/keras-team/keras/pull/9965" target="_blank" rel="external">PR</a>，这里面就描述了 keras BatchNormalization 在迁移学习中的坑。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>大佬的<a href="http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/" target="_blank" rel="external">博客</a>清楚地解释了问题，我这里再重新复述一下。</p>
<p>上面提到过了，BN 层在训练状态和测试状态下的表现是不同的，一个是使用 mini-batch 的数据，另一个是使用积累下来的 moving mean 和 moving variance。而在迁移学习中，我们通常会把 backbone 直接 freeze，训练新加的层，再 unfreeze backbone，然后一起训练。</p>
<p>不过如果不按照上面那样设置 <code>x = base_model(x, training=False)</code>，而是直接像下面这样使用已有的模型然后进行训练（实际上大多数数据增强都不会整合到 model 中，因为只有训练的时候才需要，也就是说，基本不会出现上面这种调用形式），那么就会出现问题。（以下代码来自提到的博客）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from tensorflow.keras.datasets import cifar10</div><div class="line"> </div><div class="line">from tensorflow.keras.preprocessing.image import ImageDataGenerator</div><div class="line">from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input</div><div class="line">from tensorflow.keras.models import Model, load_model</div><div class="line">from tensorflow.keras.layers import Dense, Flatten</div><div class="line">from tensorflow.keras import backend as K</div><div class="line"></div><div class="line">seed = 42</div><div class="line">epochs = 10</div><div class="line">records_per_class = 100</div><div class="line"></div><div class="line"># We take only 2 classes from CIFAR10 and a very small sample to intentionally overfit the model.</div><div class="line"># We will also use the same data for train/test and expect that Keras will give the same accuracy.</div><div class="line">(x, y), _ = cifar10.load_data()</div><div class="line"> </div><div class="line">def filter_resize(category):</div><div class="line">   # We do the preprocessing here instead in the Generator to get around a bug on Keras 2.1.5.</div><div class="line">   return [preprocess_input(img) for img in x[y.flatten()==category][:records_per_class]]</div><div class="line"> </div><div class="line">x = np.stack(filter_resize(3)+filter_resize(5))</div><div class="line">records_per_class = x.shape[0] // 2</div><div class="line">y = np.array([[1,0]]*records_per_class + [[0,1]]*records_per_class)</div><div class="line"> </div><div class="line"> </div><div class="line"># We will use a pre-trained model and finetune the top layers.</div><div class="line">np.random.seed(seed)</div><div class="line">base_model = ResNet50(weights=&apos;imagenet&apos;, include_top=False, input_shape=(32, 32, 3))</div><div class="line">l = Flatten()(base_model.output)</div><div class="line">predictions = Dense(2, activation=&apos;softmax&apos;)(l)</div><div class="line">model = Model(inputs=base_model.input, outputs=predictions)</div><div class="line"> </div><div class="line"># for layer in model.layers[:140]:</div><div class="line">#    layer.trainable = False</div><div class="line"> </div><div class="line"># for layer in model.layers[140:]:</div><div class="line">#    layer.trainable = True</div><div class="line">base_model.trainable = False</div><div class="line"> </div><div class="line">model.compile(optimizer=&apos;sgd&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;])</div><div class="line">model.fit_generator(ImageDataGenerator().flow(x, y, seed=42), </div><div class="line">                    steps_per_epoch=7,</div><div class="line">                    epochs=epochs, </div><div class="line">                    validation_data=ImageDataGenerator().flow(x, y, seed=42),</div><div class="line">                    validation_steps=7</div><div class="line">                    )</div><div class="line"> </div><div class="line"># Store the model on disk</div><div class="line">model.save(&apos;tmp.h5&apos;)</div><div class="line"> </div><div class="line"> </div><div class="line"># In every test we will clear the session and reload the model to force Learning_Phase values to change.</div><div class="line">print(&apos;DYNAMIC LEARNING_PHASE&apos;)</div><div class="line">K.clear_session()</div><div class="line">model = load_model(&apos;tmp.h5&apos;)</div><div class="line"># This accuracy should match exactly the one of the validation set on the last iteration.</div><div class="line">print(model.evaluate(ImageDataGenerator().flow(x, y, seed=42), steps=7))</div><div class="line"> </div><div class="line"> </div><div class="line">print(&apos;STATIC LEARNING_PHASE = 0&apos;)</div><div class="line">K.clear_session()</div><div class="line">K.set_learning_phase(0)</div><div class="line">model = load_model(&apos;tmp.h5&apos;)</div><div class="line"># Again the accuracy should match the above.</div><div class="line">print(model.evaluate(ImageDataGenerator().flow(x, y, seed=42), steps=7))</div><div class="line"> </div><div class="line"> </div><div class="line">print(&apos;STATIC LEARNING_PHASE = 1&apos;)</div><div class="line">K.clear_session()</div><div class="line">K.set_learning_phase(1)</div><div class="line">model = load_model(&apos;tmp.h5&apos;)</div><div class="line"># The accuracy will be close to the one of the training set on the last iteration.</div><div class="line">print(model.evaluate(ImageDataGenerator().flow(x, y, seed=42), steps=7))</div></pre></td></tr></table></figure>
<p>运行上面的代码，其中训练集和验证集是同一个数据集。我们可以看到这两者的结果截然不同，训练的结果远好于验证的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">Epoch 1/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 1.1314 - acc: 0.5298Epoch 1/10</div><div class="line">7/7 [==============================] - 3s 394ms/step - loss: 2.0678 - acc: 0.5700</div><div class="line">7/7 [==============================] - 5s 760ms/step - loss: 1.2129 - acc: 0.5300 - val_loss: 2.0678 - val_acc: 0.5700</div><div class="line">Epoch 2/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.9528 - acc: 0.6012Epoch 1/10</div><div class="line">7/7 [==============================] - 2s 265ms/step - loss: 1.4357 - acc: 0.5600</div><div class="line">7/7 [==============================] - 4s 558ms/step - loss: 0.8973 - acc: 0.6150 - val_loss: 1.4357 - val_acc: 0.5600</div><div class="line">Epoch 3/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.7655 - acc: 0.6667Epoch 1/10</div><div class="line">7/7 [==============================] - 2s 215ms/step - loss: 1.4113 - acc: 0.5950</div><div class="line">7/7 [==============================] - 4s 535ms/step - loss: 0.8119 - acc: 0.6550 - val_loss: 1.4113 - val_acc: 0.5950</div><div class="line">Epoch 4/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.7548 - acc: 0.7440Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 151ms/step - loss: 1.9380 - acc: 0.5800</div><div class="line">7/7 [==============================] - 2s 331ms/step - loss: 0.7230 - acc: 0.7350 - val_loss: 1.9380 - val_acc: 0.5800</div><div class="line">Epoch 5/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.5866 - acc: 0.7202Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 150ms/step - loss: 1.8147 - acc: 0.6000</div><div class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.5802 - acc: 0.7150 - val_loss: 1.8147 - val_acc: 0.6000</div><div class="line">Epoch 6/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.3704 - acc: 0.8095Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 151ms/step - loss: 1.5603 - acc: 0.6450</div><div class="line">7/7 [==============================] - 2s 321ms/step - loss: 0.3881 - acc: 0.7950 - val_loss: 1.5603 - val_acc: 0.6450</div><div class="line">Epoch 7/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.5056 - acc: 0.7738Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 151ms/step - loss: 1.9539 - acc: 0.6250</div><div class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.5618 - acc: 0.7400 - val_loss: 1.9539 - val_acc: 0.6250</div><div class="line">Epoch 8/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.5849 - acc: 0.7976Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 153ms/step - loss: 1.4035 - acc: 0.6600</div><div class="line">7/7 [==============================] - 2s 323ms/step - loss: 0.5465 - acc: 0.8050 - val_loss: 1.4035 - val_acc: 0.6600</div><div class="line">Epoch 9/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.4055 - acc: 0.8512Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 147ms/step - loss: 1.0538 - acc: 0.6650</div><div class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.3984 - acc: 0.8450 - val_loss: 1.0538 - val_acc: 0.6650</div><div class="line">Epoch 10/10</div><div class="line">6/7 [========================&gt;.....] - ETA: 0s - loss: 0.4082 - acc: 0.8452Epoch 1/10</div><div class="line">7/7 [==============================] - 1s 152ms/step - loss: 1.8019 - acc: 0.6000</div><div class="line">7/7 [==============================] - 2s 322ms/step - loss: 0.4177 - acc: 0.8400 - val_loss: 1.8019 - val_acc: 0.6000</div></pre></td></tr></table></figure>
<p>再看看最后输出的结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">DYNAMIC LEARNING_PHASE</div><div class="line">7/7 [==============================] - 2s 256ms/step - loss: 2.0028 - acc: 0.6000</div><div class="line">[2.002779943602426, 0.6]</div><div class="line">STATIC LEARNING_PHASE = 0</div><div class="line">7/7 [==============================] - 1s 204ms/step - loss: 2.0028 - acc: 0.6000</div><div class="line">[2.002779943602426, 0.6]</div><div class="line">STATIC LEARNING_PHASE = 1</div><div class="line">7/7 [==============================] - 1s 212ms/step - loss: 0.3017 - acc: 0.8650</div><div class="line">[0.30170093051024843, 0.865]</div></pre></td></tr></table></figure>
<p>第一个结果是 keras 直接自动设置运行状态，第二个结果是手动设定运行状态为测试状态，第三个结果是手动设定运行结果为训练状态。可以看出来，keras 在测试的时候自动设置为测试状态，但这个时候结果出现了明显的下滑，而设置为训练状态的时候结果很正常。</p>
<p>其原因在于，在训练的时候，虽然 freeze 了 BN 的参数，但是 keras 仍然认为 BN 是在训练状态，因此会使用 mini-batch 的数据来标准化。也就是说，这时候后层网络学习到的是 mini-batch（训练数据集） 的分布。但是当测试的时候，BN 使用 moving mean 和 moving variance 来标准化，这两个参数由于没更新，是来自于原来数据集的。因为二者分布偏差很大，因此在测试模式下得到的结果非常差。</p>
<p>这个 PR 的改进就是当 freeze BN 的时候，就让 BN 层按照测试状态来进行，而不使用 mini-batch 的数据。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>看了半天，keras 官方好像没有改这个 bug，但是 TF 2.0 版本已经修改了这个 bug 了，以下是在 TF 2.0 下运行同样代码的结果，可以看到训练和验证的结果是相差不大的。另外值得一提的是，改进过后收敛速度明显快了很多，loss 从 0.3 直接降到了 0.01。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">Epoch 1/10</div><div class="line">7/7 [==============================] - 2s 332ms/step - loss: 7.3916 - accuracy: 0.4700 - val_loss: 3.1501 - val_accuracy: 0.6500</div><div class="line">Epoch 2/10</div><div class="line">7/7 [==============================] - 1s 207ms/step - loss: 2.8816 - accuracy: 0.6700 - val_loss: 8.4492 - val_accuracy: 0.5100</div><div class="line">Epoch 3/10</div><div class="line">7/7 [==============================] - 1s 206ms/step - loss: 4.1846 - accuracy: 0.6750 - val_loss: 11.3409 - val_accuracy: 0.5600</div><div class="line">Epoch 4/10</div><div class="line">7/7 [==============================] - 1s 204ms/step - loss: 3.4036 - accuracy: 0.7800 - val_loss: 0.4167 - val_accuracy: 0.8650</div><div class="line">Epoch 5/10</div><div class="line">7/7 [==============================] - 1s 210ms/step - loss: 0.8244 - accuracy: 0.8150 - val_loss: 9.1833 - val_accuracy: 0.5400</div><div class="line">Epoch 6/10</div><div class="line">7/7 [==============================] - 1s 210ms/step - loss: 2.3888 - accuracy: 0.7600 - val_loss: 0.7993 - val_accuracy: 0.8100</div><div class="line">Epoch 7/10</div><div class="line">7/7 [==============================] - 1s 207ms/step - loss: 0.5801 - accuracy: 0.8600 - val_loss: 2.9707 - val_accuracy: 0.6700</div><div class="line">Epoch 8/10</div><div class="line">7/7 [==============================] - 1s 205ms/step - loss: 4.2250 - accuracy: 0.6050 - val_loss: 1.0646 - val_accuracy: 0.8500</div><div class="line">Epoch 9/10</div><div class="line">7/7 [==============================] - 1s 206ms/step - loss: 0.4886 - accuracy: 0.8900 - val_loss: 0.0866 - val_accuracy: 0.9800</div><div class="line">Epoch 10/10</div><div class="line">7/7 [==============================] - 1s 206ms/step - loss: 0.0969 - accuracy: 0.9700 - val_loss: 0.0109 - val_accuracy: 1.0000</div><div class="line">DYNAMIC LEARNING_PHASE</div><div class="line">7/7 [==============================] - 1s 95ms/step - loss: 0.0118 - accuracy: 1.0000</div><div class="line">[0.011801988817751408, 1.0]</div><div class="line">STATIC LEARNING_PHASE = 0</div><div class="line">7/7 [==============================] - 1s 94ms/step - loss: 0.0118 - accuracy: 1.0000</div><div class="line">[0.011801988817751408, 1.0]</div><div class="line">STATIC LEARNING_PHASE = 1</div><div class="line">7/7 [==============================] - 1s 92ms/step - loss: 0.0118 - accuracy: 1.0000</div><div class="line">[0.011801988817751408, 1.0]</div></pre></td></tr></table></figure>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>这种问题真的是防不胜防，毕竟很少人会去训练集和验证集使用同一个数据集，训练集和验证集相差大大家也只会怪罪到过拟合头上去。所以平常对于一些关键的东西还是得把他摸透，并且要多看官方文档，遇到问题多思考（所以深度学习就是这一点不好，出了问题有太多可能的原因，很难定位到问题所在）。</p>
<p>P.S. 今天在训练 SOD 的时候并没有出现这个问题，其原因可能在于：</p>
<ul>
<li>我的模型在 backbone 之外增加了很多的参数，减弱了 BN 的影响，因此结果是差不多的。（回头再多做一点实验）</li>
<li>DUTS 的数据本来就来自 ImageNet Detection，因此分布非常接近。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;注&quot;&gt;&lt;a href=&quot;#注&quot; class=&quot;headerlink&quot; title=&quot;注&quot;&gt;&lt;/a&gt;注&lt;/h2&gt;&lt;p&gt;这个问题已经在 TF 2.0 中修复了，见&lt;a href=&quot;https://tensorflow.google.cn/api_docs/python
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>keras处理任意大小输入</title>
    <link href="https://blog.patrickcty.cc/2020/04/11/keras%E5%A4%84%E7%90%86%E4%BB%BB%E6%84%8F%E5%A4%A7%E5%B0%8F%E8%BE%93%E5%85%A5/"/>
    <id>https://blog.patrickcty.cc/2020/04/11/keras处理任意大小输入/</id>
    <published>2020-04-11T07:56:32.000Z</published>
    <updated>2020-04-11T09:28:25.319Z</updated>
    
    <content type="html"><![CDATA[<p>让 keras 处理任意大小输入其实很简单，只需：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># for rgb</div><div class="line">main_input = layers.Input(shape=(None, None, 3))</div><div class="line"># for gray</div><div class="line">main_input = layers.Input(shape=(None, None, 1))</div></pre></td></tr></table></figure>
<p>对于 U-Net 这种又有下采样又有上采样的就要注意一下了，下采样再上采样得到的结果可能与原来的不同，比如 25 下采样是 12，再上采样就是 24，这样往往会出现一些问题，因此最好让输入经过下采样时不会出现除不尽的情况。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;让 keras 处理任意大小输入其实很简单，只需：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;lin
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tf.keras重命名模型层名</title>
    <link href="https://blog.patrickcty.cc/2020/04/01/tfkeras%E9%87%8D%E5%91%BD%E5%90%8D%E6%A8%A1%E5%9E%8B/"/>
    <id>https://blog.patrickcty.cc/2020/04/01/tfkeras重命名模型/</id>
    <published>2020-04-01T13:08:27.000Z</published>
    <updated>2020-04-03T11:57:57.714Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前因后果"><a href="#前因后果" class="headerlink" title="前因后果"></a>前因后果</h2><p>为什么要重新命名模型的层名呢，目前做的是一个多任务的网络，两个网络用的是分开的 backbone，如果就这样并在一起作为一个模型的话就会有重复的层名，这个是不允许的，因此必须要重命名层名。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="方法一：初始化模型的时候设置不同名字"><a href="#方法一：初始化模型的时候设置不同名字" class="headerlink" title="方法一：初始化模型的时候设置不同名字"></a>方法一：初始化模型的时候设置不同名字</h3><p>其实比较好的解决方法就是在模型的层名中加一个 prefix，对于不同的任务可以定义不同的 prefix，不过这样有一个问题，那就是载入参数的时候不能使用 <code>by_name=True</code>。</p>
<h3 id="方法二：改层名"><a href="#方法二：改层名" class="headerlink" title="方法二：改层名"></a>方法二：改层名</h3><p>不过因为我是直接用的官方的库，不想自己改 backbone，所以就只能退而求其次修改层名了，修改方法为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.layers[idx]._name = &apos;aux_&apos; + l.name</div></pre></td></tr></table></figure>
<p>层的 name 属性是一个 property，不能修改，如果要修改的话使用 _name 属性，不过 _name 是一个 protected 属性，这么修改可能不是最好的方法。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前因后果&quot;&gt;&lt;a href=&quot;#前因后果&quot; class=&quot;headerlink&quot; title=&quot;前因后果&quot;&gt;&lt;/a&gt;前因后果&lt;/h2&gt;&lt;p&gt;为什么要重新命名模型的层名呢，目前做的是一个多任务的网络，两个网络用的是分开的 backbone，如果就这样并在一起作为一个
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>C语言指针的一些理解</title>
    <link href="https://blog.patrickcty.cc/2020/03/18/C%E8%AF%AD%E8%A8%80%E6%8C%87%E9%92%88%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3/"/>
    <id>https://blog.patrickcty.cc/2020/03/18/C语言指针的一些理解/</id>
    <published>2020-03-18T15:17:54.000Z</published>
    <updated>2020-03-18T15:41:05.558Z</updated>
    
    <content type="html"><![CDATA[<p>今天在 C 语言一个指针的问题上卡了很久……虽然平常不用 C 语言，但是还是记录一下，不能让时间白白的被浪费了。</p>
<h2 id="指针加减偏移量"><a href="#指针加减偏移量" class="headerlink" title="指针加减偏移量"></a>指针加减偏移量</h2><p>首先是指针加一个整数，加了之后偏移的字节数等于数据类型的字节数乘以整数的数值，例如：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> *a; <span class="comment">// 假设 int 是四个字节，a 地址为 1000</span></div><div class="line">a = a + <span class="number">3</span>  <span class="comment">// 地址为 1000 + 3 * 4 = 1012</span></div><div class="line"></div><div class="line"><span class="comment">// 假设 SOMESTRUCT 结构体是 52 字节</span></div><div class="line"><span class="comment">// b 地址为 10000</span></div><div class="line">SOMESTRUCT *b;</div><div class="line">b += <span class="number">44</span>; <span class="comment">// 地址为 10000 + 44 * 52 = 12288</span></div></pre></td></tr></table></figure>
<h2 id="非整数倍的偏移量"><a href="#非整数倍的偏移量" class="headerlink" title="非整数倍的偏移量"></a>非整数倍的偏移量</h2><p>如果不想偏移数据类型的整数倍怎么办？很简单，用下标取地址，比如：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">char</span> *a;</div><div class="line">&amp;a[<span class="number">7</span>]</div></pre></td></tr></table></figure>
<p>不过要注意的是这种方法只适用于 char 型的指针，其他类型的还是不行。</p>
<h2 id="文件流的偏移量"><a href="#文件流的偏移量" class="headerlink" title="文件流的偏移量"></a>文件流的偏移量</h2><p>如果要取文件流的偏移量，那就使用 stdio.h 中的 <a href="https://www.runoob.com/cprogramming/c-function-fseek.html" target="_blank" rel="external">fseek</a> 和 <a href="https://www.runoob.com/cprogramming/c-function-fread.html" target="_blank" rel="external">fread</a> 方法。</p>
<p>fseek 会将文件流指针偏移指定大小，fread 则可以将文件流的数据读取到特定的指针之中。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">FILE *fp;</div><div class="line"><span class="keyword">unsigned</span> <span class="keyword">char</span> *p;</div><div class="line"></div><div class="line"><span class="comment">// 将 fp 指针置于文件开头的第十个字节</span></div><div class="line">fseek(fp, <span class="number">10</span>, SEEK_SET);</div><div class="line"><span class="comment">// 将以下量的数据写入 p 指针中</span></div><div class="line"><span class="comment">// 每个元素大小为 100 字节，一共有一个元素</span></div><div class="line"><span class="comment">// 写成 fread(p, 1, 100, fp) 也是相同效果</span></div><div class="line">fread(p, <span class="number">100</span>, <span class="number">1</span>, fp);</div></pre></td></tr></table></figure>
<p>另外，<a href="https://www.runoob.com/cprogramming/c-function-ftell.html" target="_blank" rel="external">ftell</a> 可以让你知道给定文件流当前指针的位置，和 fseek 配合使用可以知道文件流的大小。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">fseek(fp, <span class="number">0</span>, SEEK_END);</div><div class="line"><span class="keyword">int</span> len = ftell(fp);</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在 C 语言一个指针的问题上卡了很久……虽然平常不用 C 语言，但是还是记录一下，不能让时间白白的被浪费了。&lt;/p&gt;
&lt;h2 id=&quot;指针加减偏移量&quot;&gt;&lt;a href=&quot;#指针加减偏移量&quot; class=&quot;headerlink&quot; title=&quot;指针加减偏移量&quot;&gt;&lt;/a&gt;指
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>理解多分类中的 mAP</title>
    <link href="https://blog.patrickcty.cc/2020/03/11/%E7%90%86%E8%A7%A3%E5%A4%9A%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84-k/"/>
    <id>https://blog.patrickcty.cc/2020/03/11/理解多分类中的-k/</id>
    <published>2020-03-11T02:33:33.000Z</published>
    <updated>2020-03-11T04:18:57.213Z</updated>
    
    <content type="html"><![CDATA[<p>在多分类任务中，有时候会用 mAP（Mean Average Precision）来表示分类的准确程度，如 VOC。其原因在于 mAP 能很好地评价分类的排序，而通常用的 accuracy 则往往会被最多的那个类别支配。</p>
<p>要计算多分类的 mAP，则要先计算各个类别的 AP。</p>
<h2 id="VOC-中-AP-的计算方法"><a href="#VOC-中-AP-的计算方法" class="headerlink" title="VOC 中 AP 的计算方法"></a>VOC 中 AP 的计算方法</h2><p>总的来说，就是对于某个类别，得到 n 个对该类别的预测概率，按照概率从大到小的顺序进行排列，然后对于 k∈1~n，求每个 k 对应的 Precision 和 Recall 值，对于每个 Recall 值，得到一个 Precision 值（==保证 P-R 曲线单调非递增==），将 n 个 Precision 取平均之后即为 AP 的值。要注意的是，这里不涉及到@k，因为总是为计算所有 n 个预测的结果。</p>
<p>对于一个四分类问题，已知标签和预测结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">y_true = np.array([[2], [1], [0], [3], [0], [1]]).astype(np.int64)</div><div class="line">y_pred = np.array([[0.1, 0.2, 0.6, 0.1],</div><div class="line">                   [0.8, 0.05, 0.1, 0.05],</div><div class="line">                   [0.3, 0.4, 0.1, 0.2],</div><div class="line">                   [0.6, 0.25, 0.1, 0.05],</div><div class="line">                   [0.1, 0.2, 0.6, 0.1],</div><div class="line">                   [0.9, 0.0, 0.03, 0.07]]).astype(np.float32)</div></pre></td></tr></table></figure>
<p>以类别 3 为例，六次预测给出的概率经过排序后为<code>[0.2  0.1  0.1  0.07 0.05 0.05]</code>，对应位置预测结果为<code>[0. 0. 0. 0. 0. 1.]</code>，0 表示预测错误，1 表示预测正确，那么可以列出来一个表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">top-k</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Recall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0/1</td>
<td style="text-align:center">0/1</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">0/2</td>
<td style="text-align:center">0/1</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0/3</td>
<td style="text-align:center">0/1</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">0/4</td>
<td style="text-align:center">0/1</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0/5</td>
<td style="text-align:center">0/1</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">1/6</td>
<td style="text-align:center">1/1</td>
</tr>
</tbody>
</table>
<p>在确保 P-R 曲线单调递减的情况下，求各个 Recall 对应的 Precision 的均值，在这里是 AP = (1/6) / 1 = 1/6</p>
<p>具体的内容参考<a href="https://link.zhihu.com/?target=http%3A//blog.sina.com.cn/s/blog_9db078090102whzw.html" target="_blank" rel="external">这一篇</a>。</p>
<p>同理可以求出来各个类别的 AP 为：<code>[1/3, 1/3, 1.0, 1/6]</code>，求均值后得到 MAP = 0.458。</p>
<p>一个 numpy 的实现为，来自这个 <a href="https://github.com/broadinstitute/keras-rcnn/issues/6" target="_blank" rel="external">issue</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># y_true is one-hot</span></div><div class="line"> _, classes = y_true.shape</div><div class="line">    </div><div class="line">average_precisions = []</div><div class="line"></div><div class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(classes):</div><div class="line">        <span class="comment"># 得到从大到小排序后的标签索引</span></div><div class="line">        row_indices_sorted = numpy.argsort(-y_pred[:, index])</div><div class="line"></div><div class="line">        <span class="comment"># 重新排列后的标签和预测结果</span></div><div class="line">        y_true_cls = y_true[row_indices_sorted, index]</div><div class="line">        y_pred_cls = y_pred[row_indices_sorted, index]</div><div class="line"></div><div class="line">        tp = (y_true_cls == <span class="number">1</span>)</div><div class="line">        fp = (y_true_cls == <span class="number">0</span>)</div><div class="line"></div><div class="line">        <span class="comment"># 每个位置是 top-i 的 fp 和 tp</span></div><div class="line">        fp = numpy.cumsum(fp)</div><div class="line">        tp = numpy.cumsum(tp)</div><div class="line"></div><div class="line">        <span class="comment"># 一共有多少预测正确的标签</span></div><div class="line">        npos = numpy.sum(y_true_cls)</div><div class="line"></div><div class="line">        <span class="comment"># top-i 的 recall</span></div><div class="line">        rec = tp*<span class="number">1.0</span> / npos</div><div class="line"></div><div class="line">        <span class="comment"># avoid divide by zero in case the first detection matches a difficult</span></div><div class="line">        <span class="comment"># ground truth</span></div><div class="line">        <span class="comment"># top-i 的 precision</span></div><div class="line">        prec = tp*<span class="number">1.0</span> / numpy.maximum((tp + fp), numpy.finfo(numpy.float64).eps)</div><div class="line"></div><div class="line">        <span class="comment"># 加上头和尾</span></div><div class="line">        mrec = numpy.concatenate(([<span class="number">0.</span>], rec, [<span class="number">1.</span>]))</div><div class="line">        mpre = numpy.concatenate(([<span class="number">0.</span>], prec, [<span class="number">0.</span>]))</div><div class="line"></div><div class="line">        <span class="comment"># compute the precision envelope</span></div><div class="line">        <span class="comment"># 保证 P-R 曲线单调递减</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(mpre.size - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">            mpre[i - <span class="number">1</span>] = numpy.maximum(mpre[i - <span class="number">1</span>], mpre[i])</div><div class="line"></div><div class="line">        <span class="comment"># to calculate area under PR curve, look for points</span></div><div class="line">        <span class="comment"># where X axis (recall) changes value</span></div><div class="line">        i = numpy.where(mrec[<span class="number">1</span>:] != mrec[:<span class="number">-1</span>])[<span class="number">0</span>]</div><div class="line"></div><div class="line">        <span class="comment"># and sum (\Delta recall) * prec</span></div><div class="line">        <span class="comment"># 相当于求每个 recall 值对应的 precision 的均值</span></div><div class="line">        average_precisions.append(numpy.sum((mrec[i + <span class="number">1</span>] - mrec[i]) * mpre[i + <span class="number">1</span>]))</div></pre></td></tr></table></figure>
<h2 id="TnesorFlow-中的-AP"><a href="#TnesorFlow-中的-AP" class="headerlink" title="TnesorFlow 中的 AP"></a>TnesorFlow 中的 AP</h2><p>TensorFlow 中使用以下方法来计算 mAP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">tf.compat.v1.metrics.average_precision_at_k(</div><div class="line">    labels, predictions, k, weights=None, metrics_collections=None,</div><div class="line">    updates_collections=None, name=None</div><div class="line">)</div></pre></td></tr></table></figure>
<p>但是这里并不是按照分类的标准类计算 mAP，而是对于检索来计算 mAP，而对于检索来说，总是==要考虑@k==，也就是考虑检索出来前 k 个的结果。</p>
<p>还是对于上面的例子，TF 中将 pred 看成了 6 次检索，每次检索有一个待检索对象（真值标签），检索产生四个概率值，这四个概率值的和为一。不过要注意的是，通常检索的时候待检索对象往往大于一，并且检索所产生的多个概率值的和不一定为一。</p>
<p>对于以下这一个预测结果，当 k = 1 的时候，0.8 对应的是标签 0，因此 Precision = 0/1，Recall = 0/1；k = 2，0.1 对应的是标签 2，因此 Precision = 0/2，Recall = 0/1；k = 3，0.05 对应的是标签 1，因此 Precision = 1/3，Recall = 1/1；k = 4，此时已经全部检索到了，因此 Precision = 1/3，Recall = 1。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">pred = [0.8, 0.05, 0.1, 0.05]</div><div class="line">true = [1]</div></pre></td></tr></table></figure>
<p>其他预测结果同理可求，因此 mAP@3 = (1 + 1/3 + 1/2 + 0 + 1/3 + 0) / 6 = 0.3611。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在多分类任务中，有时候会用 mAP（Mean Average Precision）来表示分类的准确程度，如 VOC。其原因在于 mAP 能很好地评价分类的排序，而通常用的 accuracy 则往往会被最多的那个类别支配。&lt;/p&gt;
&lt;p&gt;要计算多分类的 mAP，则要先计算各个
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tensorflow遇到的坑</title>
    <link href="https://blog.patrickcty.cc/2020/03/09/tensorflow%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/03/09/tensorflow遇到的坑/</id>
    <published>2020-03-08T16:09:12.000Z</published>
    <updated>2020-03-08T16:22:16.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="IO-的坑"><a href="#IO-的坑" class="headerlink" title="IO 的坑"></a>IO 的坑</h2><p>对于<code>tf.io.decode_image()</code>，如果指定 <code>dtype=tf.float32</code>，那么会默认把 tensor 的值除以 255。</p>
<p>我之前就在这里直接指定了，后面还减掉了 ImageNet 的均值（[123.68, 116.779, 103.939]），所以对于任何的图，其 RGB 通道上的值都基本上是 [-123.68, -116.779, -103.939]，也难怪网络的预测结果都是数量最多的那两个标签……</p>
<p>如果要像我这么做的话，就不指定 dtype，然后再用 <code>tf.cast()</code> 将 tensor 转化为浮点型，这时候就只转换数据类型不改变值的大小了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;IO-的坑&quot;&gt;&lt;a href=&quot;#IO-的坑&quot; class=&quot;headerlink&quot; title=&quot;IO 的坑&quot;&gt;&lt;/a&gt;IO 的坑&lt;/h2&gt;&lt;p&gt;对于&lt;code&gt;tf.io.decode_image()&lt;/code&gt;，如果指定 &lt;code&gt;dtype=tf.fl
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>colab遇到的坑</title>
    <link href="https://blog.patrickcty.cc/2020/02/28/colab%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/02/28/colab遇到的坑/</id>
    <published>2020-02-28T10:11:35.000Z</published>
    <updated>2020-02-28T10:19:15.662Z</updated>
    
    <content type="html"><![CDATA[<h2 id="训练特别慢"><a href="#训练特别慢" class="headerlink" title="训练特别慢"></a>训练特别慢</h2><h3 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h3><p>训练的数据集不要放在 google driver 里面，读取里面的文件特别慢，把 google driver 的文件拷贝到 colab 里面也特别慢。（不过把 colab 文件拷贝过去特别快）</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>直接再把相关的文件下载一遍，记得提前写好相关操作的脚本，每次直接执行一下。如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">!wget http://www.cs.bu.edu/groups/ivc/data/SOS/ESOS.zip;mkdir data;mkdir data/ESOS;mv ESOS.zip data/ESOS;unzip ESOS.zip &gt; zip.log</div><div class="line">from utils.dataset_utils.SOS import create_csv</div><div class="line">create_csv(&apos;/content/data/ESOS&apos;)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;训练特别慢&quot;&gt;&lt;a href=&quot;#训练特别慢&quot; class=&quot;headerlink&quot; title=&quot;训练特别慢&quot;&gt;&lt;/a&gt;训练特别慢&lt;/h2&gt;&lt;h3 id=&quot;坑&quot;&gt;&lt;a href=&quot;#坑&quot; class=&quot;headerlink&quot; title=&quot;坑&quot;&gt;&lt;/a&gt;坑&lt;/h
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>七牛云绑定域名的坑</title>
    <link href="https://blog.patrickcty.cc/2020/01/07/%E4%B8%83%E7%89%9B%E4%BA%91%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/01/07/七牛云绑定域名的坑/</id>
    <published>2020-01-07T13:36:01.000Z</published>
    <updated>2020-01-07T13:38:59.035Z</updated>
    
    <content type="html"><![CDATA[<p>之前七牛云绑定域名的 SSL 证书过期了，在过期之前我已经特意申请了一个新（也是在七牛云申请的）的并绑定了，但是当旧的证书过期的时候新证书却并没有生效。</p>
<p>原因在于绑定域名的证书是通过绑定域名的配置来决定的，要手动修改相应的配置，将证书选择为新的证书，然后才会使用新的证书 orz。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前七牛云绑定域名的 SSL 证书过期了，在过期之前我已经特意申请了一个新（也是在七牛云申请的）的并绑定了，但是当旧的证书过期的时候新证书却并没有生效。&lt;/p&gt;
&lt;p&gt;原因在于绑定域名的证书是通过绑定域名的配置来决定的，要手动修改相应的配置，将证书选择为新的证书，然后才会使
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>最近 tf_keras 中遇到的一些坑</title>
    <link href="https://blog.patrickcty.cc/2020/01/07/%E6%9C%80%E8%BF%91keras%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2020/01/07/最近keras中遇到的一些坑/</id>
    <published>2020-01-07T12:25:41.000Z</published>
    <updated>2020-02-16T01:37:54.455Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文件名格式不对产生-OSError"><a href="#文件名格式不对产生-OSError" class="headerlink" title="文件名格式不对产生 OSError"></a>文件名格式不对产生 OSError</h2><h3 id="产生原因"><a href="#产生原因" class="headerlink" title="产生原因"></a>产生原因</h3><p>在使用 model.save(model_path) 的时候，由于 model_path 格式不对，而产生了 OSError，一个不对的格式是：</p>
<blockquote>
<p>Tue-Jan-7-16-59-41-2020.sos_model.035-mae.0.1543.hdf5</p>
</blockquote>
<p>可能是这个 <code>.0.</code> 导致了这个错误。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>把 mae 后面的 <code>.</code> 改成 <code>-</code> 即可，即：</p>
<blockquote>
<p>Tue-Jan-7-16-59-41-2020.sos_model.035-mae-0.1543.hdf5</p>
</blockquote>
<h2 id="在同样的环境中重复载入模型"><a href="#在同样的环境中重复载入模型" class="headerlink" title="在同样的环境中重复载入模型"></a>在同样的环境中重复载入模型</h2><h3 id="产生原因-1"><a href="#产生原因-1" class="headerlink" title="产生原因"></a>产生原因</h3><p>在同样的环境中重复载入模型会导致没有指定名称的层的名字会发生改变，比如某个卷积层的定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x = layers.Conv2D(layer, (3, 3))(x)</div></pre></td></tr></table></figure>
<p>那么，第一次载入这一层的名字可能就是 conv2d，再次在同样的环境中载入，名字可能就变成了 conv2d_1。</p>
<p>这一点要特别注意一下，特别是在 notebook 中运行的时候，很可能会让原本功能正常的方法报错。</p>
<blockquote>
<p>另外，不用担心同一个环境中加载的不同模型有同样的层名会有问题，因为不在一个计算图里面，所以不会产生冲突。</p>
</blockquote>
<h3 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法"></a>解决方法</h3><p>报错了就清除当前的上下文，notebook 里面就重启 kernel。</p>
<h2 id="lr-multiplier-无法正确读入自定义的优化器"><a href="#lr-multiplier-无法正确读入自定义的优化器" class="headerlink" title="lr multiplier 无法正确读入自定义的优化器"></a>lr multiplier 无法正确读入自定义的优化器</h2><h3 id="产生原因-2"><a href="#产生原因-2" class="headerlink" title="产生原因"></a>产生原因</h3><p><a href="https://github.com/CyberZHG/keras-lr-multiplier" target="_blank" rel="external">原模块</a>引入 keras 的方法可能和现在的方法不匹配。</p>
<h3 id="解决方法-2"><a href="#解决方法-2" class="headerlink" title="解决方法"></a>解决方法</h3><p>把源文件复制到当前项目下，再改一下引入就正常了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 修改之后的方法</div><div class="line">import tensorflow.python.keras.optimizers as optimizers</div></pre></td></tr></table></figure>
<h2 id="plot-model-报缺少-graphviz-的错"><a href="#plot-model-报缺少-graphviz-的错" class="headerlink" title="plot_model 报缺少 graphviz 的错"></a>plot_model 报缺少 graphviz 的错</h2><p>直接运行 <code>plot_model</code> 的时候会报缺少依赖的错误，安装了相关依赖之后还有相同的错误，如下所示:</p>
<blockquote>
<p>keras ImportError: Failed to import pydot. You must install pydot and graphviz for <code>pydotprint</code> to work.</p>
</blockquote>
<h3 id="产生原因-3"><a href="#产生原因-3" class="headerlink" title="产生原因"></a>产生原因</h3><p>graphviz 需要系统级的安装。</p>
<h3 id="解决方法-3"><a href="#解决方法-3" class="headerlink" title="解决方法"></a>解决方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># for mac</div><div class="line">brew install graphviz</div><div class="line"></div><div class="line"># for ubuntu</div><div class="line">sudo apt-get install graphviz</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;文件名格式不对产生-OSError&quot;&gt;&lt;a href=&quot;#文件名格式不对产生-OSError&quot; class=&quot;headerlink&quot; title=&quot;文件名格式不对产生 OSError&quot;&gt;&lt;/a&gt;文件名格式不对产生 OSError&lt;/h2&gt;&lt;h3 id=&quot;产生原因&quot;
    
    </summary>
    
    
      <category term="keras" scheme="https://blog.patrickcty.cc/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>部署tf_keras模型遇到的坑</title>
    <link href="https://blog.patrickcty.cc/2019/12/14/%E9%83%A8%E7%BD%B2tf-keras%E6%A8%A1%E5%9E%8B%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2019/12/14/部署tf-keras模型遇到的坑/</id>
    <published>2019-12-14T03:20:19.000Z</published>
    <updated>2019-12-14T03:22:21.364Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一次-load-多次-predict"><a href="#一次-load-多次-predict" class="headerlink" title="一次 load 多次 predict"></a>一次 load 多次 predict</h2><p>由于 TensorFlow 使用的是静态图，因此默认并不能做到一次 load 然后就任意在其他的地方调用，因此需要以下方法来做到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># load</span></div><div class="line">model = load_model(<span class="string">'path_to_model'</span>)</div><div class="line">graph = tf.get_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># predict, maybe in another thread</span></div><div class="line"><span class="keyword">global</span> graph</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    model.predict()</div></pre></td></tr></table></figure>
<h2 id="限制不要占满-GPU-显存"><a href="#限制不要占满-GPU-显存" class="headerlink" title="限制不要占满 GPU 显存"></a>限制不要占满 GPU 显存</h2><p>TensorFlow 默认的机制是会占用满 GPU，但是可以通过设置 allow_growth 来只使用必须的显存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.python.keras.backend <span class="keyword">import</span> set_session</div><div class="line"></div><div class="line">config = tf.ConfigProto()</div><div class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></div><div class="line">sess = tf.Session(config=config)</div><div class="line">set_session(sess)  <span class="comment"># 必须要先设置 session 才能 load model</span></div></pre></td></tr></table></figure>
<p>进行这样设置之后如果要在多个不同情况下调用模型则也必须每次都设置 session：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># predict, maybe in another thread</span></div><div class="line"><span class="keyword">global</span> graph</div><div class="line"><span class="keyword">global</span> sess</div><div class="line"><span class="keyword">with</span> graph.as_default():</div><div class="line">    set_session(sess)</div><div class="line">    model.predict()</div></pre></td></tr></table></figure>
<h2 id="忽视掉-TensorFlow-的一大堆初始化-log"><a href="#忽视掉-TensorFlow-的一大堆初始化-log" class="headerlink" title="忽视掉 TensorFlow 的一大堆初始化 log"></a>忽视掉 TensorFlow 的一大堆初始化 log</h2><p>log level 为 3 的话就只会输出错误信息了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'3'</span></div><div class="line">tf.get_logger().setLevel(<span class="string">'ERROR'</span>)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一次-load-多次-predict&quot;&gt;&lt;a href=&quot;#一次-load-多次-predict&quot; class=&quot;headerlink&quot; title=&quot;一次 load 多次 predict&quot;&gt;&lt;/a&gt;一次 load 多次 predict&lt;/h2&gt;&lt;p&gt;由于 Ten
    
    </summary>
    
    
      <category term="TensorFlow" scheme="https://blog.patrickcty.cc/tags/TensorFlow/"/>
    
      <category term="keras" scheme="https://blog.patrickcty.cc/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>配置内网穿透</title>
    <link href="https://blog.patrickcty.cc/2019/11/20/%E9%85%8D%E7%BD%AE%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    <id>https://blog.patrickcty.cc/2019/11/20/配置内网穿透/</id>
    <published>2019-11-20T09:03:14.000Z</published>
    <updated>2019-11-20T10:29:44.214Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>内网穿透，顾名思义就是从外网访问一个在内网的服务器。我们实验室的 GPU 服务器都是在内网里面，远程访问非常不方便，为了以防万一，还是需要配配置一个内网穿透。</p>
<h2 id="frp"><a href="#frp" class="headerlink" title="frp"></a>frp</h2><p>这里介绍使用 <a href="https://github.com/fatedier/frp" target="_blank" rel="external">frp</a> 配置内网穿透的过程。</p>
<blockquote>
<p>frp 是一个可用于内网穿透的高性能的反向代理应用，支持 tcp, udp 协议，为 http 和 https 应用协议提供了额外的能力，且尝试性支持了点对点穿透。</p>
</blockquote>
<h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul>
<li>一个有外网 ip 的服务器：作为内网服务器与客户端的中介</li>
<li>frp 运行文件：这个服务器和客户端都要下载，链接在<a href="https://github.com/fatedier/frp/releases" target="_blank" rel="external">这里</a></li>
</ul>
<h2 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h2><ol>
<li>下载 frp 文件，解压</li>
<li><p>进入相应文件夹，编辑 frps.ini</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># frps.ini</div><div class="line">[common]</div><div class="line">bind_port = 7000 # frp运行端口,需要开放防火墙</div><div class="line">vhost_http_port = 8080 # http服务端口,可以 和 bind_port 相同</div></pre></td></tr></table></figure>
</li>
<li><p>启动 ./frps -c ./frps.ini</p>
</li>
</ol>
<h2 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h2><ol>
<li>下载 frp 文件，解压</li>
<li><p>进入相应文件夹，编辑 frpc.ini（注意是 frpc）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># frpc.ini</div><div class="line">[common]</div><div class="line">server_addr = 上一步的服务器地址</div><div class="line">server_port = 上一步填写的端口</div><div class="line"></div><div class="line">[ssh]</div><div class="line">type = tcp</div><div class="line">local_ip = 127.0.0.1</div><div class="line">local_port = 22</div><div class="line">remote_port = 6000  # 通过这个端口来访问</div></pre></td></tr></table></figure>
</li>
<li><p>启动 ./frpc -c ./frpc.ini</p>
</li>
<li>使用 ssh 访问 <code>ssh -oPort 6000 ssh_name@ssh_ip</code></li>
</ol>
<h2 id="多个客户端配置"><a href="#多个客户端配置" class="headerlink" title="多个客户端配置"></a>多个客户端配置</h2><p>多个客户端配置只要 ssh 下的 remote_port 不冲突即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># frpc2.ini</div><div class="line">[common]</div><div class="line">server_addr = 上一步的服务器地址</div><div class="line">server_port = 上一步填写的端口</div><div class="line"></div><div class="line">[ssh]</div><div class="line">type = tcp</div><div class="line">local_ip = 127.0.0.1</div><div class="line">local_port = 22</div><div class="line">remote_port = 6001  # 这个地方不冲突即可</div><div class="line"></div><div class="line"># 使用 ssh 访问 ssh -oPort 6001 ssh_name@ssh_ip</div></pre></td></tr></table></figure></p>
<h2 id="配置客户端-http-访问"><a href="#配置客户端-http-访问" class="headerlink" title="配置客户端 http 访问"></a>配置客户端 http 访问</h2><ol>
<li><p>修改客户端上 frpc.ini 文件，加入以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[web]</div><div class="line">type = http</div><div class="line">local_port = 80  # 或者是其他的端口也都可以</div><div class="line">custom_domains = www.example.com</div></pre></td></tr></table></figure>
</li>
<li><p>访问网页 <code>www.example.com:8080</code>(这个端口是服务器 frps.ini 里面的 vhost_http_port)</p>
</li>
</ol>
<h2 id="将-frp-添加到启动项"><a href="#将-frp-添加到启动项" class="headerlink" title="将 frp 添加到启动项"></a>将 frp 添加到启动项</h2><ol>
<li><p><code>sudo vim /lib/systemd/system/frps.service</code>，添加以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=frps</div><div class="line">After=network.target</div><div class="line"></div><div class="line">[Service]</div><div class="line">TimeoutStartSec=30</div><div class="line">ExecStart=$&#123;frps的绝对路径&#125; -c $&#123;frps.ini的绝对路径&#125;</div><div class="line">ExecStop=/bin/kill $MAINPID</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
</li>
<li><p>启用服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">systemctl enable frps</div><div class="line">systemctl start frps</div><div class="line">systemctl status frps</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.xinshangshangxin.com/2018/06/18/frp/" target="_blank" rel="external">https://blog.xinshangshangxin.com/2018/06/18/frp/</a></li>
<li><a href="https://github.com/fatedier/frp/issues/407" target="_blank" rel="external">https://github.com/fatedier/frp/issues/407</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;内网穿透，顾名思义就是从外网访问一个在内网的服务器。我们实验室的 GPU 服务器都是在内网里面，远程访问非常不方便，为了以防万一，还是需要配
    
    </summary>
    
      <category term="utils" scheme="https://blog.patrickcty.cc/categories/utils/"/>
    
    
      <category term="内网穿透" scheme="https://blog.patrickcty.cc/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>使用keras进行语义分割注意事项</title>
    <link href="https://blog.patrickcty.cc/2019/08/22/%E4%BD%BF%E7%94%A8keras%E8%BF%9B%E8%A1%8C%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
    <id>https://blog.patrickcty.cc/2019/08/22/使用keras进行语义分割注意事项/</id>
    <published>2019-08-22T12:37:28.000Z</published>
    <updated>2019-08-22T12:38:25.198Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><ul>
<li>载入数据<ul>
<li>图像和标签相对应</li>
<li>数据预处理与增强</li>
<li>标签处理</li>
<li>标签还原与可视化预测结果 </li>
<li>多输入/出处理</li>
</ul>
</li>
<li>网络结构<ul>
<li>反卷积</li>
<li>上采样</li>
<li>todo</li>
</ul>
</li>
<li>训练脚本<ul>
<li>Warm-up</li>
<li>多输入/出（合并到第一部分）</li>
</ul>
</li>
</ul>
<h2 id="载入数据"><a href="#载入数据" class="headerlink" title="载入数据"></a>载入数据</h2><h3 id="图像和标签对应"><a href="#图像和标签对应" class="headerlink" title="图像和标签对应"></a>图像和标签对应</h3><p>语义分割中需要同时载入图像和标签，和分类问题不一样，语义分割的标签也是图像，因此要和载入的图像相对应。</p>
<p>如果使用 keras 自带的 ImageDataGenerator 来获取数据则需要分别为图像和标签初始化两个对象，并传入相同的随机化种子，具体操作如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># we create two instances with the same arguments</div><div class="line">data_gen_args = dict(featurewise_center=True,</div><div class="line">                     featurewise_std_normalization=True,</div><div class="line">                     rotation_range=90,</div><div class="line">                     width_shift_range=0.1,</div><div class="line">                     height_shift_range=0.1,</div><div class="line">                     zoom_range=0.2)</div><div class="line">image_datagen = ImageDataGenerator(**data_gen_args)</div><div class="line">mask_datagen = ImageDataGenerator(**data_gen_args)</div><div class="line"></div><div class="line"># Provide the same seed and keyword arguments to the fit and flow methods</div><div class="line">seed = 1</div><div class="line">image_datagen.fit(images, augment=True, seed=seed)</div><div class="line">mask_datagen.fit(masks, augment=True, seed=seed)</div><div class="line"></div><div class="line">image_generator = image_datagen.flow_from_directory(</div><div class="line">    directory=&apos;data&apos;,</div><div class="line">    class_mode=None,</div><div class="line">    classes=[&apos;图像文件夹的名字&apos;],</div><div class="line">    seed=seed)</div><div class="line"></div><div class="line">mask_generator = mask_datagen.flow_from_directory(</div><div class="line">    &apos;data&apos;,</div><div class="line">    class_mode=None,</div><div class="line">    classes=[&apos;标签文件夹的名字&apos;],</div><div class="line">    seed=seed)</div><div class="line"></div><div class="line"># combine generators into one which yields image and masks</div><div class="line">train_generator = zip(image_generator, mask_generator)</div><div class="line"></div><div class="line">model.fit_generator(</div><div class="line">    directory=train_generator,</div><div class="line">    # 要指定这个参数，因为 zip 之后无法知道</div><div class="line">    # 每个 epoch 有多少次迭代</div><div class="line">    steps_per_epoch=2000,</div><div class="line">    epochs=50)</div></pre></td></tr></table></figure>
<p>其中要注意的有：</p>
<ul>
<li><code>flow_from_directory</code> 方法是通过 directory + class name 来寻找图片的，也就是说如果在这里让 <code>directory=&#39;data/images&#39;</code> 并且不设置 classes 则无法读取到图片；另外，<code>classes</code> 应该传入一个列表；还有则是 <code>class_mode</code> 要设置为 None，不然就会根据子文件夹名返回标签</li>
<li><code>fit_generator</code> 中要指定 <code>steps_per_epoch</code>，在这里可以通过 <code>ceil(len(image_generator / batch_size)</code> 来计算</li>
<li><code>fit</code> 和 <code>flow</code> 传入相同的随机化种子以保证生成相对应的图片</li>
</ul>
<h3 id="图像预处理与增强"><a href="#图像预处理与增强" class="headerlink" title="图像预处理与增强"></a>图像预处理与增强</h3><ul>
<li>标准化: <ul>
<li><code>img = (img - img_mean) / img_std</code></li>
<li>简化起见也可以 <code>img = img / 255 - 0.5</code></li>
</ul>
</li>
<li>数据增强：<ul>
<li>语义分割中最主要的增强是通过随机裁剪来进行的 ，具体的操作是把一张很大的图片随机裁剪出若干张(256, 256) 大小的图片，这样既增加了数据大小，也避免缩放出现的信息丢失</li>
<li>todo：还需要进一步查看文档</li>
</ul>
</li>
</ul>
<h3 id="标签处理"><a href="#标签处理" class="headerlink" title="标签处理"></a>标签处理</h3><p>由于语义分割的标签是和原图像同样大小的彩色图片，并且为了标注的方便，标签图像用一种颜色，也就是一组 RGB 的值来表示一种类别。</p>
<p>在进行损失函数的计算时，我们需要将 RGB 值转换成一组 one-hot 的标签，并且把图片拉成一个向量（以便于计算交叉熵损失）。比如对于 (256, 256, 3) 的标签，我们要将其转变成 (256 <strong> 2, 3) 的向量，然后转换为 (256 </strong> 2, num_classes) 的 one-hot 形式。具体的操作如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"># VOC 中的列表表示</div><div class="line">VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],</div><div class="line">                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],</div><div class="line">                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],</div><div class="line">                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],</div><div class="line">                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],</div><div class="line">                [0, 64, 128]]</div><div class="line"></div><div class="line">VOC_CLASSES = [&apos;background&apos;, &apos;aeroplane&apos;, &apos;bicycle&apos;, &apos;bird&apos;, &apos;boat&apos;,</div><div class="line">               &apos;bottle&apos;, &apos;bus&apos;, &apos;car&apos;, &apos;cat&apos;, &apos;chair&apos;, &apos;cow&apos;,</div><div class="line">               &apos;diningtable&apos;, &apos;dog&apos;, &apos;horse&apos;, &apos;motorbike&apos;, &apos;person&apos;,</div><div class="line">               &apos;potted plant&apos;, &apos;sheep&apos;, &apos;sofa&apos;, &apos;train&apos;, &apos;tv/monitor&apos;]</div><div class="line"></div><div class="line"># 完成 RGB 与类别数值的映射</div><div class="line">colormap2label = np.zeros(256 ** 3)</div><div class="line">label2colormap = np.zeros_like(VOC_CLASSES)</div><div class="line">for i, colormap in enumerate(VOC_COLORMAP):</div><div class="line">    idx = (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]</div><div class="line">    colormap2label[idx] = i</div><div class="line">    label2colormap[i] = idx</div><div class="line"></div><div class="line">def mask_preprocessing(mask):</div><div class="line">    mask = mask.astype(&apos;int32&apos;)</div><div class="line">    if len(mask.shape) == 3:  # 输入为单个图片</div><div class="line">        w, h, _ = mask.shape</div><div class="line">        # 将 RGB 的类别值转换成单一数字表示的的类别值</div><div class="line">        idx = ((mask[:, :, 0] * 256 + mask[:, :, 1]) * 256 + mask[:, :, 2])</div><div class="line">        # 将图片拉成向量</div><div class="line">        new_mask = colormap2label[idx].reshape((w * h, -1))</div><div class="line">    else:  # len == 4  # 输入为一个 batch 的图片</div><div class="line">        b, w, h, _ = mask.shape</div><div class="line">        idx = ((mask[:, :, :, 0] * 256 + mask[:, :, :, 1]) * 256 + mask[:, :, :, 2])</div><div class="line">        new_mask = colormap2label[idx].reshape((b, w * h, -1))</div><div class="line">    # 将标签转换为 one-hot 的形式</div><div class="line">    r_mask = to_categorical(new_mask, num_classes=self.config.num_classes)</div><div class="line">    return r_mask</div></pre></td></tr></table></figure>
<h3 id="标签还原与可视化预测结果"><a href="#标签还原与可视化预测结果" class="headerlink" title="标签还原与可视化预测结果"></a>标签还原与可视化预测结果</h3><p>预测结果的标签和输入的一样都是一个 (w * h, num_classes) 的 one-hot 矩阵，我们要先将其变成非 one-hot 的类别形式，然后再还原成 RGB 的形式，然后将其变成图片的形状，之后输出的标签就和标注好的标签比较类似了。操作方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"># 将图片从单个数字表示的类别映射回 RGB</div><div class="line">def labelVisualize(self, num_class, img):</div><div class="line">    img = img[:, :, 0] if len(img.shape) == 3 else img</div><div class="line">    img_out = np.zeros(img.shape + (3,))</div><div class="line">    for i in range(num_class):</div><div class="line">        img_out[img == i, :] = VOC_COLORMAP[i]</div><div class="line">    return img_out / 255</div><div class="line"></div><div class="line">def mask_propressing(predict_mask_list, save_to=None):</div><div class="line">    # 这里是因为有多个输出因此要用循环来处理</div><div class="line">    for i in range(len(predict_mask_list)):</div><div class="line">        # 将 one-hot 还原</div><div class="line">        num_classes = predict_mask_list[0].shape[-1]</div><div class="line">        predict_mask = predict_mask_list[i]</div><div class="line">        num_classes = predict_mask.shape[-1]</div><div class="line">        mkdir_if_not_exist(os.path.join(save_to, str(i)))</div><div class="line">        if len(predict_mask.shape) == 3:  # (?, 65536, 21)</div><div class="line">            temp_mask = predict_mask.reshape((predict_mask.shape[0], self.config.input_shape,</div><div class="line">                                              self.config.input_shape, num_classes))</div><div class="line">            for item in temp_mask:</div><div class="line">                img = self.labelVisualize(num_classes, item)</div><div class="line">                if save_to:</div><div class="line">                    io.imsave(os.path.join(save_to, str(i), &apos;%s_pred.png&apos; % str(uuid.uuid4())[:4]), img)</div><div class="line"></div><div class="line">        else:  # (65536, 21)</div><div class="line">            temp_mask = predict_mask.reshape((self.config.input_shape, self.config.input_shape, num_classes))</div><div class="line">            img = self.labelVisualize(num_classes, temp_mask)</div><div class="line">            if save_to:</div><div class="line">                io.imsave(os.path.join(save_to, str(i), &apos;%s_pred.png&apos; % str(uuid.uuid4())[:4]), img)</div></pre></td></tr></table></figure>
<h3 id="多输出处理"><a href="#多输出处理" class="headerlink" title="多输出处理"></a>多输出处理</h3><p>如果神经网络有多输入/出，则对于每个输入和输出都要有相应的标签进行对应：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 输入到 fit_generator 的形式</div><div class="line"># tuple 的第一个第二个分别是输入和输入</div><div class="line"># 多个不同的通过名字和数据的字典来对应</div><div class="line">(&#123;&apos;input1&apos;: img1, &apos;input2&apos;, img2&#125;, </div><div class="line"> &#123;&apos;pred1&apos;: mask1, &apos;pred2&apos;: mask2, &apos;pred3&apos;: mask3&#125;)</div><div class="line"> </div><div class="line"># 输入 comple 的形式</div><div class="line"># 这里主要是对输入的内容进行指定</div><div class="line">model.compile(optimizer=&apos;rmsprop&apos;,</div><div class="line">              loss=&#123;&apos;pred1&apos;: &apos;binary_crossentropy&apos;, &apos;pred2&apos;: &apos;binary_crossentropy&apos;&#125;,</div><div class="line">              &apos;pred3&apos;: &apos;binary_crossentropy&apos;&#125;,</div><div class="line">              loss_weights=&#123;&apos;pred1&apos;: 1., &apos;pred2&apos;: 0.2, &apos;pred3&apos;: 0.2&#125;)</div></pre></td></tr></table></figure>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>todo</p>
<h2 id="训练脚本"><a href="#训练脚本" class="headerlink" title="训练脚本"></a>训练脚本</h2><h3 id="Warm-up"><a href="#Warm-up" class="headerlink" title="Warm-up"></a>Warm-up</h3><p>在开始的时候使用比较小的学习率训练，再使用正常的学习率训练，这种方法可以防止神经网络在一开始跑向了错误的方向而取得不理想的效果。</p>
<p>假设初始的学习率为 <code>lr_init</code>，warm-up 的 epoch 数为 <code>num</code>，当前 epoch 数为 <code>i</code>（从 1 开始计数），则当前学习率为：<code>lr = lr_init * i / num (i &lt; num)</code>。</p>
<p>在 keras 里面可以用 LeaningRateScheduler 的回调来实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def lr_scheduler_warm_up(epoch_idx, cur_lr):</div><div class="line">    if epoch_idx == 0:</div><div class="line">        return cur_lr / 5</div><div class="line">    if epoch_idx &lt; 5:  # 前五个 epoch</div><div class="line">        return cur_lr * (epoch_idx + 1) / epoch_idx</div><div class="line">    return cur_lr</div><div class="line"></div><div class="line">lr_warm_up = LearningRateScheduler(lr_scheduler_warm_up)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;总览&quot;&gt;&lt;a href=&quot;#总览&quot; class=&quot;headerlink&quot; title=&quot;总览&quot;&gt;&lt;/a&gt;总览&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;载入数据&lt;ul&gt;
&lt;li&gt;图像和标签相对应&lt;/li&gt;
&lt;li&gt;数据预处理与增强&lt;/li&gt;
&lt;li&gt;标签处理&lt;/li&gt;
&lt;li&gt;标
    
    </summary>
    
      <category term="keras" scheme="https://blog.patrickcty.cc/categories/keras/"/>
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/categories/keras/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割" scheme="https://blog.patrickcty.cc/categories/keras/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
    
      <category term="语义分割" scheme="https://blog.patrickcty.cc/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>深度学习一些概念整理</title>
    <link href="https://blog.patrickcty.cc/2019/05/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/"/>
    <id>https://blog.patrickcty.cc/2019/05/03/深度学习一些概念整理/</id>
    <published>2019-05-03T03:26:37.000Z</published>
    <updated>2019-12-30T13:48:30.511Z</updated>
    
    <content type="html"><![CDATA[<h2 id="全连接层转换为卷积层"><a href="#全连接层转换为卷积层" class="headerlink" title="全连接层转换为卷积层"></a>全连接层转换为卷积层</h2><h3 id="如何转换"><a href="#如何转换" class="headerlink" title="如何转换"></a>如何转换</h3><p>卷积层与全连接层可以相互转换，例如，对于输入为 7 <em> 7 </em> 512 的有 4096 个神经元的全连接层，可以表示为如下的卷积层：</p>
<ul>
<li>Kernel size: 7</li>
<li>Padding: 0</li>
<li>Stride: 1</li>
<li>Filters: 4096</li>
</ul>
<p>这样卷积层输出就是 1 <em> 1 </em> 4096，与全连接层的输出相同。</p>
<h3 id="转换的原理"><a href="#转换的原理" class="headerlink" title="转换的原理"></a>转换的原理</h3><p>还是以上面的为例，全连接层的每个输出（1/4096）需要输入的每一个元素都参与，即：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Dense_&#123;out_i&#125; = \Sigma_&#123;j=1&#125;^&#123;25088&#125;&#123;(w_&#123;ij&#125; * in_j)&#125; \\ i = 1, 2, \dots, 4096</div></pre></td></tr></table></figure>
<p>而对应的卷积层的输出也是需要每个元素的参与，即：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Conv_&#123;out_i&#125; = \Sigma_&#123;j=1&#125;^&#123;512&#125;&#123;\Sigma_&#123;k=1&#125;^&#123;49&#125;&#123;(w_&#123;ijk&#125; * in_&#123;jk&#125;)&#125;&#125; \\ i = 1, 2, \dots, 4096</div></pre></td></tr></table></figure>
<p>可以看出来，本质没有变，只是形式变了。</p>
<h3 id="转换的作用"><a href="#转换的作用" class="headerlink" title="转换的作用"></a>转换的作用</h3><ul>
<li>便于计算不同分辨率的输入</li>
</ul>
<p>比如 AlexNet 输入为 227 <em> 227，如果输入为 384 </em> 384 则到了全连接层会报错，而如果进行了如上转换，则会正常输出，不过输出结果为 6 <em> 6 </em> 1000 而非 1 <em> 1 </em> 1000。</p>
<p>实际上，常用于分割任务上的全卷积网络（FCN）就使用了这种方法去掉了全连接层以支持不同分辨率的输入。</p>
<h2 id="1-1-卷积核作用"><a href="#1-1-卷积核作用" class="headerlink" title="1 * 1 卷积核作用"></a>1 * 1 卷积核作用</h2><ul>
<li>实现数据维度改变</li>
</ul>
<p>可以把维度从 m <em> n </em> 10 改变到 m <em> n </em> 5 或者 m <em> n </em> 50，只需要改变 filter 的数量</p>
<ul>
<li>实现跨通道的交互信息的整合</li>
</ul>
<p>由于次个卷积会把多个通道的信息汇总，因此这个 1 * 1 卷积的过程也会把跨通道的交互信息整合起来</p>
<ul>
<li>增加非线性</li>
</ul>
<p>通过卷积之后的非线性激活函数来实现</p>
<p>具体讲解参考<a href="https://zhuanlan.zhihu.com/p/40050371" target="_blank" rel="external">这篇文章</a>。</p>
<h2 id="Global-Average-Pooling-GAP-作用"><a href="#Global-Average-Pooling-GAP-作用" class="headerlink" title="Global Average Pooling (GAP) 作用"></a>Global Average Pooling (GAP) 作用</h2><p>GAP 是用来将每个通道的信息统一到同一个像素上，比如输入为 7 <em> 7 </em> 512 的张量，经过 GAP 之后输出就是 1 <em> 1 </em> 512。</p>
<ul>
<li>代替全连接层</li>
</ul>
<p>CNN 最后一层卷积的输出就是一个高度提取的 feature map（本质上是特征），而全连接层就是将最后一层卷积得到的特征整合起来映射到样本空间（分类，识别等）（由于有非线性激活函数，因此不是线性映射）。</p>
<p>具体理解参照<a href="https://www.zhihu.com/question/41037974/answer/320267531" target="_blank" rel="external">这个回答</a>。</p>
<p>关于 GAP 的效果对比可以参考<a href="https://www.cnblogs.com/hutao722/p/10008581.html" target="_blank" rel="external">这篇博客</a>。</p>
<p>我自己在小型的神经网络上训练的结果（GAP 代替了 128 FC）是使用 GAP 提高了 2% 左右的准确度。</p>
<h2 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h2><p>深度学习更新参数通常不是由一个数据来决定的，不然更新的方向可能会差别较大（一下这边一下那边）。</p>
<p>一批里面有多个数据更容易朝着正确的方向进行，利用各个数据之间的一些共性。</p>
<p>另外，使用批处理也避免了一次使用所有数据进行训练，减小了显存的压力，也更有可能找到极小值。</p>
<h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a>epoch</h2><p>一个完整的数据集经过网络一次并返回一次的结果。一个 epoch 并不一定能让网络达到最佳的方向，因而要训练多个 epoch，但是训练过多会过拟合。</p>
<h2 id="Batch-Normalization-作用"><a href="#Batch-Normalization-作用" class="headerlink" title="Batch Normalization 作用"></a>Batch Normalization 作用</h2><p>将一个 batch 中的各个元素归一化（各个通道分别进行 BN），通常放到激活函数之前。</p>
<p>好处：</p>
<ul>
<li>减小梯度弥散（梯度消失/爆炸）</li>
</ul>
<p>改变激活函数的输入，让数据分布更均匀，从而杀掉的神经元更少。</p>
<ul>
<li>训练更快，可以用更高的学习率</li>
</ul>
<p>数据分布更均匀，神经网络不用去适应各种分布</p>
<ul>
<li>一定程度增加泛化能力，避免过拟合</li>
</ul>
<p>数据通过 BN 处理引入的随机噪声能够起到对模型参数进行正则化的作用，有利于增强模型泛化能力</p>
<p>缺陷：</p>
<ul>
<li>不适用 batch 非常小</li>
<li>不适用 RNN</li>
</ul>
<p>关于 BN 的理解，尤其是背后的原理理解参考<a href="https://www.jiqizhixin.com/articles/2018-08-29-7" target="_blank" rel="external">这篇文章</a></p>
<h2 id="验证-测试集的作用"><a href="#验证-测试集的作用" class="headerlink" title="验证/测试集的作用"></a>验证/测试集的作用</h2><h3 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h3><ul>
<li>与测试集同分布，用来观察训练的走向（是否过拟合等）</li>
<li>选择超参数</li>
</ul>
<h3 id="测试集"><a href="#测试集" class="headerlink" title="测试集"></a>测试集</h3><p>评估模型的训练状况，检验泛化能力</p>
<h2 id="Softmax-是什么"><a href="#Softmax-是什么" class="headerlink" title="Softmax 是什么"></a>Softmax 是什么</h2><p>Softmax 函数用来产生 k 个概率，而 Softmax 损失函数，准确来说是用 Softmax 的结果来计算交叉熵分类损失函数。</p>
<p>Softmax 函数可以参考<a href="https://blog.csdn.net/ture_dream/article/details/54948518" target="_blank" rel="external">这篇博文</a>，交叉熵可以参考<a href="https://zhuanlan.zhihu.com/p/35709485" target="_blank" rel="external">这篇博文</a>。</p>
<h2 id="logits-是什么"><a href="#logits-是什么" class="headerlink" title="logits 是什么"></a>logits 是什么</h2><p>出现于 <code>tf.nn.softmax_cross_entropy_with_logits</code>，简单来说就是 softmax 的输入。更多内容可以参考<a href="https://www.zhihu.com/question/60751553" target="_blank" rel="external">这个问题</a>。</p>
<h2 id="weight-的冷知识"><a href="#weight-的冷知识" class="headerlink" title="weight 的冷知识"></a>weight 的冷知识</h2><p>weight decay = 对 weight 施加 norm = 对输入数据增加方差小的噪音</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;全连接层转换为卷积层&quot;&gt;&lt;a href=&quot;#全连接层转换为卷积层&quot; class=&quot;headerlink&quot; title=&quot;全连接层转换为卷积层&quot;&gt;&lt;/a&gt;全连接层转换为卷积层&lt;/h2&gt;&lt;h3 id=&quot;如何转换&quot;&gt;&lt;a href=&quot;#如何转换&quot; class=&quot;head
    
    </summary>
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Batch Normalization" scheme="https://blog.patrickcty.cc/tags/Batch-Normalization/"/>
    
      <category term="Global Average Pooling" scheme="https://blog.patrickcty.cc/tags/Global-Average-Pooling/"/>
    
      <category term="Deep Learning" scheme="https://blog.patrickcty.cc/tags/Deep-Learning/"/>
    
      <category term="CNN" scheme="https://blog.patrickcty.cc/tags/CNN/"/>
    
  </entry>
  
</feed>
