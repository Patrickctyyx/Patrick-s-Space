<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Patrick&#39;s Space</title>
  
  <subtitle>Stay hungry, stay foolish!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.patrickcty.cc/"/>
  <updated>2021-11-20T10:57:40.371Z</updated>
  <id>https://blog.patrickcty.cc/</id>
  
  <author>
    <name>patrickcty</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>记录 PyTorch 中 masked_fill 的一个坑</title>
    <link href="https://blog.patrickcty.cc/2021/11/20/%E8%AE%B0%E5%BD%95PyTorch%E4%B8%ADmask-fill%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91/"/>
    <id>https://blog.patrickcty.cc/2021/11/20/记录PyTorch中mask-fill的一个坑/</id>
    <published>2021-11-20T02:55:26.000Z</published>
    <updated>2021-11-20T10:57:40.371Z</updated>
    
    <content type="html"><![CDATA[<p>这个函数一般被用于将 pad 的部分填充 0，即 <code>value.masked_fill(mask[..., None], float(0))</code>。从<a href="https://pytorch.org/docs/master/generated/torch.Tensor.masked_fill_.html#torch-tensor-masked-fill" target="_blank" rel="noopener">文档</a>中来看，mask 的 dtype 应该为 bool（int 也行，只能为 0 或 1），为 True 的部分就会就会被填充值替换掉。但是如果 mask dtype 为 float 则不会被 masked 掉。</p><p>在使用的时候，记得将 pad 部分的 mask 设为 True，其他设为 False，不然被 masked 掉的就是有用的部分了……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个函数一般被用于将 pad 的部分填充 0，即 &lt;code&gt;value.masked_fill(mask[..., None], float(0))&lt;/code&gt;。从&lt;a href=&quot;https://pytorch.org/docs/master/generated/to
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
    
      <category term="PyTorch" scheme="https://blog.patrickcty.cc/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyCharm 调试卡在 collecting data</title>
    <link href="https://blog.patrickcty.cc/2021/11/03/pycharm%E8%B0%83%E8%AF%95%E5%8D%A1%E5%9C%A8collect-data/"/>
    <id>https://blog.patrickcty.cc/2021/11/03/pycharm调试卡在collect-data/</id>
    <published>2021-11-03T06:17:50.000Z</published>
    <updated>2021-11-03T06:25:07.284Z</updated>
    
    <content type="html"><![CDATA[<p>在使用 PyCharm Debug PyTorch 程序的时候，经常会卡在 collecting data。根据 <a href="https://stackoverflow.com/questions/39371676/debugger-times-out-at-collecting-data" target="_blank" rel="noopener">stackoverflow</a> 可以知道原因在于默认设置下对多个 worker 处理的不太好。</p><h2 id="解决方法一"><a href="#解决方法一" class="headerlink" title="解决方法一"></a>解决方法一</h2><p>在 Preferences | Build, Execution, Deployment | Python Debugger 中勾选 Gevent compatible</p><p>Gevent 是 Python 的一个协程库，而 PyTorch 代码中设置 dataloader num_workers 大于一，因此这样改能解决问题</p><p>Gevent 相关内容参考 <a href="https://www.jianshu.com/p/73ccb425a710" target="_blank" rel="noopener">https://www.jianshu.com/p/73ccb425a710</a></p><h2 id="解决方法二"><a href="#解决方法二" class="headerlink" title="解决方法二"></a>解决方法二</h2><p>将 num_workers 设置为 0 或 1</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在使用 PyCharm Debug PyTorch 程序的时候，经常会卡在 collecting data。根据 &lt;a href=&quot;https://stackoverflow.com/questions/39371676/debugger-times-out-at-coll
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
    
      <category term="PyTorch" scheme="https://blog.patrickcty.cc/tags/PyTorch/"/>
    
      <category term="PyCharm" scheme="https://blog.patrickcty.cc/tags/PyCharm/"/>
    
  </entry>
  
  <entry>
    <title>pandas读取数据错误</title>
    <link href="https://blog.patrickcty.cc/2021/10/23/pandas%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%94%99%E8%AF%AF/"/>
    <id>https://blog.patrickcty.cc/2021/10/23/pandas读取数据错误/</id>
    <published>2021-10-23T08:30:54.000Z</published>
    <updated>2021-10-23T08:36:23.029Z</updated>
    
    <content type="html"><![CDATA[<p>在读取 json 文件的时候报错 <code>TypeError: Cannot interpret &#39;&lt;attribute &#39;dtype&#39; of &#39;numpy.generic&#39; objects&gt;&#39; as a data type</code></p><p>出现原因：numpy 的版本和 pandas 版本不兼容，见 <a href="https://github.com/numpy/numpy/issues/18355" target="_blank" rel="noopener">github issue</a></p><p>解决方法：更新 pandas</p><pre><code class="hljs shell">pip install -U pandas</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在读取 json 文件的时候报错 &lt;code&gt;TypeError: Cannot interpret &amp;#39;&amp;lt;attribute &amp;#39;dtype&amp;#39; of &amp;#39;numpy.generic&amp;#39; objects&amp;gt;&amp;#39; as a da
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
    
  </entry>
  
  <entry>
    <title>ubuntu18.04安装openssh-server失败</title>
    <link href="https://blog.patrickcty.cc/2021/10/22/ubuntu18-04%E5%AE%89%E8%A3%85openssh-server%E5%A4%B1%E8%B4%A5/"/>
    <id>https://blog.patrickcty.cc/2021/10/22/ubuntu18-04安装openssh-server失败/</id>
    <published>2021-10-22T06:14:17.000Z</published>
    <updated>2021-10-22T06:18:52.715Z</updated>
    
    <content type="html"><![CDATA[<p>在给实验室主机配置内网穿透时，其他配置项都没问题，但是出现 <code>kex_exchange_identification: read: Connection reset by peer</code>，最后排查发现可能是 openssh-server 没装，但是在安装的时候报依赖错误。</p><pre><code class="hljs undefined">下列软件包有未满足的依赖关系：     openssh-server :         依赖: openssh-client (= 1:7.6p1-4)         依赖: openssh-sftp-server 但是它将不会被安装         推荐: ssh-import-id 但是它将不会被安装 E: 无法修正错误，因为您要求某些软件包保持现状，就是它们破坏了软件包间的依赖关系。</code></pre><p>参阅<a href="https://blog.csdn.net/qq_44641580/article/details/104730742" target="_blank" rel="noopener">博客</a>之后发现是 openssh-client 版本不对，安装上面给定的版本即可，注意等号两边不要有空格</p><pre><code class="hljs shell">sudo apt install openssh-client=1:7.6p1-4</code></pre><p>之后就能正常安装 openssh-server 了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在给实验室主机配置内网穿透时，其他配置项都没问题，但是出现 &lt;code&gt;kex_exchange_identification: read: Connection reset by peer&lt;/code&gt;，最后排查发现可能是 openssh-server 没装，但是在安装的
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
    
      <category term="Linux" scheme="https://blog.patrickcty.cc/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>hexo中代码高亮显示错误</title>
    <link href="https://blog.patrickcty.cc/2021/10/01/hexo%E4%B8%AD%E4%BB%A3%E7%A0%81%E9%AB%98%E4%BA%AE%E6%98%BE%E7%A4%BA%E9%94%99%E8%AF%AF/"/>
    <id>https://blog.patrickcty.cc/2021/10/01/hexo中代码高亮显示错误/</id>
    <published>2021-10-01T13:38:38.000Z</published>
    <updated>2021-10-02T13:34:19.515Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>今天在 hexo 高亮的问题上卡了一下午 orz。起因是 Python 代码的高亮总是有异常，一大块代码都被识别成了注释，而且有装饰器的部分会多出莫名其妙的空行。</p><h2 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h2><p>使用开发者工作查看之后发现 Python 的代码块被识别成了其他语言的代码块，如 ruby。而 hexo 中默认使用的高亮工具是 highlight.js。因此首先怀疑是不是主题里面的一些配置异常。</p><p>观察 <code>themes/hexo-theme-material/layout/_partial/import_js.ejs</code> 发现只引入了 js 相关高亮的 js 代码，没有引入 Python 高亮的 js 代码。加入引用后重新生成，仍然显示异常。</p><p>观察其他主题的代码，只是引用了 highlight.js 包，并没有做额外的处理。怀疑是 hexo 自身的问题。由于 hexo 版本比较老，下载了一个新主题由于版本问题没运行成功，因此没有用其他主题来进行佐证。</p><p>再次在开发者工具中观察代码块，发现一块代码包含多个层，其中外围解析正确为 Python，中层解析异常，最内层又解析正常。根据 <a href="https://hexo.io/zh-cn/docs/syntax-highlight.html#wrap" target="_blank" rel="noopener">hexo 文档</a>，由于 highlight.js 原生不支持行号，hexo 将输出包裹在了 <code>&lt;figure&gt;</code> 和 <code>&lt;table&gt;</code> 内部。可能正是这个过程中有 bug 导致最终显示的时候没有解析到正确的语言。</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>在 _config.yml 中设置 <code>highlight.line_number: false</code> 即可。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>hexo 的主题通过定义多个 ejs 文件来动态生成博客的内容，如果想自定义的话可以修改对应的 ejs 文件。如果在主题中没有找到相应的问题那么可能就要从 hexo 本身来找问题了。</p><p>可惜我的前端功夫还不到家，还不能做到对前端内容进行更新 orz。好久没折腾这些了，感觉整个人都变菜了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h2&gt;&lt;p&gt;今天在 hexo 高亮的问题上卡了一下午 orz。起因是 Python 代码的高亮总是有异常，一大块代码都被识别成了注释，而且有装饰器的部分
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
    
      <category term="hexo" scheme="https://blog.patrickcty.cc/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>pip安装路径修改</title>
    <link href="https://blog.patrickcty.cc/2021/09/22/pip%E5%AE%89%E8%A3%85%E8%B7%AF%E5%BE%84%E4%BF%AE%E6%94%B9/"/>
    <id>https://blog.patrickcty.cc/2021/09/22/pip安装路径修改/</id>
    <published>2021-09-22T03:30:53.000Z</published>
    <updated>2021-10-02T13:37:12.538Z</updated>
    
    <content type="html"><![CDATA[<h2 id="出现问题"><a href="#出现问题" class="headerlink" title="出现问题"></a>出现问题</h2><p>在跑 PVT 的时候需要安装 mmdet，但是每次都默认安装到 .local 目录下而不是虚拟环境中，导致会和其他环境冲突。查看 sys.path 发现 .local 在虚拟环境之前，导致 pip 默认安装路径不符合预期。</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>根据<a href="https://blog.csdn.net/mukvintt/article/details/80908951" target="_blank" rel="noopener">这篇博客</a>，pip 安装包默认路径是通过 site.py 来确定的，输入 <code>python -m site</code>，得到以下内容：</p><pre><code class="hljs python">sys.path = [    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python37.zip'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7/lib-dynload'</span>,    <span class="hljs-string">'/home/sse/.local/lib/python3.7/site-packages'</span>,    <span class="hljs-string">'/home/sse/.local/lib/python3.7/site-packages/apex-0.1-py3.7-linux-x86_64.egg'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7/site-packages'</span>,]USER_BASE: <span class="hljs-string">'/home/sse/.local'</span> (exists)USER_SITE: <span class="hljs-string">'/home/sse/.local/lib/python3.7/site-packages'</span> (exists)ENABLE_USER_SITE: <span class="hljs-literal">True</span></code></pre><p>可以看出来，当直接 <code>pip install mmdet</code> 的时候，是会安装到 sys.path 第一个目录下的，但是由于当时我使用了 <code>pip install mmdet --user</code>，因此会安装到 USER_SITE 下，也就是 .local。如果需要更改 USER_SITE 和 USER_BASE 配置的话则需要去修改 site.py 文件。</p><p>运行 <code>python -m site -help</code>，可以得到 site.py 文件目录，打开之后发现这二者配置如下：</p><pre><code class="hljs python">USER_SITE = <span class="hljs-literal">None</span>USER_BASE = <span class="hljs-literal">None</span></code></pre><p>继续往下之后可以发现</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_getuserbase</span><span class="hljs-params">()</span>:</span>    env_base = os.environ.get(<span class="hljs-string">"PYTHONUSERBASE"</span>, <span class="hljs-literal">None</span>)    <span class="hljs-keyword">if</span> env_base:        <span class="hljs-keyword">return</span> env_base    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">joinuser</span><span class="hljs-params">(*args)</span>:</span>        <span class="hljs-keyword">return</span> os.path.expanduser(os.path.join(*args))    <span class="hljs-keyword">if</span> os.name == <span class="hljs-string">"nt"</span>:        base = os.environ.get(<span class="hljs-string">"APPDATA"</span>) <span class="hljs-keyword">or</span> <span class="hljs-string">"~"</span>        <span class="hljs-keyword">return</span> joinuser(base, <span class="hljs-string">"Python"</span>)    <span class="hljs-keyword">if</span> sys.platform == <span class="hljs-string">"darwin"</span> <span class="hljs-keyword">and</span> sys._framework:        <span class="hljs-keyword">return</span> joinuser(<span class="hljs-string">"~"</span>, <span class="hljs-string">"Library"</span>, sys._framework,                        <span class="hljs-string">"%d.%d"</span> % sys.version_info[:<span class="hljs-number">2</span>])    <span class="hljs-keyword">return</span> joinuser(<span class="hljs-string">"~"</span>, <span class="hljs-string">".local"</span>)</code></pre><p>由于目前所在系统 os.name == “posix”, sys.format == “linux”，因此默认的 base 为 .local，符合预期。如果要让使用了 –user 之后还能下载到虚拟环境下则需要手动添加环境，即修改那两个变量的值：</p><pre><code class="hljs undefined">USER_SITE = &apos;/home/sse/anaconda3/envs/pvt/lib/python3.7/site-packages&apos;USER_BASE = &apos;/home/sse/anaconda3/envs/pvt&apos;</code></pre><p>修改之后保存，再次运行 <code>python -m site</code>，得到结果如下：</p><pre><code class="hljs python">sys.path = [    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python37.zip'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7/lib-dynload'</span>,    <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7/site-packages'</span>,]USER_BASE: <span class="hljs-string">'/home/sse/anaconda3/envs/pvt'</span> (exists)USER_SITE: <span class="hljs-string">'/home/sse/anaconda3/envs/pvt/lib/python3.7/site-packages'</span> (exists)ENABLE_USER_SITE: <span class="hljs-literal">True</span></code></pre><p>此时结果符合预期，下载的时候也会下载到虚拟环境目录下</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用 pip install 的时候会从 sys.path 获取路径，越靠前优先度越高。</p><p>使用 –user 选项下载的时候则会从 site.py 中获取 USER_SITE，如果没有则会根须系统生成一个，然后再安装到这个目录下。</p><p>使用虚拟环境的时候没必要使用 –user 选项，因为默认的安装路径就不是系统的路径，不会造成兼容性问题，反而使用了这个选项之后会造成问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;出现问题&quot;&gt;&lt;a href=&quot;#出现问题&quot; class=&quot;headerlink&quot; title=&quot;出现问题&quot;&gt;&lt;/a&gt;出现问题&lt;/h2&gt;&lt;p&gt;在跑 PVT 的时候需要安装 mmdet，但是每次都默认安装到 .local 目录下而不是虚拟环境中，导致会和其他环境冲突。
      
    
    </summary>
    
      <category term="Python" scheme="https://blog.patrickcty.cc/categories/Python/"/>
    
    
      <category term="Python" scheme="https://blog.patrickcty.cc/tags/Python/"/>
    
      <category term="pip" scheme="https://blog.patrickcty.cc/tags/pip/"/>
    
  </entry>
  
  <entry>
    <title>mmdet问题整理</title>
    <link href="https://blog.patrickcty.cc/2021/07/10/mmdet%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86/"/>
    <id>https://blog.patrickcty.cc/2021/07/10/mmdet问题整理/</id>
    <published>2021-07-10T04:54:04.000Z</published>
    <updated>2021-10-02T13:36:42.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tmp-文件无法访问"><a href="#tmp-文件无法访问" class="headerlink" title="/tmp 文件无法访问"></a>/tmp 文件无法访问</h2><p>多半是配置中语法出现问题</p><h2 id="跑-Deformable-DETR-报错"><a href="#跑-Deformable-DETR-报错" class="headerlink" title="跑 Deformable DETR 报错"></a>跑 Deformable DETR 报错</h2><blockquote><p>UnboundLocalErrorUnboundLocalError: : local variable ‘beta1’ referenced before assignment</p></blockquote><p>原因：PyTorch 1.8 有 bug，按照这个 <a href="https://github.com/pytorch/pytorch/pull/52944/commits/731a5245642480fcdeba8b40a34c117f9074c9c6" target="_blank" rel="noopener">PR</a> 来修改你用的优化器即可。</p><h2 id="创建-symlink-错误的"><a href="#创建-symlink-错误的" class="headerlink" title="创建 symlink 错误的"></a>创建 symlink 错误的</h2><p><a href="https://blog.patrickcty.cc/2020/10/10/exFAT%E4%B8%8D%E6%94%AF%E6%8C%81%E8%BD%AF%E9%93%BE%E6%8E%A5%E4%B8%8E%E7%A1%AC%E9%93%BE%E6%8E%A5/">exFAT 不支持 symlink</a>，需要手动在报错的地方修改代码。</p><h2 id="DETR-改小-num-query-测试时报错"><a href="#DETR-改小-num-query-测试时报错" class="headerlink" title="DETR 改小 num_query 测试时报错"></a>DETR 改小 num_query 测试时报错</h2><p>当 num_query 改小的时候 max_per_img 就会大于 num_query</p><pre><code class="hljs python">max_per_img = self.test_cfg.get(<span class="hljs-string">'max_per_img'</span>, self.num_query)</code></pre><p>从而导致 score.shape &lt; max_per_img，score.shape 即为 num_query</p><pre><code class="hljs python">scores, bbox_index = scores.topk(max_per_img)</code></pre><p>从而导致<a href="https://blog.csdn.net/songyu0120/article/details/106309618" target="_blank" rel="noopener">报错</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tmp-文件无法访问&quot;&gt;&lt;a href=&quot;#tmp-文件无法访问&quot; class=&quot;headerlink&quot; title=&quot;/tmp 文件无法访问&quot;&gt;&lt;/a&gt;/tmp 文件无法访问&lt;/h2&gt;&lt;p&gt;多半是配置中语法出现问题&lt;/p&gt;
&lt;h2 id=&quot;跑-Deformabl
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="mmdetection" scheme="https://blog.patrickcty.cc/tags/mmdetection/"/>
    
      <category term="目标检测" scheme="https://blog.patrickcty.cc/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>swin-object-detection遇到的问题</title>
    <link href="https://blog.patrickcty.cc/2021/06/25/swin-object-detection%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://blog.patrickcty.cc/2021/06/25/swin-object-detection遇到的问题/</id>
    <published>2021-06-25T15:24:13.000Z</published>
    <updated>2021-10-02T13:38:51.277Z</updated>
    
    <content type="html"><![CDATA[<p>在配置 <a href="https://github.com/SwinTransformer/Swin-Transformer-Object-Detection" target="_blank" rel="noopener">Swin-Transformer-Object-Detection</a> 的过程中出现了 pycocotools 不兼容额问题，但是另一个类似的环境在安装 mmdetection 的时候并没有问题。（pycocotools 版本相同）</p><p>于是按照 mmdetection 的结构对 <code>mmdet/datasets/coco.py</code> 进行修改。</p><p>修改 <code>COCO</code> 和 <code>COCOEval</code> 的引用：</p><pre><code class="hljs Python"><span class="hljs-keyword">from</span> .api_wrappers <span class="hljs-keyword">import</span> COCO, COCOeval</code></pre><p>去掉 <code>get_img_ids</code> 的参数</p><pre><code class="hljs Python">self.img_ids = self.coco.get_img_ids()</code></pre><p>新建 <code>mmdet/datasets/api_wrappers/coco_api.py</code>，将以下内容粘贴进去</p><pre><code class="hljs Python"><span class="hljs-comment"># This file add snake case alias for coco api</span><span class="hljs-keyword">import</span> warnings<span class="hljs-keyword">import</span> pycocotools<span class="hljs-keyword">from</span> pycocotools.coco <span class="hljs-keyword">import</span> COCO <span class="hljs-keyword">as</span> _COCO<span class="hljs-keyword">from</span> pycocotools.cocoeval <span class="hljs-keyword">import</span> COCOeval <span class="hljs-keyword">as</span> _COCOeval<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">COCO</span><span class="hljs-params">(_COCO)</span>:</span>    <span class="hljs-string">"""This class is almost the same as official pycocotools package.    It implements some snake case function aliases. So that the COCO class has    the same interface as LVIS class.    """</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, annotation_file=None)</span>:</span>        <span class="hljs-keyword">if</span> getattr(pycocotools, <span class="hljs-string">'__version__'</span>, <span class="hljs-string">'0'</span>) &gt;= <span class="hljs-string">'12.0.2'</span>:            warnings.warn(                <span class="hljs-string">'mmpycocotools is deprecated. Please install official pycocotools by "pip install pycocotools"'</span>,  <span class="hljs-comment"># noqa: E501</span>                UserWarning)        super().__init__(annotation_file=annotation_file)        self.img_ann_map = self.imgToAnns        self.cat_img_map = self.catToImgs    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_ann_ids</span><span class="hljs-params">(self, img_ids=[], cat_ids=[], area_rng=[], iscrowd=None)</span>:</span>        <span class="hljs-keyword">return</span> self.getAnnIds(img_ids, cat_ids, area_rng, iscrowd)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_cat_ids</span><span class="hljs-params">(self, cat_names=[], sup_names=[], cat_ids=[])</span>:</span>        <span class="hljs-keyword">return</span> self.getCatIds(cat_names, sup_names, cat_ids)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_img_ids</span><span class="hljs-params">(self, img_ids=[], cat_ids=[])</span>:</span>        <span class="hljs-keyword">return</span> self.getImgIds(img_ids, cat_ids)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_anns</span><span class="hljs-params">(self, ids)</span>:</span>        <span class="hljs-keyword">return</span> self.loadAnns(ids)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_cats</span><span class="hljs-params">(self, ids)</span>:</span>        <span class="hljs-keyword">return</span> self.loadCats(ids)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_imgs</span><span class="hljs-params">(self, ids)</span>:</span>        <span class="hljs-keyword">return</span> self.loadImgs(ids)<span class="hljs-comment"># just for the ease of import</span>COCOeval = _COCOeval</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在配置 &lt;a href=&quot;https://github.com/SwinTransformer/Swin-Transformer-Object-Detection&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Swin-Transformer-Object
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="目标检测" scheme="https://blog.patrickcty.cc/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Transformer" scheme="https://blog.patrickcty.cc/tags/Transformer/"/>
    
      <category term="COCO" scheme="https://blog.patrickcty.cc/tags/COCO/"/>
    
  </entry>
  
  <entry>
    <title>mmdet常用配置</title>
    <link href="https://blog.patrickcty.cc/2021/06/19/mmdet%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"/>
    <id>https://blog.patrickcty.cc/2021/06/19/mmdet常用配置/</id>
    <published>2021-06-19T14:37:06.000Z</published>
    <updated>2021-10-02T13:36:27.921Z</updated>
    
    <content type="html"><![CDATA[<pre><code class="hljs Python"><span class="hljs-comment"># 总的 epoch 数</span>total_epochs = <span class="hljs-number">200</span>runner = dict(type=<span class="hljs-string">'EpochBasedRunner'</span>, max_epochs=<span class="hljs-number">200</span>)<span class="hljs-comment"># 设置 batchsize</span>data = dict(samples_per_gpu=<span class="hljs-number">2</span>, workers_per_gpu=<span class="hljs-number">2</span>)<span class="hljs-comment"># 验证的间隔，太短的话可能增多训练时间</span>evaluation = dict(metric=[<span class="hljs-string">'bbox'</span>, <span class="hljs-string">'segm'</span>], interval=<span class="hljs-number">10</span>)<span class="hljs-comment"># 输出参数的文件夹，默认是在项目目录下的</span>work_dir = <span class="hljs-string">''</span><span class="hljs-comment"># 保存 checkpoint 的间隔</span>checkpoint_config = dict(interval=<span class="hljs-number">10</span>)<span class="hljs-comment"># 加载检查点</span>load_from = <span class="hljs-string">'xxxx.pth'</span><span class="hljs-comment"># 从中断的地方继续训练</span>resume_from = <span class="hljs-string">'xxxx.pth'</span></code></pre><p>根据 step 来训练</p><pre><code class="hljs Python">lr_config = dict(policy=<span class="hljs-string">'poly'</span>, power=<span class="hljs-number">0.9</span>, min_lr=<span class="hljs-number">1e-4</span>, by_epoch=<span class="hljs-literal">False</span>)total_iters = <span class="hljs-number">160000</span>checkpoint_config = dict(by_epoch=<span class="hljs-literal">False</span>, interval=<span class="hljs-number">16000</span>)evaluation = dict(interval=<span class="hljs-number">16000</span>, metric=<span class="hljs-string">'mIoU'</span>)</code></pre><p>P.S. 如果某个配置中有 <code>_delete_=True</code>，那么表明用当前配置覆盖掉继承的配置，不加的话仅仅只覆盖掉当前指定的配置。比如当前指定了 <code>evaluation = dict(interval=16000, metric=&#39;mIoU&#39;)</code>，而继承的配置中传入了三个参数，那另一个参数还是会使用原本的配置，但是加了 <code>_delete_=True</code> 另一个参数就不会使用原本的配置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code class=&quot;hljs Python&quot;&gt;&lt;span class=&quot;hljs-comment&quot;&gt;# 总的 epoch 数&lt;/span&gt;
total_epochs = &lt;span class=&quot;hljs-number&quot;&gt;200&lt;/span&gt;
runner = d
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="mmdetection" scheme="https://blog.patrickcty.cc/tags/mmdetection/"/>
    
      <category term="目标检测" scheme="https://blog.patrickcty.cc/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>mmsegmentation二分类配置</title>
    <link href="https://blog.patrickcty.cc/2021/05/21/mmsegmentation%E4%BA%8C%E5%88%86%E7%B1%BB%E9%85%8D%E7%BD%AE/"/>
    <id>https://blog.patrickcty.cc/2021/05/21/mmsegmentation二分类配置/</id>
    <published>2021-05-21T09:26:51.000Z</published>
    <updated>2021-10-02T13:36:49.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><p>configs 目录下有很多已有的配置，在自己乱试之前先看看！比如医学图像分割也是二分类任务，并且其最有名的模型是 U-Net，因此今天的配置都可以直接参考相关的配置。</p><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><p>由于 mmsegmentation 的版本更新得比较快，因此基于此开发的项目环境一般不太兼容，最好每个都重新搞一个虚拟环境，命令如下：</p><pre><code class="hljs shell">conda create -n open-mmlab python=3.7 -yconda activate open-mmlabconda install pytorch=1.6.0 torchvision cudatoolkit=10.1 -c pytorch -ypip install mmcv-full==1.2.2 -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.htmlgit clone https://github.com/fudan-zvg/SETR.git  # 某个基于 mmsegmentation 开发的项目cd SETRpip install -e .  # or "python setup.py develop"pip install -r requirements/optional.txt<span class="hljs-meta">#</span> 可选，这里是为了切换多个 CUDA 版本conda env config vars set PATH=/home/sse/anaconda3/envs/open-mmlab/bin:/usr/local/cuda-10.1/bin:$PATH LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64:$LD_LIBRARY_PATH -n open-mmlab</code></pre><h2 id="纲要"><a href="#纲要" class="headerlink" title="纲要"></a>纲要</h2><ul><li>自定义数据集<ul><li><code>configs/_base_/datasets/&lt;your_dataset&gt;.py</code></li><li><code>mmseg/datasets/&lt;your_dataset&gt;.py</code></li><li><code>mmseg/datasets/__init__.py</code></li></ul></li><li>自定义训练</li></ul><h3 id="自定义数据集"><a href="#自定义数据集" class="headerlink" title="自定义数据集"></a>自定义数据集</h3><p>MM 系列框架都经过了非常高的封装，我们只用修改配置文件就可以非常容易地训练不同的模型。其本质上就是通过各个模块的注册来实现的。比如说在 <code>mmseg/datasets</code> 目录下有一个 <code>builder.py</code> 文件，这个文件中有 <code>DATASETS</code> 和 <code>PIPELINE</code> 两个全局变量，前者用来注册数据集，后者用来注册数据加载处理的一些操作。比如打开 <code>ade.py</code> 这个数据集定义文件，我们可以看到在定义类的时候有一个装饰器 <a href="mailto:`@DATASETS.registe" target="_blank" rel="noopener">`@DATASETS.registe</a>r_module()<code>自动注册了这个数据集。因此如果要自定义数据集甚至是网络的模块，只用在相关位置定义然后注册即可。注册的话还有另一个步骤，那就是将其添加到</code>mmseg/datasets/<strong>init</strong>.py` 之中。</p><h4 id="创建数据集类"><a href="#创建数据集类" class="headerlink" title="创建数据集类"></a>创建数据集类</h4><p>先别慌着写，先把已有的数据集类都扫一遍。扫一遍之后我们可以发现要自定义一个数据集类，首先需要继承 <code>CustomDataset</code> 类，然后再在其中自定义 <code>CLASSES</code> 类变量与 <code>PALETTE</code> 类变量，其中前者是为了将类别标号与自然语言对应起来，后者是可视化的时候对应不同的颜色。再就只用根据数据集的特点来自定义构造方法了。我们先看看 <code>CustomDataset</code> 构造方法的参数，在 <code>mmseg/datasets/custom.py</code> 的 47 行，由于其中大部分都通过配置文件来指定，因此我们只集中于几个重要的参数：</p><pre><code class="hljs undefined">Args:    img_suffix (str): 图像的后缀，只读取特定后缀的图像，对于混合搭建的数据集非常友好，默认值：&apos;.jpg&apos;    seg_map_suffix (str): GT 的后缀，只读取特定后缀的 GT，默认值：&apos;.png&apos;    ignore_index (int): 忽视掉的标签，默认值：255。比如我们可以把 GT padding 设为 255，这样这部分就不会被网络处理    reduce_zero_label (bool): 忽视为 0 的标签，在实现中是将 0 变成 255，然后将其他所有标签值减一，默认值：False</code></pre><h4 id="添加索引"><a href="#添加索引" class="headerlink" title="添加索引"></a>添加索引</h4><p>然后观察完之后得知 <code>drive.py</code>，<code>hrf.py</code>，<code>stare.py</code> 这几个数据集是二分类数据集，因此数据的组织可以参考这几个。新建一个 <code>mmseg/datasets/saliency.py</code></p><pre><code class="hljs Python"><span class="hljs-comment"># mmseg/datasets/saliency.py</span><span class="hljs-keyword">import</span> os.path <span class="hljs-keyword">as</span> osp<span class="hljs-keyword">from</span> .builder <span class="hljs-keyword">import</span> DATASETS<span class="hljs-keyword">from</span> .custom <span class="hljs-keyword">import</span> CustomDataset<span class="hljs-meta">@DATASETS.register_module()</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SaliencyDataset</span><span class="hljs-params">(CustomDataset)</span>:</span>    <span class="hljs-string">"""Saliency dataset.    In segmentation map annotation for Saliency, 0 stands for background, which is    included in 2 categories. ``reduce_zero_label`` is fixed to False.     """</span>    CLASSES = (<span class="hljs-string">'background'</span>, <span class="hljs-string">'saliency'</span>)    PALETTE = [[<span class="hljs-number">120</span>, <span class="hljs-number">120</span>, <span class="hljs-number">120</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">230</span>, <span class="hljs-number">230</span>]]    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>        super(SaliencyDataset, self).__init__(            reduce_zero_label=<span class="hljs-literal">False</span>,            **kwargs)        <span class="hljs-keyword">assert</span> osp.exists(self.img_dir)</code></pre><p>然后将定义好的类添加到 <code>mmseg/datasets/__init__.py</code> 中</p><pre><code class="hljs Python"><span class="hljs-keyword">from</span> .saliency.py <span class="hljs-keyword">import</span> SaliencyDataset__all__ = [    ..., <span class="hljs-string">'SaliencyDataset'</span>]</code></pre><h4 id="添加数据集配置"><a href="#添加数据集配置" class="headerlink" title="添加数据集配置"></a>添加数据集配置</h4><p>添加 <code>configs/_base_/datasets/cod10k.py</code>，内容依旧对着上面提到的二分类数据集改即可</p><pre><code class="hljs Python"><span class="hljs-comment"># dataset settings</span>dataset_type = <span class="hljs-string">'SaliencyDataset'</span>data_root = <span class="hljs-string">'data/Saliency'</span>  <span class="hljs-comment"># 修改成你数据集所在路径</span>img_norm_cfg = dict(    mean=[<span class="hljs-number">123.675</span>, <span class="hljs-number">116.28</span>, <span class="hljs-number">103.53</span>], std=[<span class="hljs-number">58.395</span>, <span class="hljs-number">57.12</span>, <span class="hljs-number">57.375</span>], to_rgb=<span class="hljs-literal">True</span>)img_scale = (<span class="hljs-number">800</span>, <span class="hljs-number">800</span>)  <span class="hljs-comment"># 修改成你数据集合适的大小</span>crop_size = (<span class="hljs-number">384</span>, <span class="hljs-number">384</span>)  <span class="hljs-comment"># 修改成合适的大小</span>train_pipeline = [    dict(type=<span class="hljs-string">'LoadImageFromFile'</span>),    dict(type=<span class="hljs-string">'LoadAnnotations'</span>),  <span class="hljs-comment"># 这里不要 reduce_zero_label!!!</span>    dict(type=<span class="hljs-string">'Resize'</span>, img_scale=img_scale, ratio_range=(<span class="hljs-number">0.5</span>, <span class="hljs-number">2.0</span>)),    dict(type=<span class="hljs-string">'RandomCrop'</span>, crop_size=crop_size, cat_max_ratio=<span class="hljs-number">0.75</span>),    dict(type=<span class="hljs-string">'RandomFlip'</span>, prob=<span class="hljs-number">0.5</span>),    dict(type=<span class="hljs-string">'PhotoMetricDistortion'</span>),    dict(type=<span class="hljs-string">'Normalize'</span>, **img_norm_cfg),    dict(type=<span class="hljs-string">'Pad'</span>, size=crop_size, pad_val=<span class="hljs-number">0</span>, seg_pad_val=<span class="hljs-number">255</span>),    dict(type=<span class="hljs-string">'DefaultFormatBundle'</span>),    dict(type=<span class="hljs-string">'Collect'</span>, keys=[<span class="hljs-string">'img'</span>, <span class="hljs-string">'gt_semantic_seg'</span>])]test_pipeline = [    dict(type=<span class="hljs-string">'LoadImageFromFile'</span>),    dict(        type=<span class="hljs-string">'MultiScaleFlipAug'</span>,        img_scale=img_scale,        <span class="hljs-comment"># img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0],</span>        flip=<span class="hljs-literal">False</span>,        transforms=[            dict(type=<span class="hljs-string">'Resize'</span>, keep_ratio=<span class="hljs-literal">True</span>),            dict(type=<span class="hljs-string">'RandomFlip'</span>),            dict(type=<span class="hljs-string">'Normalize'</span>, **img_norm_cfg),            dict(type=<span class="hljs-string">'ImageToTensor'</span>, keys=[<span class="hljs-string">'img'</span>]),            dict(type=<span class="hljs-string">'Collect'</span>, keys=[<span class="hljs-string">'img'</span>])        ])]data = dict(    samples_per_gpu=<span class="hljs-number">4</span>,    workers_per_gpu=<span class="hljs-number">4</span>,    train=dict(        type=dataset_type,        data_root=data_root,        img_dir=<span class="hljs-string">'images/training'</span>,  <span class="hljs-comment"># 修改数据集路径，下同</span>        ann_dir=<span class="hljs-string">'annotations/training'</span>,        pipeline=train_pipeline),    val=dict(        type=dataset_type,        data_root=data_root,        img_dir=<span class="hljs-string">'images/validation'</span>,        ann_dir=<span class="hljs-string">'annotations/validation'</span>,        pipeline=test_pipeline),    test=dict(        type=dataset_type,        data_root=data_root,        img_dir=<span class="hljs-string">'images/validation'</span>,        ann_dir=<span class="hljs-string">'annotations/validation'</span>,        pipeline=test_pipeline))</code></pre><h3 id="自定义训练配置"><a href="#自定义训练配置" class="headerlink" title="自定义训练配置"></a>自定义训练配置</h3><p>新建一个 <code>configs/swin/upernet_swin_tiny_patch4_window7_384x384_40k_cod.py</code></p><pre><code class="hljs Python">_base_ = [  <span class="hljs-comment"># 注意修改路径</span>    <span class="hljs-string">'../_base_/models/upernet_swin.py'</span>, <span class="hljs-string">'../_base_/datasets/cod10k.py'</span>,    <span class="hljs-string">'../_base_/default_runtime.py'</span>, <span class="hljs-string">'../_base_/schedules/schedule_40k.py'</span>]model = dict(    backbone=dict(        embed_dim=<span class="hljs-number">96</span>,        depths=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>],        num_heads=[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">24</span>],        window_size=<span class="hljs-number">7</span>,        ape=<span class="hljs-literal">False</span>,        drop_path_rate=<span class="hljs-number">0.3</span>,        patch_norm=<span class="hljs-literal">True</span>,        use_checkpoint=<span class="hljs-literal">False</span>    ),    decode_head=dict(        in_channels=[<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>],        num_classes=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 注意这里是两类</span>        <span class="hljs-comment"># 由于正负样本非常不平衡，因此这里给正样本加了权重</span>        loss_decode=dict(class_weight=(<span class="hljs-number">1</span> ,<span class="hljs-number">8</span>))    ),    auxiliary_head=dict(        in_channels=<span class="hljs-number">384</span>,        num_classes=<span class="hljs-number">2</span>,  <span class="hljs-comment"># 注意这里是两类</span>        loss_decode=dict(class_weight=(<span class="hljs-number">1</span> ,<span class="hljs-number">8</span>))    ))<span class="hljs-comment"># AdamW optimizer, no weight decay for position embedding &amp; layer norm in backbone</span>optimizer = dict(_delete_=<span class="hljs-literal">True</span>, type=<span class="hljs-string">'AdamW'</span>, lr=<span class="hljs-number">0.00006</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), weight_decay=<span class="hljs-number">0.01</span>,                 paramwise_cfg=dict(custom_keys=&#123;<span class="hljs-string">'absolute_pos_embed'</span>: dict(decay_mult=<span class="hljs-number">0.</span>),                                                 <span class="hljs-string">'relative_position_bias_table'</span>: dict(decay_mult=<span class="hljs-number">0.</span>),                                                 <span class="hljs-string">'norm'</span>: dict(decay_mult=<span class="hljs-number">0.</span>)&#125;))lr_config = dict(_delete_=<span class="hljs-literal">True</span>, policy=<span class="hljs-string">'poly'</span>,                 warmup=<span class="hljs-string">'linear'</span>,                 warmup_iters=<span class="hljs-number">1500</span>,                 warmup_ratio=<span class="hljs-number">1e-6</span>,                 power=<span class="hljs-number">1.0</span>, min_lr=<span class="hljs-number">0.0</span>, by_epoch=<span class="hljs-literal">False</span>)<span class="hljs-comment"># By default, models are trained on 8 GPUs with 2 images per GPU</span><span class="hljs-comment"># 修改了 batchsize 之后按比例修改 lr，bs 越大 lr 应该越大</span>data = dict(samples_per_gpu=<span class="hljs-number">5</span>)</code></pre><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><pre><code class="hljs shell">tools/dist_train.sh configs/swin/upernet_swin_tiny_patch4_window7_384x384_40k_cod.py 2 --options model.pretrained=&lt;PRETRAIN_MODEL&gt;</code></pre><p>其中 2 指的是 GPU 的数量，具体的还是看原项目的 <a href="https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation" target="_blank" rel="noopener">README</a></p><h2 id="一些报错"><a href="#一些报错" class="headerlink" title="一些报错"></a>一些报错</h2><h3 id="CUDA-error-an-illegal-memory-access-was-encountered"><a href="#CUDA-error-an-illegal-memory-access-was-encountered" class="headerlink" title="CUDA error: an illegal memory access was encountered"></a>CUDA error: an illegal memory access was encountered</h3><p>==mmsegmentation 中 gt 的范围是 [0, num_classes - 1]==，如果 gt 的值超过了这个范围就会出这个问题。由于增强等原因，可能前几个 ep 训练正常，到某个地方突然就报错。</p><h3 id="KeyError-“‘XXX-is-not-in-the-YYY-registry’”"><a href="#KeyError-“‘XXX-is-not-in-the-YYY-registry’”" class="headerlink" title="KeyError: “‘XXX is not in the YYY registry’”"></a>KeyError: “‘XXX is not in the YYY registry’”</h3><p>这个错误非常常见，我们只需要在 <code>mmseg</code> 下的相应文件里面去找即可。比如如果 YYY 是 backbone，那么就去 <code>mmseg/models/backbones/__init__.py</code> 里面去找，如果里面没有 XXX，那么要么是打错了，要么是不支持这个 backbone，也有可能没安装好。</p><p>第三种可能就输入 <code>pip install -e .</code> 命令重新安装即可。</p><h3 id="Training-loss-is-always-0-000"><a href="#Training-loss-is-always-0-000" class="headerlink" title="Training loss is always 0.000"></a>Training loss is always 0.000</h3><p>很有可能是类配置错了或者数据集配置有问题，对着上面的排查一下就好</p><h3 id="RuntimeError-CUDA-error-device-side-assert-triggered"><a href="#RuntimeError-CUDA-error-device-side-assert-triggered" class="headerlink" title="RuntimeError: CUDA error: device-side assert triggered"></a>RuntimeError: CUDA error: device-side assert triggered</h3><p>检查一下是不是 <code>num_classes</code> 配置有问题</p><h3 id="RuntimeError-DataLoader-worker-pid-5657-is-killed-by-signal-Killed"><a href="#RuntimeError-DataLoader-worker-pid-5657-is-killed-by-signal-Killed" class="headerlink" title="RuntimeError: DataLoader worker (pid 5657) is killed by signal: Killed"></a>RuntimeError: DataLoader worker (pid 5657) is killed by signal: Killed</h3><p>检查一下是不是爆内存了，检查一下数据集是不是有问题</p><h3 id="跑-SETR-测试的时候出问题"><a href="#跑-SETR-测试的时候出问题" class="headerlink" title="跑 SETR 测试的时候出问题"></a>跑 SETR 测试的时候出问题</h3><p>vit 只支持固定大小的输入，正常的 test 流程出来的图片大小不一，会报错。</p><p>SETR 中在配置文件中设置 <code>test_cfg = dict(mode=&#39;slide&#39;, crop_size=crop_size, stride=(320, 320))</code>，不使用整个图像来预测，而是用固定大小的块来进行预测。</p><p>但是我们使用的数据集中有的图像较小，导致 crop 出来的还是大小不一，因此还需要在 <code>configs/_base_/datasets/cod10k.py</code> 里面修改 <code>test_pipeline</code>，具体来说就是在 Resize 之后再加上一个 Pad 操作：</p><pre><code class="hljs Python">transforms=[    dict(type=<span class="hljs-string">'Resize'</span>, ...),    dict(type=<span class="hljs-string">'Pad'</span>, size=crop_size, pad_val=<span class="hljs-number">0</span>, seg_pad_val=<span class="hljs-number">255</span>),    ...]</code></pre><h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><p>优先检查数据集是不是有问题，然后再检查配置文件是不是没对应上（尤其是类别数）。另外今天遇到一个问题是在 <code>configs/_base_/datasets/cod10k.py</code> 里面错误地设置了 <code>reduce_zero_lable=True</code>，导致训练出现了异常。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;教训&quot;&gt;&lt;a href=&quot;#教训&quot; class=&quot;headerlink&quot; title=&quot;教训&quot;&gt;&lt;/a&gt;教训&lt;/h2&gt;&lt;p&gt;configs 目录下有很多已有的配置，在自己乱试之前先看看！比如医学图像分割也是二分类任务，并且其最有名的模型是 U-Net，因此今天的配
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="mmsegmentation" scheme="https://blog.patrickcty.cc/tags/mmsegmentation/"/>
    
  </entry>
  
  <entry>
    <title>使用COCOAPI评测结果</title>
    <link href="https://blog.patrickcty.cc/2021/02/15/%E4%BD%BF%E7%94%A8COCOAPI%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C/"/>
    <id>https://blog.patrickcty.cc/2021/02/15/使用COCOAPI评测结果/</id>
    <published>2021-02-15T02:44:13.000Z</published>
    <updated>2021-10-02T13:22:45.141Z</updated>
    
    <content type="html"><![CDATA[<p>目标检测、实例分割以及关键点检测都可以使用 COCO API 来进行评测，主要方法有两种：</p><ol><li>按照 COCO 的格式保存预测的结果，然后评测</li><li>不保存预测结果，直接评测</li></ol><h2 id="保存预测结果"><a href="#保存预测结果" class="headerlink" title="保存预测结果"></a>保存预测结果</h2><p>这一种方法要简单一些，只用给定格式来保存即可，具体格式参考<a href="https://zhuanlan.zhihu.com/p/134236324" target="_blank" rel="noopener">这篇博客</a>，注意事项如下：</p><ol><li>Box 是按照 [x, y, w, h] 的格式来保存</li><li>Mask 是用 RLE 格式来保存，使用如下代码来转换</li></ol><pre><code class="hljs Python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> pycocotools.mask <span class="hljs-keyword">as</span> mask_utilmasks = masks &gt; <span class="hljs-number">0.5</span> <span class="hljs-comment"># 首先转化为二值 mask，这里假设 mask 通道为 1</span>rle = mask_util.encode(np.array(segmap_masked[:, :, np.newaxis], dtype=np.uint8, order=<span class="hljs-string">"F"</span>))[<span class="hljs-number">0</span>]rle[<span class="hljs-string">'counts'</span>] = rle[<span class="hljs-string">"counts"</span>].decode(<span class="hljs-string">"utf-8"</span>)dataset_results.append(&#123;<span class="hljs-string">'image_id'</span>: all_imgs[i][<span class="hljs-string">'id'</span>], <span class="hljs-string">'category_id'</span>: <span class="hljs-number">1</span>,                        <span class="hljs-string">'segmentation'</span>: rle, <span class="hljs-string">"score"</span>: float(cls_scores[k])&#125;)</code></pre><p>获得预测结果之后使用以下代码来评测:</p><pre><code class="hljs undefined">from pycocotools.coco impor COCOfrom pycocotools.cocoeval import COCOevalcocoGt = COCO(PATH_TO_GT_JSON)    cocoDt = cocoGt.loadRes(PATH_TO_RESULT_JSON)# 最后一个参数是数据格式，按照具体任务来指定cocoEval = COCOeval(cocoGt, cocoDt, &quot;segm&quot;)cocoEval.evaluate()cocoEval.accumulate()cocoEval.summarize()</code></pre><h2 id="直接评测"><a href="#直接评测" class="headerlink" title="直接评测"></a>直接评测</h2><p>主要思想还是先创建一个空的 COCOeval 对象，然后每次预测的时候更新结果到其中，最后再直接评测，这个实现起来可能比上面的要麻烦，这里直接贴上 DETR 里面的部分代码：</p><pre><code class="hljs Python"><span class="hljs-comment"># 每次预测后更新结果</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self, predictions)</span>:</span>    img_ids = [predictions[<span class="hljs-string">'image_id'</span>]]    self.img_ids.extend(img_ids)    <span class="hljs-keyword">for</span> iou_type <span class="hljs-keyword">in</span> self.iou_types:        results = self.prepare(predictions, iou_type)        <span class="hljs-comment"># suppress pycocotools prints</span>        <span class="hljs-keyword">with</span> open(os.devnull, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> devnull:            <span class="hljs-keyword">with</span> contextlib.redirect_stdout(devnull):                <span class="hljs-comment"># 将预测结果转换为 COCO 的对象</span>                coco_dt = COCO.loadRes(self.coco_gt, results) <span class="hljs-keyword">if</span> results <span class="hljs-keyword">else</span> COCO()        coco_eval = self.coco_eval[iou_type]        <span class="hljs-comment"># 将结果保存到 COCOeval 对象中</span>        coco_eval.cocoDt = coco_dt        coco_eval.params.imgIds = list(img_ids)        <span class="hljs-comment"># 进行 evaluate 操作，作用等同于上面代码段中的 cocoEval.evaluate()</span>        img_ids, eval_imgs = evaluate(coco_eval)        self.eval_imgs[iou_type].append(eval_imgs)</code></pre><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>直接评测的完整代码</p><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(sess, net, inputs, test_collect, data_loader, base_ds)</span>:</span>    iou_types = (<span class="hljs-string">'segm'</span>, )    coco_evaluator = CocoEvaluator(base_ds, iou_types)    <span class="hljs-keyword">for</span> im, image_id <span class="hljs-keyword">in</span> data_loader:        cat_prob, boxes, seg_pred, masks = im_detect(sess, net, inputs, im, test_collect)        cls_scores = cat_prob[:, <span class="hljs-number">1</span>]        segmaps = np.zeros([len(seg_pred), im.shape[<span class="hljs-number">0</span>], im.shape[<span class="hljs-number">1</span>]])        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(len(seg_pred)):            img_for_single_instance = copy.deepcopy(im)            segmap = seg_pred[k, :, :, <span class="hljs-number">1</span>]            segmap = cv2.resize(segmap, (img_for_single_instance.shape[<span class="hljs-number">1</span>], img_for_single_instance.shape[<span class="hljs-number">0</span>]),                                interpolation=cv2.INTER_LANCZOS4)            segmap_masked = segmap * masks[k]            segmaps[k] = segmap_masked        res = &#123;<span class="hljs-string">'scores'</span>: cls_scores, <span class="hljs-string">'segmaps'</span>: segmaps, <span class="hljs-string">'image_id'</span>: image_id&#125;        <span class="hljs-comment"># 每次预测后更新结果</span>        <span class="hljs-keyword">if</span> coco_evaluator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            coco_evaluator.update(res)    <span class="hljs-comment"># accumulate predictions from all images</span>    <span class="hljs-keyword">if</span> coco_evaluator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:        coco_evaluator.synchronize_between_processes()        coco_evaluator.accumulate()        coco_evaluator.summarize()</code></pre><p>DETR COCOEvaluator，有删改</p><pre><code class="hljs Python"><span class="hljs-comment"># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved</span><span class="hljs-string">"""COCO evaluator that works in distributed mode.Mostly copy-paste from https://github.com/pytorch/vision/blob/edfd5a7/references/detection/coco_eval.pyThe difference is that there is less copy-pasting from pycocotoolsin the end of the file, as python3 can suppress prints with contextlib"""</span><span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> contextlib<span class="hljs-keyword">import</span> copy<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> pycocotools.cocoeval <span class="hljs-keyword">import</span> COCOeval<span class="hljs-keyword">from</span> pycocotools.coco <span class="hljs-keyword">import</span> COCO<span class="hljs-keyword">import</span> pycocotools.mask <span class="hljs-keyword">as</span> mask_util<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CocoEvaluator</span><span class="hljs-params">(object)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, coco_gt, iou_types)</span>:</span>        <span class="hljs-keyword">assert</span> isinstance(iou_types, (list, tuple))        coco_gt = copy.deepcopy(coco_gt)        self.coco_gt = coco_gt        self.iou_types = iou_types        self.coco_eval = &#123;&#125;        <span class="hljs-keyword">for</span> iou_type <span class="hljs-keyword">in</span> iou_types:            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)        self.img_ids = []        self.eval_imgs = &#123;k: [] <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> iou_types&#125;    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self, predictions)</span>:</span>        img_ids = [predictions[<span class="hljs-string">'image_id'</span>]]        self.img_ids.extend(img_ids)        <span class="hljs-keyword">for</span> iou_type <span class="hljs-keyword">in</span> self.iou_types:            results = self.prepare(predictions, iou_type)            <span class="hljs-comment"># suppress pycocotools prints</span>            <span class="hljs-keyword">with</span> open(os.devnull, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> devnull:                <span class="hljs-keyword">with</span> contextlib.redirect_stdout(devnull):                    coco_dt = COCO.loadRes(self.coco_gt, results) <span class="hljs-keyword">if</span> results <span class="hljs-keyword">else</span> COCO()            coco_eval = self.coco_eval[iou_type]            coco_eval.cocoDt = coco_dt            coco_eval.params.imgIds = list(img_ids)            img_ids, eval_imgs = evaluate(coco_eval)            <span class="hljs-comment"># print('eeee', eval_imgs)</span>            self.eval_imgs[iou_type].append(eval_imgs)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">synchronize_between_processes</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">for</span> iou_type <span class="hljs-keyword">in</span> self.iou_types:            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], <span class="hljs-number">2</span>)            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accumulate</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">for</span> coco_eval <span class="hljs-keyword">in</span> self.coco_eval.values():            coco_eval.accumulate()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">summarize</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">for</span> iou_type, coco_eval <span class="hljs-keyword">in</span> self.coco_eval.items():            print(<span class="hljs-string">"IoU metric: &#123;&#125;"</span>.format(iou_type))            coco_eval.summarize()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare</span><span class="hljs-params">(self, predictions, iou_type)</span>:</span>        <span class="hljs-keyword">if</span> iou_type == <span class="hljs-string">"bbox"</span>:            <span class="hljs-keyword">return</span> self.prepare_for_coco_detection(predictions)        <span class="hljs-keyword">elif</span> iou_type == <span class="hljs-string">"segm"</span>:            <span class="hljs-keyword">return</span> self.prepare_for_coco_segmentation(predictions)        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"Unknown iou type &#123;&#125;"</span>.format(iou_type))    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_for_coco_detection</span><span class="hljs-params">(self, predictions)</span>:</span>        coco_results = []        <span class="hljs-keyword">for</span> original_id, prediction <span class="hljs-keyword">in</span> predictions.items():            <span class="hljs-keyword">if</span> len(prediction) == <span class="hljs-number">0</span>:                <span class="hljs-keyword">continue</span>            boxes = prediction[<span class="hljs-string">"boxes"</span>]            boxes = convert_to_xywh(boxes).tolist()            scores = prediction[<span class="hljs-string">"scores"</span>].tolist()            labels = prediction[<span class="hljs-string">"labels"</span>].tolist()            coco_results.extend(                [                    &#123;                        <span class="hljs-string">"image_id"</span>: original_id,                        <span class="hljs-string">"category_id"</span>: labels[k],                        <span class="hljs-string">"bbox"</span>: box,                        <span class="hljs-string">"score"</span>: scores[k],                    &#125;                    <span class="hljs-keyword">for</span> k, box <span class="hljs-keyword">in</span> enumerate(boxes)                ]            )        <span class="hljs-keyword">return</span> coco_results    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_for_coco_segmentation</span><span class="hljs-params">(self, predictions)</span>:</span>        coco_results = []        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> range(len(predictions[<span class="hljs-string">'scores'</span>])):            masks = predictions[<span class="hljs-string">'segmaps'</span>][idx]            masks = masks &gt; <span class="hljs-number">0.5</span>            scores = predictions[<span class="hljs-string">'scores'</span>][idx]            rle = mask_util.encode(np.array(masks[:, :, np.newaxis], dtype=np.uint8, order=<span class="hljs-string">"F"</span>))[<span class="hljs-number">0</span>]            rle[<span class="hljs-string">'counts'</span>] = rle[<span class="hljs-string">"counts"</span>].decode(<span class="hljs-string">"utf-8"</span>)            coco_results.append(                &#123;                    <span class="hljs-string">"image_id"</span>: predictions[<span class="hljs-string">'image_id'</span>],                    <span class="hljs-string">"category_id"</span>: <span class="hljs-number">1</span>,                    <span class="hljs-string">"segmentation"</span>: rle,                    <span class="hljs-string">"score"</span>: scores,                &#125;            )        <span class="hljs-keyword">return</span> coco_results<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convert_to_xywh</span><span class="hljs-params">(boxes)</span>:</span>    xmin, ymin, xmax, ymax = boxes.unbind(<span class="hljs-number">1</span>)    <span class="hljs-keyword">return</span> np.stack((xmin, ymin, xmax - xmin, ymax - ymin), axis=<span class="hljs-number">1</span>)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_common_coco_eval</span><span class="hljs-params">(coco_eval, img_ids, eval_imgs)</span>:</span>    img_ids = list(img_ids)    eval_imgs = list(eval_imgs.flatten())    coco_eval.evalImgs = eval_imgs    coco_eval.params.imgIds = img_ids    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)<span class="hljs-comment">#################################################################</span><span class="hljs-comment"># From pycocotools, just removed the prints and fixed</span><span class="hljs-comment"># a Python3 bug about unicode not defined</span><span class="hljs-comment">#################################################################</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span><span class="hljs-params">(self)</span>:</span>    <span class="hljs-string">'''    Run per image evaluation on given images and store results (a list of dict) in self.evalImgs    :return: None    '''</span>    <span class="hljs-comment"># tic = time.time()</span>    <span class="hljs-comment"># print('Running per image evaluation...')</span>    p = self.params    <span class="hljs-comment"># add backward compatibility if useSegm is specified in params</span>    <span class="hljs-keyword">if</span> p.useSegm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:        p.iouType = <span class="hljs-string">'segm'</span> <span class="hljs-keyword">if</span> p.useSegm == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-string">'bbox'</span>        print(<span class="hljs-string">'useSegm (deprecated) is not None. Running &#123;&#125; evaluation'</span>.format(p.iouType))    <span class="hljs-comment"># print('Evaluate annotation type *&#123;&#125;*'.format(p.iouType))</span>    p.imgIds = list(np.unique(p.imgIds))    <span class="hljs-keyword">if</span> p.useCats:        p.catIds = list(np.unique(p.catIds))    p.maxDets = sorted(p.maxDets)    self.params = p    self._prepare()    <span class="hljs-comment"># loop through images, area range, max detection number</span>    catIds = p.catIds <span class="hljs-keyword">if</span> p.useCats <span class="hljs-keyword">else</span> [<span class="hljs-number">-1</span>]    <span class="hljs-keyword">if</span> p.iouType == <span class="hljs-string">'segm'</span> <span class="hljs-keyword">or</span> p.iouType == <span class="hljs-string">'bbox'</span>:        computeIoU = self.computeIoU    <span class="hljs-keyword">elif</span> p.iouType == <span class="hljs-string">'keypoints'</span>:        computeIoU = self.computeOks    self.ious = &#123;        (imgId, catId): computeIoU(imgId, catId)        <span class="hljs-keyword">for</span> imgId <span class="hljs-keyword">in</span> p.imgIds        <span class="hljs-keyword">for</span> catId <span class="hljs-keyword">in</span> catIds&#125;    evaluateImg = self.evaluateImg    maxDet = p.maxDets[<span class="hljs-number">-1</span>]    evalImgs = [        evaluateImg(imgId, catId, areaRng, maxDet)        <span class="hljs-keyword">for</span> catId <span class="hljs-keyword">in</span> catIds        <span class="hljs-keyword">for</span> areaRng <span class="hljs-keyword">in</span> p.areaRng        <span class="hljs-keyword">for</span> imgId <span class="hljs-keyword">in</span> p.imgIds    ]    <span class="hljs-comment"># this is NOT in the pycocotools code, but could be done outside</span>    evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))    self._paramsEval = copy.deepcopy(self.params)    <span class="hljs-comment"># toc = time.time()</span>    <span class="hljs-comment"># print('DONE (t=&#123;:0.2f&#125;s).'.format(toc-tic))</span>    <span class="hljs-keyword">return</span> p.imgIds, evalImgs<span class="hljs-comment">#################################################################</span><span class="hljs-comment"># end of straight copy from pycocotools, just removing the prints</span><span class="hljs-comment">#################################################################</span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目标检测、实例分割以及关键点检测都可以使用 COCO API 来进行评测，主要方法有两种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;按照 COCO 的格式保存预测的结果，然后评测&lt;/li&gt;
&lt;li&gt;不保存预测结果，直接评测&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;保存预测结果&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python" scheme="https://blog.patrickcty.cc/tags/Python/"/>
    
      <category term="COCO" scheme="https://blog.patrickcty.cc/tags/COCO/"/>
    
  </entry>
  
  <entry>
    <title>获取 COCO 数据集分布信息</title>
    <link href="https://blog.patrickcty.cc/2021/02/09/%E8%8E%B7%E5%8F%96COCO%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E5%B8%83%E4%BF%A1%E6%81%AF/"/>
    <id>https://blog.patrickcty.cc/2021/02/09/获取COCO数据集分布信息/</id>
    <published>2021-02-09T14:04:03.000Z</published>
    <updated>2021-10-02T13:17:35.314Z</updated>
    
    <content type="html"><![CDATA[<p>使用 COCO API 来获得数据集中的数据分布信息，参考<a href="https://blog.csdn.net/gzj2013/article/details/82425954" target="_blank" rel="noopener">CSDN 文章</a>。</p><pre><code class="hljs Python"><span class="hljs-keyword">from</span> pycocotools.coco <span class="hljs-keyword">import</span> COCOcoco=COCO(PATH_TO_COCO_JSON)<span class="hljs-comment"># 获取所有类别对应的名称与 id，一共有 80 类，id 号则是 1~90</span>cat_info = [(c[<span class="hljs-string">'id'</span>], c[<span class="hljs-string">'name'</span>]) <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> coco.cats.values()]<span class="hljs-keyword">for</span> cat_id, cat_name <span class="hljs-keyword">in</span> cat_info:    anns = coco.catToImgs[cat_id]    print(<span class="hljs-string">"&#123;&#125; | &#123;&#125; | &#123;:.2%&#125; | &#123;&#125; | &#123;:.2%&#125;"</span>.format(cat_name, len(set(anns)), len(set(anns)) / len(coco.imgs), len(anns), len(anns) / len(coco.anns)))</code></pre><p>其中 val2017 分布如下：</p><table><thead><tr><th style="text-align:center">类别名</th><th style="text-align:center">图像数</th><th style="text-align:center">类别比例</th><th style="text-align:center">框数</th><th style="text-align:center">框比例</th></tr></thead><tbody><tr><td style="text-align:center">person</td><td style="text-align:center">2693</td><td style="text-align:center">53.86%</td><td style="text-align:center">11004</td><td style="text-align:center">29.92%</td></tr><tr><td style="text-align:center">bicycle</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">316</td><td style="text-align:center">0.86%</td></tr><tr><td style="text-align:center">car</td><td style="text-align:center">535</td><td style="text-align:center">10.70%</td><td style="text-align:center">1932</td><td style="text-align:center">5.25%</td></tr><tr><td style="text-align:center">motorcycle</td><td style="text-align:center">159</td><td style="text-align:center">3.18%</td><td style="text-align:center">371</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">airplane</td><td style="text-align:center">97</td><td style="text-align:center">1.94%</td><td style="text-align:center">143</td><td style="text-align:center">0.39%</td></tr><tr><td style="text-align:center">bus</td><td style="text-align:center">189</td><td style="text-align:center">3.78%</td><td style="text-align:center">285</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">train</td><td style="text-align:center">157</td><td style="text-align:center">3.14%</td><td style="text-align:center">190</td><td style="text-align:center">0.52%</td></tr><tr><td style="text-align:center">truck</td><td style="text-align:center">250</td><td style="text-align:center">5.00%</td><td style="text-align:center">415</td><td style="text-align:center">1.13%</td></tr><tr><td style="text-align:center">boat</td><td style="text-align:center">121</td><td style="text-align:center">2.42%</td><td style="text-align:center">430</td><td style="text-align:center">1.17%</td></tr><tr><td style="text-align:center">traffic light</td><td style="text-align:center">191</td><td style="text-align:center">3.82%</td><td style="text-align:center">637</td><td style="text-align:center">1.73%</td></tr><tr><td style="text-align:center">fire hydrant</td><td style="text-align:center">86</td><td style="text-align:center">1.72%</td><td style="text-align:center">101</td><td style="text-align:center">0.27%</td></tr><tr><td style="text-align:center">stop sign</td><td style="text-align:center">69</td><td style="text-align:center">1.38%</td><td style="text-align:center">75</td><td style="text-align:center">0.20%</td></tr><tr><td style="text-align:center">parking meter</td><td style="text-align:center">37</td><td style="text-align:center">0.74%</td><td style="text-align:center">60</td><td style="text-align:center">0.16%</td></tr><tr><td style="text-align:center">bench</td><td style="text-align:center">235</td><td style="text-align:center">4.70%</td><td style="text-align:center">413</td><td style="text-align:center">1.12%</td></tr><tr><td style="text-align:center">bird</td><td style="text-align:center">125</td><td style="text-align:center">2.50%</td><td style="text-align:center">440</td><td style="text-align:center">1.20%</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">184</td><td style="text-align:center">3.68%</td><td style="text-align:center">202</td><td style="text-align:center">0.55%</td></tr><tr><td style="text-align:center">dog</td><td style="text-align:center">177</td><td style="text-align:center">3.54%</td><td style="text-align:center">218</td><td style="text-align:center">0.59%</td></tr><tr><td style="text-align:center">horse</td><td style="text-align:center">128</td><td style="text-align:center">2.56%</td><td style="text-align:center">273</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">sheep</td><td style="text-align:center">65</td><td style="text-align:center">1.30%</td><td style="text-align:center">361</td><td style="text-align:center">0.98%</td></tr><tr><td style="text-align:center">cow</td><td style="text-align:center">87</td><td style="text-align:center">1.74%</td><td style="text-align:center">380</td><td style="text-align:center">1.03%</td></tr><tr><td style="text-align:center">elephant</td><td style="text-align:center">89</td><td style="text-align:center">1.78%</td><td style="text-align:center">255</td><td style="text-align:center">0.69%</td></tr><tr><td style="text-align:center">bear</td><td style="text-align:center">49</td><td style="text-align:center">0.98%</td><td style="text-align:center">71</td><td style="text-align:center">0.19%</td></tr><tr><td style="text-align:center">zebra</td><td style="text-align:center">85</td><td style="text-align:center">1.70%</td><td style="text-align:center">268</td><td style="text-align:center">0.73%</td></tr><tr><td style="text-align:center">giraffe</td><td style="text-align:center">101</td><td style="text-align:center">2.02%</td><td style="text-align:center">232</td><td style="text-align:center">0.63%</td></tr><tr><td style="text-align:center">backpack</td><td style="text-align:center">228</td><td style="text-align:center">4.56%</td><td style="text-align:center">371</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">umbrella</td><td style="text-align:center">174</td><td style="text-align:center">3.48%</td><td style="text-align:center">413</td><td style="text-align:center">1.12%</td></tr><tr><td style="text-align:center">handbag</td><td style="text-align:center">292</td><td style="text-align:center">5.84%</td><td style="text-align:center">540</td><td style="text-align:center">1.47%</td></tr><tr><td style="text-align:center">tie</td><td style="text-align:center">145</td><td style="text-align:center">2.90%</td><td style="text-align:center">254</td><td style="text-align:center">0.69%</td></tr><tr><td style="text-align:center">suitcase</td><td style="text-align:center">105</td><td style="text-align:center">2.10%</td><td style="text-align:center">303</td><td style="text-align:center">0.82%</td></tr><tr><td style="text-align:center">frisbee</td><td style="text-align:center">84</td><td style="text-align:center">1.68%</td><td style="text-align:center">115</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">skis</td><td style="text-align:center">120</td><td style="text-align:center">2.40%</td><td style="text-align:center">241</td><td style="text-align:center">0.66%</td></tr><tr><td style="text-align:center">snowboard</td><td style="text-align:center">49</td><td style="text-align:center">0.98%</td><td style="text-align:center">69</td><td style="text-align:center">0.19%</td></tr><tr><td style="text-align:center">sports ball</td><td style="text-align:center">169</td><td style="text-align:center">3.38%</td><td style="text-align:center">263</td><td style="text-align:center">0.72%</td></tr><tr><td style="text-align:center">kite</td><td style="text-align:center">91</td><td style="text-align:center">1.82%</td><td style="text-align:center">336</td><td style="text-align:center">0.91%</td></tr><tr><td style="text-align:center">baseball bat</td><td style="text-align:center">97</td><td style="text-align:center">1.94%</td><td style="text-align:center">146</td><td style="text-align:center">0.40%</td></tr><tr><td style="text-align:center">baseball glove</td><td style="text-align:center">100</td><td style="text-align:center">2.00%</td><td style="text-align:center">148</td><td style="text-align:center">0.40%</td></tr><tr><td style="text-align:center">skateboard</td><td style="text-align:center">127</td><td style="text-align:center">2.54%</td><td style="text-align:center">179</td><td style="text-align:center">0.49%</td></tr><tr><td style="text-align:center">surfboard</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">269</td><td style="text-align:center">0.73%</td></tr><tr><td style="text-align:center">tennis racket</td><td style="text-align:center">167</td><td style="text-align:center">3.34%</td><td style="text-align:center">225</td><td style="text-align:center">0.61%</td></tr><tr><td style="text-align:center">bottle</td><td style="text-align:center">379</td><td style="text-align:center">7.58%</td><td style="text-align:center">1025</td><td style="text-align:center">2.79%</td></tr><tr><td style="text-align:center">wine glass</td><td style="text-align:center">110</td><td style="text-align:center">2.20%</td><td style="text-align:center">343</td><td style="text-align:center">0.93%</td></tr><tr><td style="text-align:center">cup</td><td style="text-align:center">390</td><td style="text-align:center">7.80%</td><td style="text-align:center">899</td><td style="text-align:center">2.44%</td></tr><tr><td style="text-align:center">fork</td><td style="text-align:center">155</td><td style="text-align:center">3.10%</td><td style="text-align:center">215</td><td style="text-align:center">0.58%</td></tr><tr><td style="text-align:center">knife</td><td style="text-align:center">181</td><td style="text-align:center">3.62%</td><td style="text-align:center">326</td><td style="text-align:center">0.89%</td></tr><tr><td style="text-align:center">spoon</td><td style="text-align:center">153</td><td style="text-align:center">3.06%</td><td style="text-align:center">253</td><td style="text-align:center">0.69%</td></tr><tr><td style="text-align:center">bowl</td><td style="text-align:center">314</td><td style="text-align:center">6.28%</td><td style="text-align:center">626</td><td style="text-align:center">1.70%</td></tr><tr><td style="text-align:center">banana</td><td style="text-align:center">103</td><td style="text-align:center">2.06%</td><td style="text-align:center">379</td><td style="text-align:center">1.03%</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">76</td><td style="text-align:center">1.52%</td><td style="text-align:center">239</td><td style="text-align:center">0.65%</td></tr><tr><td style="text-align:center">sandwich</td><td style="text-align:center">98</td><td style="text-align:center">1.96%</td><td style="text-align:center">177</td><td style="text-align:center">0.48%</td></tr><tr><td style="text-align:center">orange</td><td style="text-align:center">85</td><td style="text-align:center">1.70%</td><td style="text-align:center">287</td><td style="text-align:center">0.78%</td></tr><tr><td style="text-align:center">broccoli</td><td style="text-align:center">71</td><td style="text-align:center">1.42%</td><td style="text-align:center">316</td><td style="text-align:center">0.86%</td></tr><tr><td style="text-align:center">carrot</td><td style="text-align:center">81</td><td style="text-align:center">1.62%</td><td style="text-align:center">371</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">hot dog</td><td style="text-align:center">51</td><td style="text-align:center">1.02%</td><td style="text-align:center">127</td><td style="text-align:center">0.35%</td></tr><tr><td style="text-align:center">pizza</td><td style="text-align:center">153</td><td style="text-align:center">3.06%</td><td style="text-align:center">285</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">donut</td><td style="text-align:center">62</td><td style="text-align:center">1.24%</td><td style="text-align:center">338</td><td style="text-align:center">0.92%</td></tr><tr><td style="text-align:center">cake</td><td style="text-align:center">124</td><td style="text-align:center">2.48%</td><td style="text-align:center">316</td><td style="text-align:center">0.86%</td></tr><tr><td style="text-align:center">chair</td><td style="text-align:center">580</td><td style="text-align:center">11.60%</td><td style="text-align:center">1791</td><td style="text-align:center">4.87%</td></tr><tr><td style="text-align:center">couch</td><td style="text-align:center">195</td><td style="text-align:center">3.90%</td><td style="text-align:center">261</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">potted plant</td><td style="text-align:center">172</td><td style="text-align:center">3.44%</td><td style="text-align:center">343</td><td style="text-align:center">0.93%</td></tr><tr><td style="text-align:center">bed</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">163</td><td style="text-align:center">0.44%</td></tr><tr><td style="text-align:center">dining table</td><td style="text-align:center">501</td><td style="text-align:center">10.02%</td><td style="text-align:center">697</td><td style="text-align:center">1.90%</td></tr><tr><td style="text-align:center">toilet</td><td style="text-align:center">149</td><td style="text-align:center">2.98%</td><td style="text-align:center">179</td><td style="text-align:center">0.49%</td></tr><tr><td style="text-align:center">tv</td><td style="text-align:center">207</td><td style="text-align:center">4.14%</td><td style="text-align:center">288</td><td style="text-align:center">0.78%</td></tr><tr><td style="text-align:center">laptop</td><td style="text-align:center">183</td><td style="text-align:center">3.66%</td><td style="text-align:center">231</td><td style="text-align:center">0.63%</td></tr><tr><td style="text-align:center">mouse</td><td style="text-align:center">88</td><td style="text-align:center">1.76%</td><td style="text-align:center">106</td><td style="text-align:center">0.29%</td></tr><tr><td style="text-align:center">remote</td><td style="text-align:center">145</td><td style="text-align:center">2.90%</td><td style="text-align:center">283</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">keyboard</td><td style="text-align:center">106</td><td style="text-align:center">2.12%</td><td style="text-align:center">153</td><td style="text-align:center">0.42%</td></tr><tr><td style="text-align:center">cell phone</td><td style="text-align:center">214</td><td style="text-align:center">4.28%</td><td style="text-align:center">262</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">microwave</td><td style="text-align:center">54</td><td style="text-align:center">1.08%</td><td style="text-align:center">55</td><td style="text-align:center">0.15%</td></tr><tr><td style="text-align:center">oven</td><td style="text-align:center">115</td><td style="text-align:center">2.30%</td><td style="text-align:center">143</td><td style="text-align:center">0.39%</td></tr><tr><td style="text-align:center">toaster</td><td style="text-align:center">8</td><td style="text-align:center">0.16%</td><td style="text-align:center">9</td><td style="text-align:center">0.02%</td></tr><tr><td style="text-align:center">sink</td><td style="text-align:center">187</td><td style="text-align:center">3.74%</td><td style="text-align:center">225</td><td style="text-align:center">0.61%</td></tr><tr><td style="text-align:center">refrigerator</td><td style="text-align:center">101</td><td style="text-align:center">2.02%</td><td style="text-align:center">126</td><td style="text-align:center">0.34%</td></tr><tr><td style="text-align:center">book</td><td style="text-align:center">230</td><td style="text-align:center">4.60%</td><td style="text-align:center">1161</td><td style="text-align:center">3.16%</td></tr><tr><td style="text-align:center">clock</td><td style="text-align:center">204</td><td style="text-align:center">4.08%</td><td style="text-align:center">267</td><td style="text-align:center">0.73%</td></tr><tr><td style="text-align:center">vase</td><td style="text-align:center">137</td><td style="text-align:center">2.74%</td><td style="text-align:center">277</td><td style="text-align:center">0.75%</td></tr><tr><td style="text-align:center">scissors</td><td style="text-align:center">28</td><td style="text-align:center">0.56%</td><td style="text-align:center">36</td><td style="text-align:center">0.10%</td></tr><tr><td style="text-align:center">teddy bear</td><td style="text-align:center">94</td><td style="text-align:center">1.88%</td><td style="text-align:center">191</td><td style="text-align:center">0.52%</td></tr><tr><td style="text-align:center">hair drier</td><td style="text-align:center">9</td><td style="text-align:center">0.18%</td><td style="text-align:center">11</td><td style="text-align:center">0.03%</td></tr><tr><td style="text-align:center">toothbrush</td><td style="text-align:center">34</td><td style="text-align:center">0.68%</td><td style="text-align:center">57</td><td style="text-align:center">0.15%</td></tr></tbody></table><p>train2017 分布如下</p><table><thead><tr><th style="text-align:center">类别名</th><th style="text-align:center">图像数</th><th style="text-align:center">类别比例</th><th style="text-align:center">框数</th><th style="text-align:center">框比例</th></tr></thead><tbody><tr><td style="text-align:center">person</td><td style="text-align:center">64115</td><td style="text-align:center">54.20%</td><td style="text-align:center">262465</td><td style="text-align:center">30.52%</td></tr><tr><td style="text-align:center">bicycle</td><td style="text-align:center">3252</td><td style="text-align:center">2.75%</td><td style="text-align:center">7113</td><td style="text-align:center">0.83%</td></tr><tr><td style="text-align:center">car</td><td style="text-align:center">12251</td><td style="text-align:center">10.36%</td><td style="text-align:center">43867</td><td style="text-align:center">5.10%</td></tr><tr><td style="text-align:center">motorcycle</td><td style="text-align:center">3502</td><td style="text-align:center">2.96%</td><td style="text-align:center">8725</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">airplane</td><td style="text-align:center">2986</td><td style="text-align:center">2.52%</td><td style="text-align:center">5135</td><td style="text-align:center">0.60%</td></tr><tr><td style="text-align:center">bus</td><td style="text-align:center">3952</td><td style="text-align:center">3.34%</td><td style="text-align:center">6069</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">train</td><td style="text-align:center">3588</td><td style="text-align:center">3.03%</td><td style="text-align:center">4571</td><td style="text-align:center">0.53%</td></tr><tr><td style="text-align:center">truck</td><td style="text-align:center">6127</td><td style="text-align:center">5.18%</td><td style="text-align:center">9973</td><td style="text-align:center">1.16%</td></tr><tr><td style="text-align:center">boat</td><td style="text-align:center">3025</td><td style="text-align:center">2.56%</td><td style="text-align:center">10759</td><td style="text-align:center">1.25%</td></tr><tr><td style="text-align:center">traffic light</td><td style="text-align:center">4139</td><td style="text-align:center">3.50%</td><td style="text-align:center">12884</td><td style="text-align:center">1.50%</td></tr><tr><td style="text-align:center">fire hydrant</td><td style="text-align:center">1711</td><td style="text-align:center">1.45%</td><td style="text-align:center">1865</td><td style="text-align:center">0.22%</td></tr><tr><td style="text-align:center">stop sign</td><td style="text-align:center">1734</td><td style="text-align:center">1.47%</td><td style="text-align:center">1983</td><td style="text-align:center">0.23%</td></tr><tr><td style="text-align:center">parking meter</td><td style="text-align:center">705</td><td style="text-align:center">0.60%</td><td style="text-align:center">1285</td><td style="text-align:center">0.15%</td></tr><tr><td style="text-align:center">bench</td><td style="text-align:center">5570</td><td style="text-align:center">4.71%</td><td style="text-align:center">9838</td><td style="text-align:center">1.14%</td></tr><tr><td style="text-align:center">bird</td><td style="text-align:center">3237</td><td style="text-align:center">2.74%</td><td style="text-align:center">10806</td><td style="text-align:center">1.26%</td></tr><tr><td style="text-align:center">cat</td><td style="text-align:center">4114</td><td style="text-align:center">3.48%</td><td style="text-align:center">4768</td><td style="text-align:center">0.55%</td></tr><tr><td style="text-align:center">dog</td><td style="text-align:center">4385</td><td style="text-align:center">3.71%</td><td style="text-align:center">5508</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">horse</td><td style="text-align:center">2941</td><td style="text-align:center">2.49%</td><td style="text-align:center">6587</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">sheep</td><td style="text-align:center">1529</td><td style="text-align:center">1.29%</td><td style="text-align:center">9509</td><td style="text-align:center">1.11%</td></tr><tr><td style="text-align:center">cow</td><td style="text-align:center">1968</td><td style="text-align:center">1.66%</td><td style="text-align:center">8147</td><td style="text-align:center">0.95%</td></tr><tr><td style="text-align:center">elephant</td><td style="text-align:center">2143</td><td style="text-align:center">1.81%</td><td style="text-align:center">5513</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">bear</td><td style="text-align:center">960</td><td style="text-align:center">0.81%</td><td style="text-align:center">1294</td><td style="text-align:center">0.15%</td></tr><tr><td style="text-align:center">zebra</td><td style="text-align:center">1916</td><td style="text-align:center">1.62%</td><td style="text-align:center">5303</td><td style="text-align:center">0.62%</td></tr><tr><td style="text-align:center">giraffe</td><td style="text-align:center">2546</td><td style="text-align:center">2.15%</td><td style="text-align:center">5131</td><td style="text-align:center">0.60%</td></tr><tr><td style="text-align:center">backpack</td><td style="text-align:center">5528</td><td style="text-align:center">4.67%</td><td style="text-align:center">8720</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">umbrella</td><td style="text-align:center">3968</td><td style="text-align:center">3.35%</td><td style="text-align:center">11431</td><td style="text-align:center">1.33%</td></tr><tr><td style="text-align:center">handbag</td><td style="text-align:center">6841</td><td style="text-align:center">5.78%</td><td style="text-align:center">12354</td><td style="text-align:center">1.44%</td></tr><tr><td style="text-align:center">tie</td><td style="text-align:center">3810</td><td style="text-align:center">3.22%</td><td style="text-align:center">6496</td><td style="text-align:center">0.76%</td></tr><tr><td style="text-align:center">suitcase</td><td style="text-align:center">2402</td><td style="text-align:center">2.03%</td><td style="text-align:center">6192</td><td style="text-align:center">0.72%</td></tr><tr><td style="text-align:center">frisbee</td><td style="text-align:center">2184</td><td style="text-align:center">1.85%</td><td style="text-align:center">2682</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">skis</td><td style="text-align:center">3082</td><td style="text-align:center">2.61%</td><td style="text-align:center">6646</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">snowboard</td><td style="text-align:center">1654</td><td style="text-align:center">1.40%</td><td style="text-align:center">2685</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">sports ball</td><td style="text-align:center">4262</td><td style="text-align:center">3.60%</td><td style="text-align:center">6347</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">kite</td><td style="text-align:center">2261</td><td style="text-align:center">1.91%</td><td style="text-align:center">9076</td><td style="text-align:center">1.06%</td></tr><tr><td style="text-align:center">baseball bat</td><td style="text-align:center">2506</td><td style="text-align:center">2.12%</td><td style="text-align:center">3276</td><td style="text-align:center">0.38%</td></tr><tr><td style="text-align:center">baseball glove</td><td style="text-align:center">2629</td><td style="text-align:center">2.22%</td><td style="text-align:center">3747</td><td style="text-align:center">0.44%</td></tr><tr><td style="text-align:center">skateboard</td><td style="text-align:center">3476</td><td style="text-align:center">2.94%</td><td style="text-align:center">5543</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">surfboard</td><td style="text-align:center">3486</td><td style="text-align:center">2.95%</td><td style="text-align:center">6126</td><td style="text-align:center">0.71%</td></tr><tr><td style="text-align:center">tennis racket</td><td style="text-align:center">3394</td><td style="text-align:center">2.87%</td><td style="text-align:center">4812</td><td style="text-align:center">0.56%</td></tr><tr><td style="text-align:center">bottle</td><td style="text-align:center">8501</td><td style="text-align:center">7.19%</td><td style="text-align:center">24342</td><td style="text-align:center">2.83%</td></tr><tr><td style="text-align:center">wine glass</td><td style="text-align:center">2533</td><td style="text-align:center">2.14%</td><td style="text-align:center">7913</td><td style="text-align:center">0.92%</td></tr><tr><td style="text-align:center">cup</td><td style="text-align:center">9189</td><td style="text-align:center">7.77%</td><td style="text-align:center">20650</td><td style="text-align:center">2.40%</td></tr><tr><td style="text-align:center">fork</td><td style="text-align:center">3555</td><td style="text-align:center">3.01%</td><td style="text-align:center">5479</td><td style="text-align:center">0.64%</td></tr><tr><td style="text-align:center">knife</td><td style="text-align:center">4326</td><td style="text-align:center">3.66%</td><td style="text-align:center">7770</td><td style="text-align:center">0.90%</td></tr><tr><td style="text-align:center">spoon</td><td style="text-align:center">3529</td><td style="text-align:center">2.98%</td><td style="text-align:center">6165</td><td style="text-align:center">0.72%</td></tr><tr><td style="text-align:center">bowl</td><td style="text-align:center">7111</td><td style="text-align:center">6.01%</td><td style="text-align:center">14358</td><td style="text-align:center">1.67%</td></tr><tr><td style="text-align:center">banana</td><td style="text-align:center">2243</td><td style="text-align:center">1.90%</td><td style="text-align:center">9458</td><td style="text-align:center">1.10%</td></tr><tr><td style="text-align:center">apple</td><td style="text-align:center">1586</td><td style="text-align:center">1.34%</td><td style="text-align:center">5851</td><td style="text-align:center">0.68%</td></tr><tr><td style="text-align:center">sandwich</td><td style="text-align:center">2365</td><td style="text-align:center">2.00%</td><td style="text-align:center">4373</td><td style="text-align:center">0.51%</td></tr><tr><td style="text-align:center">orange</td><td style="text-align:center">1699</td><td style="text-align:center">1.44%</td><td style="text-align:center">6399</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">broccoli</td><td style="text-align:center">1939</td><td style="text-align:center">1.64%</td><td style="text-align:center">7308</td><td style="text-align:center">0.85%</td></tr><tr><td style="text-align:center">carrot</td><td style="text-align:center">1683</td><td style="text-align:center">1.42%</td><td style="text-align:center">7852</td><td style="text-align:center">0.91%</td></tr><tr><td style="text-align:center">hot dog</td><td style="text-align:center">1222</td><td style="text-align:center">1.03%</td><td style="text-align:center">2918</td><td style="text-align:center">0.34%</td></tr><tr><td style="text-align:center">pizza</td><td style="text-align:center">3166</td><td style="text-align:center">2.68%</td><td style="text-align:center">5821</td><td style="text-align:center">0.68%</td></tr><tr><td style="text-align:center">donut</td><td style="text-align:center">1523</td><td style="text-align:center">1.29%</td><td style="text-align:center">7179</td><td style="text-align:center">0.83%</td></tr><tr><td style="text-align:center">cake</td><td style="text-align:center">2925</td><td style="text-align:center">2.47%</td><td style="text-align:center">6353</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">chair</td><td style="text-align:center">12774</td><td style="text-align:center">10.80%</td><td style="text-align:center">38491</td><td style="text-align:center">4.48%</td></tr><tr><td style="text-align:center">couch</td><td style="text-align:center">4423</td><td style="text-align:center">3.74%</td><td style="text-align:center">5779</td><td style="text-align:center">0.67%</td></tr><tr><td style="text-align:center">potted plant</td><td style="text-align:center">4452</td><td style="text-align:center">3.76%</td><td style="text-align:center">8652</td><td style="text-align:center">1.01%</td></tr><tr><td style="text-align:center">bed</td><td style="text-align:center">3682</td><td style="text-align:center">3.11%</td><td style="text-align:center">4192</td><td style="text-align:center">0.49%</td></tr><tr><td style="text-align:center">dining table</td><td style="text-align:center">11837</td><td style="text-align:center">10.01%</td><td style="text-align:center">15714</td><td style="text-align:center">1.83%</td></tr><tr><td style="text-align:center">toilet</td><td style="text-align:center">3353</td><td style="text-align:center">2.83%</td><td style="text-align:center">4157</td><td style="text-align:center">0.48%</td></tr><tr><td style="text-align:center">tv</td><td style="text-align:center">4561</td><td style="text-align:center">3.86%</td><td style="text-align:center">5805</td><td style="text-align:center">0.67%</td></tr><tr><td style="text-align:center">laptop</td><td style="text-align:center">3524</td><td style="text-align:center">2.98%</td><td style="text-align:center">4970</td><td style="text-align:center">0.58%</td></tr><tr><td style="text-align:center">mouse</td><td style="text-align:center">1876</td><td style="text-align:center">1.59%</td><td style="text-align:center">2262</td><td style="text-align:center">0.26%</td></tr><tr><td style="text-align:center">remote</td><td style="text-align:center">3076</td><td style="text-align:center">2.60%</td><td style="text-align:center">5703</td><td style="text-align:center">0.66%</td></tr><tr><td style="text-align:center">keyboard</td><td style="text-align:center">2115</td><td style="text-align:center">1.79%</td><td style="text-align:center">2855</td><td style="text-align:center">0.33%</td></tr><tr><td style="text-align:center">cell phone</td><td style="text-align:center">4803</td><td style="text-align:center">4.06%</td><td style="text-align:center">6434</td><td style="text-align:center">0.75%</td></tr><tr><td style="text-align:center">microwave</td><td style="text-align:center">1547</td><td style="text-align:center">1.31%</td><td style="text-align:center">1673</td><td style="text-align:center">0.19%</td></tr><tr><td style="text-align:center">oven</td><td style="text-align:center">2877</td><td style="text-align:center">2.43%</td><td style="text-align:center">3334</td><td style="text-align:center">0.39%</td></tr><tr><td style="text-align:center">toaster</td><td style="text-align:center">217</td><td style="text-align:center">0.18%</td><td style="text-align:center">225</td><td style="text-align:center">0.03%</td></tr><tr><td style="text-align:center">sink</td><td style="text-align:center">4678</td><td style="text-align:center">3.95%</td><td style="text-align:center">5610</td><td style="text-align:center">0.65%</td></tr><tr><td style="text-align:center">refrigerator</td><td style="text-align:center">2360</td><td style="text-align:center">2.00%</td><td style="text-align:center">2637</td><td style="text-align:center">0.31%</td></tr><tr><td style="text-align:center">book</td><td style="text-align:center">5332</td><td style="text-align:center">4.51%</td><td style="text-align:center">24715</td><td style="text-align:center">2.87%</td></tr><tr><td style="text-align:center">clock</td><td style="text-align:center">4659</td><td style="text-align:center">3.94%</td><td style="text-align:center">6334</td><td style="text-align:center">0.74%</td></tr><tr><td style="text-align:center">vase</td><td style="text-align:center">3593</td><td style="text-align:center">3.04%</td><td style="text-align:center">6613</td><td style="text-align:center">0.77%</td></tr><tr><td style="text-align:center">scissors</td><td style="text-align:center">947</td><td style="text-align:center">0.80%</td><td style="text-align:center">1481</td><td style="text-align:center">0.17%</td></tr><tr><td style="text-align:center">teddy bear</td><td style="text-align:center">2140</td><td style="text-align:center">1.81%</td><td style="text-align:center">4793</td><td style="text-align:center">0.56%</td></tr><tr><td style="text-align:center">hair drier</td><td style="text-align:center">189</td><td style="text-align:center">0.16%</td><td style="text-align:center">198</td><td style="text-align:center">0.02%</td></tr><tr><td style="text-align:center">toothbrush</td><td style="text-align:center">1007</td><td style="text-align:center">0.85%</td><td style="text-align:center">1954</td><td style="text-align:center">0.23%</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用 COCO API 来获得数据集中的数据分布信息，参考&lt;a href=&quot;https://blog.csdn.net/gzj2013/article/details/82425954&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CSDN 文章&lt;/a&gt;。
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python" scheme="https://blog.patrickcty.cc/tags/Python/"/>
    
      <category term="COCO" scheme="https://blog.patrickcty.cc/tags/COCO/"/>
    
  </entry>
  
  <entry>
    <title>Linux 安装蓝牙接收器驱动</title>
    <link href="https://blog.patrickcty.cc/2021/01/26/Linux%E5%AE%89%E8%A3%85%E8%93%9D%E7%89%99%E6%8E%A5%E6%94%B6%E5%99%A8%E9%A9%B1%E5%8A%A8/"/>
    <id>https://blog.patrickcty.cc/2021/01/26/Linux安装蓝牙接收器驱动/</id>
    <published>2021-01-26T07:59:00.000Z</published>
    <updated>2021-10-02T13:35:48.652Z</updated>
    
    <content type="html"><![CDATA[<p>买了一个奥睿科的蓝牙驱动器，型号为 BTA-508，官网上介绍是只支持 Windows 系统。本来以为凉了，结果在 <a href="https://ubuntuforums.org/showthread.php?t=2453631&amp;p=14000037&amp;highlight=#post14000037" target="_blank" rel="noopener">Ubuntu 论坛</a>发现可以直接下载对应芯片的驱动。</p><blockquote><p>驱动不支持 Linux 怎么办？-&gt; 查看用的是什么芯片 -&gt; 下载安装对应芯片的驱动</p></blockquote><p>当然对于 Logitech 的 Flow 这种需要软件支持的驱动就不太好使了，硬件的驱动一般都能找到 Linux 版本。这次真的长见识了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;买了一个奥睿科的蓝牙驱动器，型号为 BTA-508，官网上介绍是只支持 Windows 系统。本来以为凉了，结果在 &lt;a href=&quot;https://ubuntuforums.org/showthread.php?t=2453631&amp;amp;p=14000037&amp;amp;h
      
    
    </summary>
    
      <category term="其他" scheme="https://blog.patrickcty.cc/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="Linux" scheme="https://blog.patrickcty.cc/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch使用DDP进行分布式训练</title>
    <link href="https://blog.patrickcty.cc/2020/11/18/PyTorch%E4%BD%BF%E7%94%A8DDP%E8%BF%9B%E8%A1%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    <id>https://blog.patrickcty.cc/2020/11/18/PyTorch使用DDP进行分布式训练/</id>
    <published>2020-11-18T12:31:01.000Z</published>
    <updated>2021-10-02T13:38:09.242Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>DDP 是 DistributedDataParallel 的简写，用来进行分布式训练，可以是单主机多 GPU 也可以是多主机多 GPU，以下均从单主机多 GPU 来介绍。其原理是把模型复制到其他的 GPU 上，然后在训练的过程中汇总梯度，进行迭代，从感知上就像是增大了 N 倍的显存。</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>具体的操作是产生多个进程，每个进程在一个 GPU 上训练，然后结果自动地在主进程中进行汇总。因此启动方式需要通过 <code>torch.distributed.launch</code> 来启动，如下：</p><pre><code class="hljs undefined">python -m torch.distributed.launch --nproc_per_node=2 main.py</code></pre><p>其中 <code>nproc_per_node</code> 是要使用的总的 GPU 数，如果一台主机上有多个 GPU，但是只想用其中的部分来进行训练，则可以用以下命令来启动：</p><pre><code class="hljs undefined">CUDA_VISIBLE_DEVICES=1,3,5 python -m torch.distributed.launch --nproc_per_node=3 main.py</code></pre><p>如果想调试分布式的代码，那么用以下方式来启动：</p><pre><code class="hljs undefined">NCCL_DEBUG=INFO python -m torch.distributed.launch --nproc_per_node=2 main.py</code></pre><h2 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h2><p>为了让程序能很好地与 GPU 交互，<code>torch.distributed.launch</code> 在启动进程的时候会传入 <code>local_rank</code> 参数，用来标识 GPU，因此我们要在训练脚本中加入相应的参数。值得注意的是，<code>local_rank</code> 永远是从零开始。具体代码如下：</p><pre><code class="hljs python">parser.add_argument(<span class="hljs-string">'--local_rank'</span>, type=int, default=<span class="hljs-number">0</span>)</code></pre><h2 id="初始化分布式环境"><a href="#初始化分布式环境" class="headerlink" title="初始化分布式环境"></a>初始化分布式环境</h2><p>首先要初始化进程组，对于单主机来说就用下面简单的语句即可</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> distopt = parser.parse_args()  <span class="hljs-comment"># 解析命令行参数</span>torch.cuda.set_device(opt.local_rank)dist.init_process_group(<span class="hljs-string">'nccl'</span>)device = torch.device(<span class="hljs-string">f'cuda:<span class="hljs-subst">&#123;opt.local_rank&#125;</span>'</span>)</code></pre><h2 id="构建分布式模型"><a href="#构建分布式模型" class="headerlink" title="构建分布式模型"></a>构建分布式模型</h2><p>分布式环境下默认 BN 是在主 GPU 上进行计算，然后同步到其他 GPU，因此使用普通 BN 的时候不能充分发挥分布式训练中大 batch size 的优势。如果使用 Sync BN 则会解决这个问题，这个转换也可以使用一个函数来完成。</p><p>之后把模型转换为 DDP 模型即可，注意的是每一个进程都会初始化一个 DDP 模型，<code>device_id</code> 指的是当前进程要用到的 GPU 标号列表。因为我们通常一个进程一个 GPU，因此这里使用 <code>[opt.local_rank]</code> 即可，输出的话是会输出到单个设备上，因此就不用转换为 list。需要注意的是，Sync BN 不支持单进程多 GPU。</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DistributedDataParallel <span class="hljs-keyword">as</span> DDPmodel = ResNet()model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)</code></pre><h2 id="获得分布式-data-loader"><a href="#获得分布式-data-loader" class="headerlink" title="获得分布式 data loader"></a>获得分布式 data loader</h2><p>分布式训练的过程中我们要保证每个 GPU 取到的是不同的数据，因此不能直接使用普通的 Dataloader，要传入一个 sampler 参数，具体也很简单，如下所示：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> datadataset = SomeDataset(image_root, gt_root, trainsize)<span class="hljs-comment"># 要从原来 dataset 得到一个分布式 sampler</span>sampler = data.distributed.DistributedSampler(dataset)shuffle = <span class="hljs-literal">False</span>  <span class="hljs-comment"># sampler 与 shuffle 不兼容</span>data_loader = data.DataLoader(dataset=dataset,                              batch_size=batchsize,                              shuffle=shuffle,                              num_workers=num_workers,                              pin_memory=pin_memory,                              sampler=sampler)<span class="hljs-keyword">return</span> data_loader</code></pre><p>dataloader 中要传入 sampler 作为参数，其他的注意事项在注释中</p><h2 id="获得总的-loss"><a href="#获得总的-loss" class="headerlink" title="获得总的 loss"></a>获得总的 loss</h2><p>总的准备工作都完成了，接下来就是像平常一样训练了。但是每个进程中的 loss 都是通过自己的输入得到的，如果要得到总的 loss 则需要手动同步一下，具体操作如下：</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce_tensor</span><span class="hljs-params">(tensor: torch.Tensor)</span> -&gt; torch.Tensor:</span>    rt = tensor.clone()    dist.all_reduce(rt, op=dist.ReduceOp.SUM)    rt /= dist.get_world_size()  <span class="hljs-comment"># 这是进程的数量</span>    <span class="hljs-keyword">return</span> rt        reduced_loss1 = reduce_tensor(loss1.data).item()</code></pre><p><code>all_reduce</code> 会自动获取各个进程中同名 tensor，然后通过指定的 op 来进行计算，最后再同步到各个进程当中，也就是说这是一个原地的操作。为了避免可能产生的影响，这里不是直接对原来的 tensor 进行 reduce，而是先取了副本。</p><h2 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h2><p>这里有一个大坑！虽然参数会在各个进程中汇总，但是实际保存的模型的 state_dict 和非分布式的还是有区别的，如果直接载入很可能会出错，解决方法下面会提到。</p><h2 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h2><p>如果你没有掉进上一节的坑里面，那么测试阶段的代码可以和非分布式测试的完全相同。</p><h2 id="打印-log"><a href="#打印-log" class="headerlink" title="打印 log"></a>打印 log</h2><p>因为各个进程代码完全一样，因此打印结果也是打印 N 份，一个简单的解决方法就是判断当前的 <code>local_rank</code>，只有当其为特定值的时候才打印。</p><pre><code class="hljs python"><span class="hljs-keyword">if</span> opt.local_rank == <span class="hljs-number">0</span>:    print(<span class="hljs-string">'some log'</span>)</code></pre><h2 id="遇到的坑"><a href="#遇到的坑" class="headerlink" title="遇到的坑"></a>遇到的坑</h2><p>DDP 训练的模型通过非 DDP 进行加载之后结果非常差。</p><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>保存的模型 state_dict 前缀多了一个 <code>module.</code>，这样在 <code>strict=False</code> 下载入参数的时候就相当于载入了个寂寞，因此结果很差</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li>加载 DDP 模型的时候重新构造 state_dict，将二者名称统一<a href="https://discuss.pytorch.org/t/failed-to-load-model-trained-by-ddp-for-inference/84841" target="_blank" rel="noopener">[1]</a>:</li></ol><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dist_load</span><span class="hljs-params">(state_dict)</span>:</span>    new_state_dict = OrderedDict()    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> state_dict.items():        name = k[<span class="hljs-number">7</span>:]  <span class="hljs-comment"># remove 'module.' of DataParallel/DistributedDataParallel</span>        new_state_dict[name] = v    <span class="hljs-keyword">return</span> new_state_dict</code></pre><ol start="2"><li>（==推荐==）保存 DDP 模型的时候直接保存不包含 <code>module.</code> 前缀<a href="https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/17" target="_blank" rel="noopener">[2]</a>：</li></ol><pre><code class="hljs python">torch.save(model.module.state_dict(), path_to_file)</code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>如果你觉得还是讲的不清楚，那就看<a href="https://zhuanlan.zhihu.com/p/145427849" target="_blank" rel="noopener">这篇文章</a>吧，我就是跟着这篇文章来写的。更深入的理解分析就看<a href="https://zhuanlan.zhihu.com/p/250471767" target="_blank" rel="noopener">这个系列</a>。</p><h2 id="总的代码"><a href="#总的代码" class="headerlink" title="总的代码"></a>总的代码</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime<span class="hljs-keyword">from</span> torchsummary <span class="hljs-keyword">import</span> summary<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> SyncBatchNorm<span class="hljs-keyword">from</span> torch.distributed <span class="hljs-keyword">import</span> ReduceOp<span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DistributedDataParallel<span class="hljs-keyword">from</span> data <span class="hljs-keyword">import</span> get_loader<span class="hljs-keyword">from</span> model.CPD_ResNet_models <span class="hljs-keyword">import</span> CPD_ResNet<span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> clip_gradient, adjust_lr, save_single_plot, get_train_parser, init_workspaceparser = get_train_parser(loader_type=<span class="hljs-string">'rgb'</span>)parser.add_argument(<span class="hljs-string">'--local_rank'</span>, type=int, default=<span class="hljs-number">0</span>)opt = parser.parse_args()main_proc = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> opt.local_rank == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>basedir = init_workspace(opt, main_proc)<span class="hljs-comment"># init distribute environment</span>torch.cuda.set_device(opt.local_rank)dist.init_process_group(<span class="hljs-string">'nccl'</span>)device = torch.device(<span class="hljs-string">f'cuda:<span class="hljs-subst">&#123;opt.local_rank&#125;</span>'</span>)<span class="hljs-keyword">if</span> main_proc:    print(<span class="hljs-string">'Learning Rate: &#123;&#125; Model type: &#123;&#125;'</span>.format(opt.lr, opt.model))<span class="hljs-comment"># build models</span>model = CPD_ResNet()model = SyncBatchNorm.convert_sync_batchnorm(model).to(device)model = DistributedDataParallel(model, device_ids=[opt.local_rank], output_device=opt.local_rank)<span class="hljs-keyword">if</span> opt.print_model <span class="hljs-keyword">and</span> main_proc:    print(model)    summary(model, (<span class="hljs-number">4</span>, opt.trainsize, opt.trainsize))optimizer = torch.optim.Adam(model.parameters(), opt.lr)<span class="hljs-comment"># build distribute data loader</span>train_loader = get_loader(opt.train_img_dir, opt.train_gt_dir, loader_type=<span class="hljs-string">'rgb'</span>,                          batchsize=opt.batchsize, trainsize=opt.trainsize, dist=<span class="hljs-literal">True</span>)total_step = len(train_loader)<span class="hljs-comment"># build loss</span>CE = torch.nn.BCEWithLogitsLoss().to(device)<span class="hljs-comment"># save train results in df</span>df_step = pd.DataFrame(columns=(<span class="hljs-string">'loss1'</span>, <span class="hljs-string">'loss2'</span>))df_epoch = pd.DataFrame(columns=(<span class="hljs-string">'loss1'</span>, <span class="hljs-string">'loss2'</span>))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce_tensor</span><span class="hljs-params">(tensor: torch.Tensor)</span> -&gt; torch.Tensor:</span>    rt = tensor.clone()    dist.all_reduce(rt, op=dist.ReduceOp.SUM)    rt /= dist.get_world_size()    <span class="hljs-keyword">return</span> rt<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(train_loader, model, optimizer, epoch)</span>:</span>    model.train()    epoch_loss1 = []    epoch_loss2 = []    <span class="hljs-keyword">for</span> i, pack <span class="hljs-keyword">in</span> enumerate(train_loader, start=<span class="hljs-number">1</span>):        <span class="hljs-comment"># if main_proc:</span>        <span class="hljs-comment">#     print('running')</span>        optimizer.zero_grad()        images, gts = pack        images = images.to(device)        gts = gts.to(device)        <span class="hljs-comment"># if main_proc:</span>        <span class="hljs-comment">#     print('data done')</span>        atts, dets = model(images)        loss1 = CE(atts, gts)        loss2 = CE(dets, gts)        <span class="hljs-comment"># if main_proc:</span>        <span class="hljs-comment">#     print('CE done')</span>        loss = loss1 + loss2        loss.backward()        <span class="hljs-comment"># if main_proc:</span>        <span class="hljs-comment">#     print('loss done')</span>        <span class="hljs-comment"># save loss results</span>        reduced_loss1 = reduce_tensor(loss1.data).item()        <span class="hljs-comment"># if main_proc:</span>        <span class="hljs-comment">#     print('reduce loss1 done')</span>        reduced_loss2 = reduce_tensor(loss2.data).item()        <span class="hljs-comment"># if main_proc:</span>        <span class="hljs-comment">#     print('reduce loss2 done')</span>        <span class="hljs-keyword">if</span> main_proc:            epoch_loss1.append(reduced_loss1)            epoch_loss2.append(reduced_loss2)            df_step.loc[df_step.shape[<span class="hljs-number">0</span>]] = (reduced_loss1, reduced_loss2)        clip_gradient(optimizer, opt.clip)        optimizer.step()        <span class="hljs-keyword">if</span> (i % <span class="hljs-number">400</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i == total_step) <span class="hljs-keyword">and</span> main_proc:            print(<span class="hljs-string">'&#123;&#125; Epoch [&#123;:03d&#125;/&#123;:03d&#125;], Step [&#123;:04d&#125;/&#123;:04d&#125;], Loss1: &#123;:.4f&#125; Loss2: &#123;:0.4f&#125;'</span>.                  format(datetime.now(), epoch, opt.epoch, i, total_step,                         np.mean(epoch_loss1), np.mean(epoch_loss2)))    <span class="hljs-keyword">if</span> main_proc:        df_epoch.loc[df_epoch.shape[<span class="hljs-number">0</span>]] = (np.mean(epoch_loss1), np.mean(epoch_loss2))        save_path = os.path.join(basedir, <span class="hljs-string">'checkpoints'</span>)        os.makedirs(save_path, exist_ok=<span class="hljs-literal">True</span>)        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:            <span class="hljs-comment"># 注意，这里保存的是 model.module.state_dict()，这样测试的时候就不用做额外的处理</span>            torch.save(model.module.state_dict(), os.path.join(save_path, <span class="hljs-string">'CPD_&#123;&#125;.pth'</span>.format(epoch + <span class="hljs-number">1</span>)))print(<span class="hljs-string">"GPU &#123;&#125;: Let's go!"</span>.format(opt.local_rank))<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, opt.epoch + <span class="hljs-number">1</span>):    adjust_lr(optimizer, opt.lr, epoch, opt.decay_rate, opt.decay_epoch)    train(train_loader, model, optimizer, epoch)<span class="hljs-keyword">if</span> main_proc:    df_epoch.to_csv(os.path.join(basedir, <span class="hljs-string">'epoch_loss.csv'</span>), index=<span class="hljs-literal">False</span>)    df_step.to_csv(os.path.join(basedir, <span class="hljs-string">'step_loss.csv'</span>), index=<span class="hljs-literal">False</span>)    save_single_plot(df_epoch, (<span class="hljs-string">'loss1'</span>, <span class="hljs-string">'loss2'</span>), <span class="hljs-string">'epoch'</span>, <span class="hljs-string">'loss'</span>, basedir, <span class="hljs-string">'epoch'</span>)    save_single_plot(df_step, (<span class="hljs-string">'loss1'</span>, <span class="hljs-string">'loss2'</span>), <span class="hljs-string">'step'</span>, <span class="hljs-string">'loss'</span>, basedir, <span class="hljs-string">'step'</span>)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原理&lt;/h2&gt;&lt;p&gt;DDP 是 DistributedDataParallel 的简写，用来进行分布式训练，可以是单主机多 GPU 也可以是多主机多 GPU，以
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="PyTorch" scheme="https://blog.patrickcty.cc/tags/PyTorch/"/>
    
      <category term="分布式训练" scheme="https://blog.patrickcty.cc/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
      <category term="DDP" scheme="https://blog.patrickcty.cc/tags/DDP/"/>
    
  </entry>
  
  <entry>
    <title>SimCLR 代码分析</title>
    <link href="https://blog.patrickcty.cc/2020/11/10/simclr%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>https://blog.patrickcty.cc/2020/11/10/simclr代码分析/</id>
    <published>2020-11-10T09:08:49.000Z</published>
    <updated>2021-10-02T13:38:15.360Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章分析 <a href="https://github.com/sthalles/SimCLR" target="_blank" rel="noopener">PyTorch SimCLR: A Simple Framework for Contrastive Learning of Visual Representations</a>。使用自监督的方法来生成特征表示。其中 loss 部分实现得非常巧妙，因此特地拿出来分析。</p><h2 id="SimCLR-data-aug-dataset-wrapper-py"><a href="#SimCLR-data-aug-dataset-wrapper-py" class="headerlink" title="SimCLR/data_aug/dataset_wrapper.py"></a>SimCLR/data_aug/dataset_wrapper.py</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<span class="hljs-keyword">from</span> torch.utils.data.sampler <span class="hljs-keyword">import</span> SubsetRandomSampler<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-keyword">from</span> data_aug.gaussian_blur <span class="hljs-keyword">import</span> GaussianBlur<span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasetsnp.random.seed(<span class="hljs-number">0</span>)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DataSetWrapper</span><span class="hljs-params">(object)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, batch_size, num_workers, valid_size, input_shape, s)</span>:</span>        self.batch_size = batch_size        self.num_workers = num_workers        self.valid_size = valid_size        self.s = s        self.input_shape = eval(input_shape)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_data_loaders</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 数据增强</span>        data_augment = self._get_simclr_pipeline_transform()        <span class="hljs-comment"># 使用官方的 STL 数据集</span>        train_dataset = datasets.STL10(<span class="hljs-string">'./data'</span>, split=<span class="hljs-string">'train+unlabeled'</span>, download=<span class="hljs-literal">True</span>,                                       transform=SimCLRDataTransform(data_augment))        <span class="hljs-comment"># 随机划分训练集与验证集</span>        train_loader, valid_loader = self.get_train_validation_data_loaders(train_dataset)        <span class="hljs-keyword">return</span> train_loader, valid_loader    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_simclr_pipeline_transform</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 运用了很多数据增强的方法</span>        <span class="hljs-comment"># get a set of data augmentation transformations as described in the SimCLR paper.</span>        color_jitter = transforms.ColorJitter(<span class="hljs-number">0.8</span> * self.s, <span class="hljs-number">0.8</span> * self.s, <span class="hljs-number">0.8</span> * self.s, <span class="hljs-number">0.2</span> * self.s)        data_transforms = transforms.Compose([transforms.RandomResizedCrop(size=self.input_shape[<span class="hljs-number">0</span>]),                                              transforms.RandomHorizontalFlip(),                                              transforms.RandomApply([color_jitter], p=<span class="hljs-number">0.8</span>),                                              transforms.RandomGrayscale(p=<span class="hljs-number">0.2</span>),                                              GaussianBlur(kernel_size=int(<span class="hljs-number">0.1</span> * self.input_shape[<span class="hljs-number">0</span>])),                                              transforms.ToTensor()])        <span class="hljs-keyword">return</span> data_transforms    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_train_validation_data_loaders</span><span class="hljs-params">(self, train_dataset)</span>:</span>        <span class="hljs-comment"># 随机划分训练集和验证集 </span>        <span class="hljs-comment"># obtain training indices that will be used for validation</span>        num_train = len(train_dataset)        indices = list(range(num_train))        np.random.shuffle(indices)        split = int(np.floor(self.valid_size * num_train))        train_idx, valid_idx = indices[split:], indices[:split]        <span class="hljs-comment"># define samplers for obtaining training and validation batches</span>        train_sampler = SubsetRandomSampler(train_idx)        valid_sampler = SubsetRandomSampler(valid_idx)        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=train_sampler,                                  num_workers=self.num_workers, drop_last=<span class="hljs-literal">True</span>, shuffle=<span class="hljs-literal">False</span>)        valid_loader = DataLoader(train_dataset, batch_size=self.batch_size, sampler=valid_sampler,                                  num_workers=self.num_workers, drop_last=<span class="hljs-literal">True</span>)        <span class="hljs-keyword">return</span> train_loader, valid_loader<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SimCLRDataTransform</span><span class="hljs-params">(object)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, transform)</span>:</span>        self.transform = transform    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self, sample)</span>:</span>        <span class="hljs-comment"># 同一个 sample 增强两次得到两个输入</span>        xi = self.transform(sample)        xj = self.transform(sample)        <span class="hljs-keyword">return</span> xi, xj</code></pre><p>这部分要注意的主要就是对于一个数据数据得到两个增强的数据。</p><h2 id="SimCLR-models-resnet-simclr-py"><a href="#SimCLR-models-resnet-simclr-py" class="headerlink" title="SimCLR/models/resnet_simclr.py"></a>SimCLR/models/resnet_simclr.py</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ResNetSimCLR</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, base_model, out_dim)</span>:</span>        super(ResNetSimCLR, self).__init__()        self.resnet_dict = &#123;<span class="hljs-string">"resnet18"</span>: models.resnet18(pretrained=<span class="hljs-literal">False</span>),                            <span class="hljs-string">"resnet50"</span>: models.resnet50(pretrained=<span class="hljs-literal">False</span>)&#125;        resnet = self._get_basemodel(base_model)        num_ftrs = resnet.fc.in_features        self.features = nn.Sequential(*list(resnet.children())[:<span class="hljs-number">-1</span>])        <span class="hljs-comment"># projection MLP</span>        self.l1 = nn.Linear(num_ftrs, num_ftrs)        self.l2 = nn.Linear(num_ftrs, out_dim)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_basemodel</span><span class="hljs-params">(self, model_name)</span>:</span>        <span class="hljs-keyword">try</span>:            model = self.resnet_dict[model_name]            print(<span class="hljs-string">"Feature extractor:"</span>, model_name)            <span class="hljs-keyword">return</span> model        <span class="hljs-keyword">except</span>:            <span class="hljs-keyword">raise</span> (<span class="hljs-string">"Invalid model name. Check the config file and pass one of: resnet18 or resnet50"</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        h = self.features(x)        h = h.squeeze()        <span class="hljs-comment"># 用了两个 FC 层对特征进行映射</span>        <span class="hljs-comment"># 这个实现中设置最终得到 256 维向量</span>        x = self.l1(h)        x = F.relu(x)        x = self.l2(x)        <span class="hljs-keyword">return</span> h, x</code></pre><h2 id="SimCLR-loss-nt-xent-py"><a href="#SimCLR-loss-nt-xent-py" class="headerlink" title="SimCLR/loss/nt_xent.py"></a>SimCLR/loss/nt_xent.py</h2><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NTXentLoss</span><span class="hljs-params">(torch.nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, device, batch_size, temperature, use_cosine_similarity)</span>:</span>        super(NTXentLoss, self).__init__()        self.batch_size = batch_size        self.temperature = temperature        self.device = device        self.softmax = torch.nn.Softmax(dim=<span class="hljs-number">-1</span>)        <span class="hljs-comment"># 标注负样本所在的位置</span>        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)        self.similarity_function = self._get_similarity_function(use_cosine_similarity)        self.criterion = torch.nn.CrossEntropyLoss(reduction=<span class="hljs-string">"sum"</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_similarity_function</span><span class="hljs-params">(self, use_cosine_similarity)</span>:</span>        <span class="hljs-keyword">if</span> use_cosine_similarity:            self._cosine_similarity = torch.nn.CosineSimilarity(dim=<span class="hljs-number">-1</span>)            <span class="hljs-keyword">return</span> self._cosine_simililarity        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> self._dot_simililarity    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_correlated_mask</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 每个图像都经过两次增强，得到 2 * self.batch_size 个输入图像</span>        <span class="hljs-comment"># 两两求相似度之后得到 2N * 2N 矩阵</span>        <span class="hljs-comment"># 其中对角线以及上 N 下 N 对角线是同一个图像，也就是正样本</span>        diag = np.eye(<span class="hljs-number">2</span> * self.batch_size)        l1 = np.eye((<span class="hljs-number">2</span> * self.batch_size), <span class="hljs-number">2</span> * self.batch_size, k=-self.batch_size)        l2 = np.eye((<span class="hljs-number">2</span> * self.batch_size), <span class="hljs-number">2</span> * self.batch_size, k=self.batch_size)        mask = torch.from_numpy((diag + l1 + l2))        <span class="hljs-comment"># 取反就是负样本</span>        mask = (<span class="hljs-number">1</span> - mask).type(torch.bool)        <span class="hljs-keyword">return</span> mask.to(self.device)<span class="hljs-meta">    @staticmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_dot_simililarity</span><span class="hljs-params">(x, y)</span>:</span>        v = torch.tensordot(x.unsqueeze(<span class="hljs-number">1</span>), y.T.unsqueeze(<span class="hljs-number">0</span>), dims=<span class="hljs-number">2</span>)        <span class="hljs-comment"># x shape: (M, 1, C)</span>        <span class="hljs-comment"># y shape: (1, C, N)</span>        <span class="hljs-comment"># v shape: (M, N)</span>        <span class="hljs-comment"># 因为有 batch size，因此得到的相似度是一个矩阵</span>        <span class="hljs-keyword">return</span> v    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_cosine_simililarity</span><span class="hljs-params">(self, x, y)</span>:</span>        <span class="hljs-comment"># x shape: (M, 1, C)</span>        <span class="hljs-comment"># y shape: (1, N, C)</span>        <span class="hljs-comment"># v shape: (M, N)</span>        v = self._cosine_similarity(x.unsqueeze(<span class="hljs-number">1</span>), y.unsqueeze(<span class="hljs-number">0</span>))        <span class="hljs-keyword">return</span> v    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, zis, zjs)</span>:</span>        <span class="hljs-comment"># 组合成 2N 维向量，其中第 i 和 i + N 是同一个样本增强结果</span>        representations = torch.cat([zjs, zis], dim=<span class="hljs-number">0</span>)        <span class="hljs-comment"># 计算相似度，得到 2N * 2N 矩阵</span>        similarity_matrix = self.similarity_function(representations, representations)        <span class="hljs-comment"># 找到正样本，其中中间对角线是自身相乘的，可以无视掉</span>        <span class="hljs-comment"># filter out the scores from the positive samples</span>        l_pos = torch.diag(similarity_matrix, self.batch_size)        r_pos = torch.diag(similarity_matrix, -self.batch_size)        <span class="hljs-comment"># view 是将向量 reshape，得到一个 2N 维向量</span>        positives = torch.cat([l_pos, r_pos]).view(<span class="hljs-number">2</span> * self.batch_size, <span class="hljs-number">1</span>)        <span class="hljs-comment"># 负样本，同样也是 reshape，得到 2N * (2N - 2) 矩阵</span>        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(<span class="hljs-number">2</span> * self.batch_size, <span class="hljs-number">-1</span>)        <span class="hljs-comment"># 串联起来之后维度是 2N * (2N - 1)，其中每一列中第一个都是正样本，其他为负样本</span>        logits = torch.cat((positives, negatives), dim=<span class="hljs-number">1</span>)        logits /= self.temperature        <span class="hljs-comment"># 构造一个维度为 2N 的标签，0 对应相面的正样本</span>        labels = torch.zeros(<span class="hljs-number">2</span> * self.batch_size).to(self.device).long()        <span class="hljs-comment"># 交叉熵展开之后就是文中 loss 的形式</span>        loss = self.criterion(logits, labels)        <span class="hljs-keyword">return</span> loss / (<span class="hljs-number">2</span> * self.batch_size)</code></pre><p>这是代码中最关键的部分，这里在一个 batch 中构造正负样本，通过相似矩阵的形似很巧妙地挖掘出了正负样本，最终也巧妙用交叉熵实现了以下 loss 的形式。</p><p><img src="https://static.jnugeek.cn/blog/loss_contrastive.png" alt="Contrastive loss"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;这篇文章分析 &lt;a href=&quot;https://github.com/sthalles/SimCLR&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Contrastive Learning" scheme="https://blog.patrickcty.cc/tags/Contrastive-Learning/"/>
    
      <category term="Deep Leanring" scheme="https://blog.patrickcty.cc/tags/Deep-Leanring/"/>
    
  </entry>
  
  <entry>
    <title>keras 中 dense 层输入秩大于二</title>
    <link href="https://blog.patrickcty.cc/2020/11/03/keras%E4%B8%ADdense%E5%B1%82%E8%BE%93%E5%85%A5%E7%A7%A9%E5%A4%A7%E4%BA%8E%E4%BA%8C/"/>
    <id>https://blog.patrickcty.cc/2020/11/03/keras中dense层输入秩大于二/</id>
    <published>2020-11-03T10:36:16.000Z</published>
    <updated>2021-10-02T13:35:33.161Z</updated>
    
    <content type="html"><![CDATA[<p>通常情况下输入到 Dense 层（又或者叫 FC 层）的张量是一个 (batch_size, length) 的秩为 2 的形式。比如 AlexNet 和 VGG，输出的特征图都会经过 flatten 操作降维到 (None, 1024)，然后才输入到 Dense 层中。</p><p>但是今天我在看 MaskX R-CNN 的时候发现输入并不是一个二维矩阵，而是一个三维的张量。Keras 文档中在处理 Dense 秩大于二的时候会将其通过一个矩阵乘法来改变输出最后一维的长度（秩不变）。这样处理不是真正意义上所有神经元全连接，参数上也比 flatten 再还原要小很多。</p><pre><code class="hljs python">x = layers.Input((<span class="hljs-number">81</span>, <span class="hljs-number">1024</span>))  <span class="hljs-comment"># (None, 81, 1024)</span>y = layers.Dense(<span class="hljs-number">256</span>)  <span class="hljs-comment"># y shape: (None, 81, 256)</span><span class="hljs-comment"># 参数 W shape: (1024, 256) </span><span class="hljs-comment"># 参数 b shape: (256)</span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;通常情况下输入到 Dense 层（又或者叫 FC 层）的张量是一个 (batch_size, length) 的秩为 2 的形式。比如 AlexNet 和 VGG，输出的特征图都会经过 flatten 操作降维到 (None, 1024)，然后才输入到 Dense 层中。&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Keras" scheme="https://blog.patrickcty.cc/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>一个例子加深Python元类与描述符类理解</title>
    <link href="https://blog.patrickcty.cc/2020/10/27/%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E5%8A%A0%E6%B7%B1Python%E5%85%83%E7%B1%BB%E4%B8%8E%E6%8F%8F%E8%BF%B0%E7%AC%A6%E7%B1%BB%E7%90%86%E8%A7%A3/"/>
    <id>https://blog.patrickcty.cc/2020/10/27/一个例子加深Python元类与描述符类理解/</id>
    <published>2020-10-27T01:10:02.000Z</published>
    <updated>2021-10-02T13:25:49.305Z</updated>
    
    <content type="html"><![CDATA[<h2 id="通过函数注解来实现方法重载"><a href="#通过函数注解来实现方法重载" class="headerlink" title="通过函数注解来实现方法重载"></a>通过函数注解来实现方法重载</h2><p>最近在看 《Python Cookbook》，9.20 的示例涉及到非常多高级用法，有必要专门拿出来整理一下避免遗忘。这一节的目的是通过函数注解来实现方法重载，由于 Python 中对参数类型是没有硬性要求的，因此在 Python 中也没有方法重载这一特性。虽然函数注解能提示用户输入变量应该是什么类型，但实际上并没有类型检查与硬性的约束。在这一节之中就是用元类 + 描述符来实现这个功能。</p><h3 id="定义描述符类"><a href="#定义描述符类" class="headerlink" title="定义描述符类"></a>定义描述符类</h3><p>首先定义一个描述符类，用来将参数类型与对应的函数引用进行绑定，同时也可以通过传入的参数获取到函数引用。前者是使用一个注册函数来实现的，传入的是一个函数，得到这个函数可能的参数列表，然后将参数列表与函数绑定到一个字典中；后者是通过重写 <code>__call__</code> 方法来从输出参数找到对应函数，重写 <code>__get__</code> 方法将函数绑定 <code>self</code> 参数。</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> types<span class="hljs-keyword">import</span> inspect<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiMethod</span>:</span>    <span class="hljs-string">'''    Represents a single multimethod.    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name)</span>:</span>        self._methods = &#123;&#125;  <span class="hljs-comment"># 绑定参数类型与函数引用</span>        self.__name__ = name    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">register</span><span class="hljs-params">(self, meth)</span>:</span>        <span class="hljs-string">'''        Register a new method as a multimethod        '''</span>        sig = inspect.signature(meth)  <span class="hljs-comment"># 用来获取函数的参数信息</span>        <span class="hljs-comment"># Build a type-signature from the method's annotations</span>        types = []        <span class="hljs-keyword">for</span> name, parm <span class="hljs-keyword">in</span> sig.parameters.items():            <span class="hljs-keyword">if</span> name == <span class="hljs-string">'self'</span>:  <span class="hljs-comment"># 忽视掉 self 参数</span>                <span class="hljs-keyword">continue</span>            <span class="hljs-keyword">if</span> parm.annotation <span class="hljs-keyword">is</span> inspect.Parameter.empty:  <span class="hljs-comment"># 必须要有函数注解</span>                <span class="hljs-keyword">raise</span> TypeError(                    <span class="hljs-string">'Argument &#123;&#125; must be annotated with a type'</span>.format(name)                    )            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> isinstance(parm.annotation, type):  <span class="hljs-comment"># 函数注解必须要是类型</span>                <span class="hljs-keyword">raise</span> TypeError(                    <span class="hljs-string">'Argument &#123;&#125; annotation must be a type'</span>.format(name)                    )            <span class="hljs-comment"># 如果遇到有默认值的参数，那么在输入的时候不带这一项也可以</span>            <span class="hljs-comment"># 因此每一个都要单独作为一个参数类型的入口</span>            <span class="hljs-keyword">if</span> parm.default <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> inspect.Parameter.empty:                self._methods[tuple(types)] = meth            <span class="hljs-comment"># 因为不支持关键字参数，因此一旦传入了某个参数</span>            <span class="hljs-comment"># 其前面所有参数都得传入</span>            types.append(parm.annotation)        self._methods[tuple(types)] = meth    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self, *args)</span>:</span>        <span class="hljs-string">'''        Call a method based on type signature of the arguments        这样创建实例之后实例直接就是一个可调用对象了。        '''</span>        <span class="hljs-comment"># 首先将传入参数转换为类型元组，忽视掉 self 参数</span>        types = tuple(type(arg) <span class="hljs-keyword">for</span> arg <span class="hljs-keyword">in</span> args[<span class="hljs-number">1</span>:])        <span class="hljs-comment"># 然后通过元组来获取对应的方法引用</span>        meth = self._methods.get(types, <span class="hljs-literal">None</span>)        <span class="hljs-comment"># 找到了就表明是支持的参数列表</span>        <span class="hljs-keyword">if</span> meth:            <span class="hljs-keyword">return</span> meth(*args)        <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># 否则类型就不对应</span>            <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">'No matching method for types &#123;&#125;'</span>.format(types))            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__get__</span><span class="hljs-params">(self, instance, cls)</span>:</span>        <span class="hljs-string">'''        Descriptor method needed to make calls work in a class        这里主要是为了绑定 self，不然直接调用会提示少一个参数        '''</span>        <span class="hljs-keyword">if</span> instance <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            <span class="hljs-keyword">return</span> types.MethodType(self, instance)        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> self</code></pre><h3 id="定义元类"><a href="#定义元类" class="headerlink" title="定义元类"></a>定义元类</h3><p>接下来就是要使用元类把上面的集成到类中，最好的方法就是在创建的时候能通过描述符来绑定方法。这可以通过元类中的 clsdict 来实现。本文中的实现方法是修改 clsdict 的行为，在绑定方法的时候合并同名不同参数的函数。</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultiDict</span><span class="hljs-params">(dict)</span>:</span>    <span class="hljs-string">'''    Special dictionary to build multimethods in a metaclass    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__setitem__</span><span class="hljs-params">(self, key, value)</span>:</span>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> self:            <span class="hljs-comment"># If key already exists, it must be a multimethod or callable</span>            current_value = self[key]            <span class="hljs-keyword">if</span> isinstance(current_value, MultiMethod):                <span class="hljs-comment"># 某个名字的方法出现第三次，这次就直接注册了</span>                current_value.register(value)            <span class="hljs-keyword">else</span>:                <span class="hljs-comment"># 某个名字的方法出现第二次，那么首先先创建一个描述符</span>                mvalue = MultiMethod(key)                <span class="hljs-comment"># 分别注册这两个方法</span>                mvalue.register(current_value)                mvalue.register(value)                <span class="hljs-comment"># 将描述符绑定到类上</span>                super().__setitem__(key, mvalue)        <span class="hljs-keyword">else</span>:              <span class="hljs-comment"># 如果是第一次见到的，那直接设置属性</span>            <span class="hljs-comment"># 因为这个时候不会出现同名方法</span>            super().__setitem__(key, value)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MultipleMeta</span><span class="hljs-params">(type)</span>:</span>    <span class="hljs-string">'''    Metaclass that allows multiple dispatch of methods    '''</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__new__</span><span class="hljs-params">(cls, clsname, bases, clsdict)</span>:</span>        <span class="hljs-comment"># 这里的 clsdict 就是下面的 MultiDict</span>        <span class="hljs-comment"># 在 __new__ 中会创建类，也就是会将方法绑定到 clsdict 中</span>        <span class="hljs-comment"># 在绑定的时候，由于 clsdict 重写了 __setitem__</span>        <span class="hljs-comment"># 因此不会直接绑定方法，而是会绑定到描述符类</span>        <span class="hljs-keyword">return</span> type.__new__(cls, clsname, bases, dict(clsdict))<span class="hljs-meta">    @classmethod</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__prepare__</span><span class="hljs-params">(cls, clsname, bases)</span>:</span>        <span class="hljs-comment"># 这个函数在调用 __new__ 之前调用，返回一个映射对象</span>        <span class="hljs-keyword">return</span> MultiDict()</code></pre><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><pre><code class="hljs python"><span class="hljs-comment"># Some example classes that use multiple dispatch</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Spam</span><span class="hljs-params">(metaclass=MultipleMeta)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bar</span><span class="hljs-params">(self, x:int, y:int)</span>:</span>        print(<span class="hljs-string">'Bar 1:'</span>, x, y)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bar</span><span class="hljs-params">(self, s:str, n:int = <span class="hljs-number">0</span>)</span>:</span>        print(<span class="hljs-string">'Bar 2:'</span>, s, n)<span class="hljs-comment"># Example: overloaded __init__</span><span class="hljs-keyword">import</span> time<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Date</span><span class="hljs-params">(metaclass=MultipleMeta)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, year: int, month:int, day:int)</span>:</span>        self.year = year        self.month = month        self.day = day    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        t = time.localtime()        self.__init__(t.tm_year, t.tm_mon, t.tm_mday)s = Spam()s.bar(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)s.bar(<span class="hljs-string">'hello'</span>)s.bar(<span class="hljs-string">'hello'</span>, <span class="hljs-number">5</span>)<span class="hljs-keyword">try</span>:    s.bar(<span class="hljs-number">2</span>, <span class="hljs-string">'hello'</span>)<span class="hljs-keyword">except</span> TypeError <span class="hljs-keyword">as</span> e:    print(e)<span class="hljs-comment"># Overloaded __init__</span>d = Date(<span class="hljs-number">2012</span>, <span class="hljs-number">12</span>, <span class="hljs-number">21</span>)print(d.year, d.month, d.day)<span class="hljs-comment"># Get today's date</span>e = Date()print(e.year, e.month, e.day)</code></pre><p>输出结果<br><pre><code class="hljs undefined">Bar 1: 2 3Bar 2: hello 0Bar 2: hello 5No matching method for types (&lt;class &apos;int&apos;&gt;, &lt;class &apos;str&apos;&gt;)2012 12 212020 10 27</code></pre></p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><pre><code class="hljs undefined">&gt;&gt;&gt; b = s.bar&gt;&gt;&gt; b  # 绑定方法&lt;bound method bar of &lt;example1.Spam object at 0x10aa00d30&gt;&gt;&gt;&gt;&gt; b.__self__  # 类实例对象&lt;example1.Spam object at 0x10aa00d30&gt;&gt;&gt;&gt; b.__func__  # 实际的函数与描述符绑定&lt;example1.MultiMethod object at 0x10aaa8850&gt;</code></pre><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>正如作者所说，这种实现方法还是存在很多问题的，比如不支持关键字参数，对于继承也支持有限。因此在 Python 中还是使用更简单的方法，比如取不同的名字来实现比较好，不然也违背了 Python 设计的初衷。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="描述符"><a href="#描述符" class="headerlink" title="描述符"></a>描述符</h3><p>描述符是 Python 中的一类对象，它重写了 <code>__get__</code>, <code>__set__</code>, <code>__delete__</code> 中的一个或者多个。一般用于自定义的数据类型，可以在获取或者设置属性的时候加一些特殊的操作，比如类型检查、输出 log 等。在这里主要是通过 <code>__call__</code> 来从输入参数映射到不同的方法。通常情况下描述符和装饰器是可以相互转换的，在这一节中也给出了示例：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">multimethod</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, func)</span>:</span>        self._methods = &#123;&#125;        self.__name__ = func.__name__        self._default = func    <span class="hljs-comment"># 装饰器函数，这里传入函数不需要知道参数</span>    <span class="hljs-comment"># 但是装饰器需要传入类别作为参数，因此只有两层</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">match</span><span class="hljs-params">(self, *types)</span>:</span>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">register</span><span class="hljs-params">(func)</span>:</span>            ndefaults = len(func.__defaults__) <span class="hljs-keyword">if</span> func.__defaults__ <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(ndefaults+<span class="hljs-number">1</span>):                <span class="hljs-comment"># 目的和上面一样，也是为了处理默认参数</span>                self._methods[types[:len(types) - n]] = func            <span class="hljs-keyword">return</span> self        <span class="hljs-keyword">return</span> register    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__call__</span><span class="hljs-params">(self, *args)</span>:</span>        <span class="hljs-comment"># 还是要忽视掉 self 参数</span>        types = tuple(type(arg) <span class="hljs-keyword">for</span> arg <span class="hljs-keyword">in</span> args[<span class="hljs-number">1</span>:])        meth = self._methods.get(types, <span class="hljs-literal">None</span>)        <span class="hljs-keyword">if</span> meth:            <span class="hljs-keyword">return</span> meth(*args)        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> self._default(*args)            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__get__</span><span class="hljs-params">(self, instance, cls)</span>:</span>        <span class="hljs-keyword">if</span> instance <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            <span class="hljs-keyword">return</span> types.MethodType(self, instance)        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> self<span class="hljs-comment"># Example use</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Spam</span>:</span>    <span class="hljs-comment"># 相当于先初始化实例</span>    <span class="hljs-comment"># 这里定义好像不能指定函数参数类别</span>    <span class="hljs-comment"># 因此定义一个接受任意参数的函数作为缺省值来报错</span><span class="hljs-meta">    @multimethod  </span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bar</span><span class="hljs-params">(self, *args)</span>:</span>        <span class="hljs-comment"># Default method called if no match</span>        <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">'No matching method for bar'</span>)    <span class="hljs-comment"># 然后再通过类型进行绑定</span>    <span class="hljs-comment"># 这里就不用手动从参数转化到类型了</span><span class="hljs-meta">    @bar.match(int, int)  </span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bar</span><span class="hljs-params">(self, x, y)</span>:</span>        print(<span class="hljs-string">'Bar 1:'</span>, x, y)<span class="hljs-meta">    @bar.match(str, int)</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bar</span><span class="hljs-params">(self, s, n = <span class="hljs-number">0</span>)</span>:</span>        print(<span class="hljs-string">'Bar 2:'</span>, s, n)</code></pre><h2 id="元类"><a href="#元类" class="headerlink" title="元类"></a>元类</h2><p>元类是用来创建类的，可以在其中定义创建类时候的各种操作，比如这里就修改了绑定方法的地方，不直接将方法绑定到类中，而是将方法绑定到描述符中，然后通过描述符再绑定到类中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;通过函数注解来实现方法重载&quot;&gt;&lt;a href=&quot;#通过函数注解来实现方法重载&quot; class=&quot;headerlink&quot; title=&quot;通过函数注解来实现方法重载&quot;&gt;&lt;/a&gt;通过函数注解来实现方法重载&lt;/h2&gt;&lt;p&gt;最近在看 《Python Cookbook》，9.2
      
    
    </summary>
    
      <category term="Python" scheme="https://blog.patrickcty.cc/categories/Python/"/>
    
    
      <category term="元类" scheme="https://blog.patrickcty.cc/tags/%E5%85%83%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>使用mmsegmentation训练自己的数据集</title>
    <link href="https://blog.patrickcty.cc/2020/10/14/%E4%BD%BF%E7%94%A8mmsegmentation%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <id>https://blog.patrickcty.cc/2020/10/14/使用mmsegmentation训练自己的数据集/</id>
    <published>2020-10-14T12:01:44.000Z</published>
    <updated>2021-10-02T13:23:58.609Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><ul><li>安装</li><li>注册数据集</li><li>编写配置文件</li><li>运行</li><li>测试</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><pre><code class="hljs undefined">pip install mmcv# 注意只是 clone 是不行的，还要 install 一下产生版本文件pip install git+https://github.com/open-mmlab/mmsegmentation.git # install the master branch</code></pre><p>更多安装方法参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/install.md" target="_blank" rel="noopener">官方文档</a></p><h2 id="注册数据集"><a href="#注册数据集" class="headerlink" title="注册数据集"></a>注册数据集</h2><ul><li>在 <code>mmseg/datasets</code> 目录下添加自己的数据集的 .py 文件，这里主要是让框架知道模型的类别，下面 suffix 根据自己实际情况修改</li></ul><pre><code class="hljs python"><span class="hljs-keyword">import</span> os.path <span class="hljs-keyword">as</span> osp<span class="hljs-keyword">from</span> .builder <span class="hljs-keyword">import</span> DATASETS<span class="hljs-keyword">from</span> .custom <span class="hljs-keyword">import</span> CustomDataset<span class="hljs-meta">@DATASETS.register_module()</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SatelliteDataset</span><span class="hljs-params">(CustomDataset)</span>:</span>    <span class="hljs-string">"""Satellite dataset.    The ``img_suffix`` is fixed to '.tif' and ``seg_map_suffix`` is    fixed to '.png'.    """</span>    CLASSES = (<span class="hljs-string">'ford'</span>, <span class="hljs-string">'transportation'</span>, <span class="hljs-string">'building'</span>, <span class="hljs-string">'farmland'</span>, <span class="hljs-string">'grassland'</span>,               <span class="hljs-string">'woodland'</span>, <span class="hljs-string">'bare_soil'</span>,  <span class="hljs-string">'others'</span>)    PALETTE = [[<span class="hljs-number">120</span>, <span class="hljs-number">120</span>, <span class="hljs-number">120</span>], [<span class="hljs-number">180</span>, <span class="hljs-number">120</span>, <span class="hljs-number">120</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">230</span>, <span class="hljs-number">230</span>], [<span class="hljs-number">80</span>, <span class="hljs-number">50</span>, <span class="hljs-number">50</span>],               [<span class="hljs-number">4</span>, <span class="hljs-number">200</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">120</span>, <span class="hljs-number">120</span>, <span class="hljs-number">80</span>], [<span class="hljs-number">140</span>, <span class="hljs-number">140</span>, <span class="hljs-number">140</span>], [<span class="hljs-number">204</span>, <span class="hljs-number">5</span>, <span class="hljs-number">255</span>]]    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, **kwargs)</span>:</span>        super(SatelliteDataset, self).__init__(            img_suffix=<span class="hljs-string">'.tif'</span>,            seg_map_suffix=<span class="hljs-string">'.png'</span>,            reduce_zero_label=<span class="hljs-literal">False</span>,            **kwargs)        <span class="hljs-keyword">assert</span> osp.exists(self.img_dir)</code></pre><ul><li>在 <code>mmseg/datasets/__init__.py</code> 中导入你自定义的类，并在 <code>__all__</code> 变量中添加你的类名</li></ul><h2 id="编写配置文件"><a href="#编写配置文件" class="headerlink" title="编写配置文件"></a>编写配置文件</h2><p>在 <code>configs/你要用的方法/</code> 下创建一个 .py 文件，配置文件主要由四部分组成：</p><ul><li>使用模型</li><li>数据集及数据处理流程</li><li>模型调度方法</li><li>runtime 配置</li></ul><p>可以引入已有的配置，如果要修改配置就新建一个 dict 覆盖掉原来的配置项（不需要全部字段都有），如下所示：</p><pre><code class="hljs undefined">model = dict(    # 修改类别    decode_head=dict(num_classes=8, norm_cfg=norm_cfg),    auxiliary_head=dict(num_classes=8, norm_cfg=norm_cfg),    # 修改预训练路径    pretrained=&apos;open-mmlab://resnet101_v1c&apos;,    # 修改训练 backbone    backbone=dict(depth=101, norm_cfg=norm_cfg))</code></pre><p>放一个完整的配置：</p><pre><code class="hljs undefined">_base_ = [    &apos;../_base_/models/pspnet_r50-d8.py&apos;,    &apos;../_base_/default_runtime.py&apos;,    &apos;../_base_/schedules/schedule_40k.py&apos;]# norm_cfg = dict(type=&apos;BN&apos;, requires_grad=True)norm_cfg = dict(type=&apos;SyncBN&apos;, requires_grad=True)model = dict(    decode_head=dict(num_classes=8, norm_cfg=norm_cfg),    auxiliary_head=dict(num_classes=8, norm_cfg=norm_cfg),    pretrained=&apos;open-mmlab://resnet101_v1c&apos;,    backbone=dict(depth=101, norm_cfg=norm_cfg))dataset_type = &apos;SatelliteDataset&apos;data_root = &apos;/home/sse/data4T/common_datasets/satelite_dataset/&apos;img_norm_cfg = dict(    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)img_scale = (256, 256)# crop_size = (224, 224)train_pipeline = [    dict(type=&apos;LoadImageFromFile&apos;),    dict(type=&apos;LoadAnnotations&apos;),    dict(type=&apos;RandomFlip&apos;, flip_ratio=0.5),    dict(type=&apos;PhotoMetricDistortion&apos;),    dict(type=&apos;Normalize&apos;, **img_norm_cfg),    dict(type=&apos;DefaultFormatBundle&apos;),    dict(type=&apos;Collect&apos;, keys=[&apos;img&apos;, &apos;gt_semantic_seg&apos;]),]test_pipeline = [    dict(type=&apos;LoadImageFromFile&apos;),    dict(        type=&apos;MultiScaleFlipAug&apos;,        img_scale=img_scale,        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],        flip=False,        transforms=[            dict(type=&apos;Normalize&apos;, **img_norm_cfg),            dict(type=&apos;ImageToTensor&apos;, keys=[&apos;img&apos;]),            dict(type=&apos;Collect&apos;, keys=[&apos;img&apos;]),        ])]data = dict(    samples_per_gpu=12,    workers_per_gpu=0,    train=dict(        type=dataset_type,        data_root=data_root,        img_dir=&apos;img_dir/train&apos;,        ann_dir=&apos;ann_dir/train&apos;,        # split=&apos;ImageSets/SegmentationContext/train.txt&apos;,        pipeline=train_pipeline),    val=dict(  # 训练到一定轮次会自动验证        type=dataset_type,        data_root=data_root,        img_dir=&apos;img_dir/test&apos;,        ann_dir=&apos;ann_dir/test&apos;,        # split=&apos;ImageSets/SegmentationContext/val.txt&apos;,        pipeline=test_pipeline),    test=dict(  # 测试的时候才用到        type=dataset_type,        data_root=data_root,        img_dir=&apos;image_A/image_A_9&apos;,        # ann_dir=&apos;ann_dir/test&apos;,        # split=&apos;ImageSets/SegmentationContext/val.txt&apos;,        pipeline=test_pipeline))total_iters = 100000checkpoint_config = dict(by_epoch=False, interval=4000)evaluation = dict(interval=4000, metric=&apos;mIoU&apos;)# 训练结果保存路径work_dir = &apos;/home/sse/mmsegmentation/run/satellite-10-12&apos;</code></pre><p>详细配置还是参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/config.md" target="_blank" rel="noopener">官方文档</a></p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>单卡运行：</p><pre><code class="hljs undefined">python tools/train.py 配置文件路径名</code></pre><p>分布式运行</p><pre><code class="hljs undefined">./tools/dist_train.sh 配置文件路径名 GPU数 [optional arguments]</code></pre><p>从已有参数从头开始运行</p><pre><code class="hljs undefined">./tools/dist_train.sh 配置文件路径名 GPU数 --load-from 参数路径</code></pre><p>从已有参数从头继续运行</p><pre><code class="hljs undefined">./tools/dist_train.sh 配置文件路径名 GPU数 --resume-from 参数路径</code></pre><p>更详细的依然参考<a href="https://github.com/open-mmlab/mmsegmentation/blob/master/docs/getting_started.md" target="_blank" rel="noopener">官方文档</a></p><h3 id="运行时候的-一个坑"><a href="#运行时候的-一个坑" class="headerlink" title="运行时候的==一个坑=="></a>运行时候的==一个坑==</h3><p>验证数据集在验证时会把所有的预测结果和 GT 保存在内存中，如果验证集太大很可能进程会挂掉，测试同</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试的时候默认使用配置文件中指定的测试集</p><pre><code class="hljs undefined"># single-gpu testingpython tools/test.py $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; [--out $&#123;RESULT_FILE&#125;] [--eval $&#123;EVAL_METRICS&#125;] [--show]# multi-gpu testing./tools/dist_test.sh $&#123;CONFIG_FILE&#125; $&#123;CHECKPOINT_FILE&#125; $&#123;GPU_NUM&#125; [--out $&#123;RESULT_FILE&#125;] [--eval $&#123;EVAL_METRICS&#125;]</code></pre><p>可选参数</p><ul><li>RESULT_FILE: pickle 文件，结果保存在其中</li><li>EVAL_METRICS: 评价指标，制定之后需要标签文件</li><li>–show: 结果会在新窗口中打开</li><li>–show-dir: 可视化结果保存到指定文件夹中，注意保存的不是神经网络出来的结果，而是经过 RGB 调色之后和原图叠加的结果</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;总体流程&quot;&gt;&lt;a href=&quot;#总体流程&quot; class=&quot;headerlink&quot; title=&quot;总体流程&quot;&gt;&lt;/a&gt;总体流程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;安装&lt;/li&gt;
&lt;li&gt;注册数据集&lt;/li&gt;
&lt;li&gt;编写配置文件&lt;/li&gt;
&lt;li&gt;运行&lt;/li&gt;
&lt;li&gt;测
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="mmsegmentation" scheme="https://blog.patrickcty.cc/tags/mmsegmentation/"/>
    
      <category term="语义分割" scheme="https://blog.patrickcty.cc/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>【一个坑】exFAT 不支持软链接与硬链接</title>
    <link href="https://blog.patrickcty.cc/2020/10/10/exFAT%E4%B8%8D%E6%94%AF%E6%8C%81%E8%BD%AF%E9%93%BE%E6%8E%A5%E4%B8%8E%E7%A1%AC%E9%93%BE%E6%8E%A5/"/>
    <id>https://blog.patrickcty.cc/2020/10/10/exFAT不支持软链接与硬链接/</id>
    <published>2020-10-10T14:47:13.000Z</published>
    <updated>2021-10-02T13:32:12.045Z</updated>
    
    <content type="html"><![CDATA[<p>今天在跑 mmseg 的代码的时候报了一个 OSError，function not implemented。在终端使用 <code>ln</code> 命令也报了同样的问题，后来经过<a href="https://superuser.com/questions/1256530/linux-links-shortcuts-in-exfat-filesystem/1256536" target="_blank" rel="noopener">查找</a>之后发现 exFAT 文件系统不支持软链接与硬链接。</p><p>P.S. 创建软链接方法：<code>ln -s src_path dst_symlink_path</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天在跑 mmseg 的代码的时候报了一个 OSError，function not implemented。在终端使用 &lt;code&gt;ln&lt;/code&gt; 命令也报了同样的问题，后来经过&lt;a href=&quot;https://superuser.com/questions/1256
      
    
    </summary>
    
      <category term="日常踩坑" scheme="https://blog.patrickcty.cc/categories/%E6%97%A5%E5%B8%B8%E8%B8%A9%E5%9D%91/"/>
    
    
      <category term="Linux" scheme="https://blog.patrickcty.cc/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>keras自定义训练流程</title>
    <link href="https://blog.patrickcty.cc/2020/10/07/keras%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/"/>
    <id>https://blog.patrickcty.cc/2020/10/07/keras自定义训练流程/</id>
    <published>2020-10-07T12:48:50.000Z</published>
    <updated>2021-10-02T13:35:41.728Z</updated>
    
    <content type="html"><![CDATA[<h2 id="标准流程"><a href="#标准流程" class="headerlink" title="标准流程"></a>标准流程</h2><p>keras 的 api 集成度都非常高，在你没有额外需求的时候的时候能非常轻松地完成整个训练流程：</p><ul><li>加载数据<ul><li>可以选择 generator </li><li>也可以直接传入内存的数据</li><li>还可以按照一定格式组织成文件夹然后直接传文件夹名</li></ul></li><li>构造模型</li><li>编译模型<ul><li>指定优化器</li><li>指定损失函数</li><li>指定评价标准</li></ul></li><li>训练模型<ul><li>指定训练轮次（epoch）</li><li>指定回调</li></ul></li></ul><h2 id="自定义流程"><a href="#自定义流程" class="headerlink" title="自定义流程"></a>自定义流程</h2><p>标准流程在大多数情况下都能满足需求，但是对于一些需要获取网络中细节的情况下就需要自定义流程了。自定义主要也是对训练步骤进行处理，基本步骤如下：</p><ul><li>定义一个 step 的操作：<ul><li>取出这个 batch 的数据</li><li>传入网络得到输出</li><li>计算 loss</li><li>计算梯度</li><li>梯度下降</li></ul></li></ul><p>写成代码如下所示：</p><pre><code class="hljs python">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()train_loss = tf.keras.metrics.Mean(<span class="hljs-string">'train_loss'</span>, dtype=tf.float32)train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(<span class="hljs-string">'train_accuracy'</span>)<span class="hljs-keyword">for</span> idx, (x_train, y_train) <span class="hljs-keyword">in</span> enumerate(train_gen):    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:        predictions = model(x_train, training=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 传入网络得到输出</span>        loss = loss_object(y_train, predictions)  <span class="hljs-comment"># 计算 loss</span>    grads = tape.gradient(loss, model.trainable_variables)  <span class="hljs-comment"># 计算梯度</span>    optimizer.apply_gradients(zip(grads, model.trainable_variables))  <span class="hljs-comment"># 梯度下降</span>    <span class="hljs-comment"># 一些指标</span>    train_loss(loss)    train_accuracy(y_train, predictions)</code></pre><h2 id="和-TensorBoard-一起作用"><a href="#和-TensorBoard-一起作用" class="headerlink" title="和 TensorBoard 一起作用"></a>和 TensorBoard 一起作用</h2><p>尽管 keras 的 callback 里面也有 tensorboard，但是默认情况下它只能每个 ep 来保存评价指标和直方图，不能看一个 step 中的变化情况，也不能将参数或者梯度来画成图表。在这里我们将参数和梯度的 l2 范数变化情况画成图表，并且原本就有的直方图也不落下。</p><pre><code class="hljs python"><span class="hljs-comment"># create a tensorboard file writer</span>summary_writer = tf.summary.create_file_writer(some_path)loss_object = tf.keras.losses.SparseCategoricalCrossentropy()train_loss = tf.keras.metrics.Mean(<span class="hljs-string">'train_loss'</span>, dtype=tf.float32)train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(<span class="hljs-string">'train_accuracy'</span>)<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, num_epochs + <span class="hljs-number">1</span>):    <span class="hljs-keyword">for</span> idx, (x_train, y_train) <span class="hljs-keyword">in</span> enumerate(train_gen):        n_iter = (epoch - <span class="hljs-number">1</span>) * len(train_gen) + idx + <span class="hljs-number">1</span>                <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:            predictions = model(x_train, training=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 传入网络得到输出</span>            loss = loss_object(y_train, predictions)  <span class="hljs-comment"># 计算 loss</span>        gradients = tape.gradient(loss, model.trainable_variables)  <span class="hljs-comment"># 计算梯度</span>        optimizer.apply_gradients(zip(grads, model.trainable_variables))  <span class="hljs-comment"># 梯度下降</span>        trainable_vars = model.trainable_variables                <span class="hljs-comment"># 一些指标，都是标量</span>        train_loss(loss)        train_accuracy(y_train, predictions)                <span class="hljs-keyword">with</span> summary_writer.as_default():            <span class="hljs-comment"># 写入评价指标</span>            tf.summary.scalar(<span class="hljs-string">'loss'</span>, train_loss.result(), step=n_iter)            tf.summary.scalar(<span class="hljs-string">'accuracy'</span>, train_accuracy.result(), step=n_iter)                        <span class="hljs-keyword">for</span> var, grad <span class="hljs-keyword">in</span> zip(trainable_vars, gradients)                <span class="hljs-comment"># 写入各个可训练元素的直方图、梯度和参数</span>                tf.summary.histograme(var.name, var, n_iter)                tf.summary.scalar(<span class="hljs-string">'Grads:'</span> + var.name, tf.norm(grad), n_iter)                tf.summart.scalar(<span class="hljs-string">'Weights'</span> + var.name, ty.norm(var), n_iter)    print(<span class="hljs-string">'Epoch &#123;:03d&#125; finished.'</span>.format(epoch))</code></pre><p>TensorBoard 里面的数据不能导出来，也可以单独将其写入 csv 来方便后续的处理。</p><h2 id="参考教程"><a href="#参考教程" class="headerlink" title="参考教程"></a>参考教程</h2><ul><li><a href="https://keras.io/guides/customizing_what_happens_in_fit/" target="_blank" rel="noopener">https://keras.io/guides/customizing_what_happens_in_fit/</a></li><li><a href="https://www.tensorflow.org/tensorboard/get_started" target="_blank" rel="noopener">https://www.tensorflow.org/tensorboard/get_started</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;标准流程&quot;&gt;&lt;a href=&quot;#标准流程&quot; class=&quot;headerlink&quot; title=&quot;标准流程&quot;&gt;&lt;/a&gt;标准流程&lt;/h2&gt;&lt;p&gt;keras 的 api 集成度都非常高，在你没有额外需求的时候的时候能非常轻松地完成整个训练流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li
      
    
    </summary>
    
      <category term="机器学习" scheme="https://blog.patrickcty.cc/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Keras" scheme="https://blog.patrickcty.cc/tags/Keras/"/>
    
      <category term="深度学习" scheme="https://blog.patrickcty.cc/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="https://blog.patrickcty.cc/tags/TensorFlow/"/>
    
      <category term="TensorBoard" scheme="https://blog.patrickcty.cc/tags/TensorBoard/"/>
    
  </entry>
  
</feed>
